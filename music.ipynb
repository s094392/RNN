{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5 1 5 1]\n",
      " [6 1 6 1]\n",
      " [4 2 4 2]\n",
      " [5 2 5 2]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "seq = np.array(\n",
    "[[[5, 5, 5, 3],\n",
    "  [1, 2, 3, 1],\n",
    "  [4, 2, 3, 2],\n",
    "  [2, 3, 4, 5]],\n",
    " [[5, 1, 5, 1],\n",
    "  [6, 1, 6, 1],\n",
    "  [4, 2, 4, 2],\n",
    "  [5, 2, 5, 2]],\n",
    " [[1, 3, 5, 1],\n",
    "  [1, 3, 6, 3],\n",
    "  [2, 4, 6, 4],\n",
    "  [2, 5, 7, 5]],\n",
    " [[1, 7, 1, 5],\n",
    "  [6, 5, 3, 1],\n",
    "  [4, 3, 2, 1],\n",
    "  [1, 7, 1, 2]],\n",
    " [[5, 1, 2, 1],\n",
    "  [1, 7, 1, 3],\n",
    "  [6, 7, 1, 2],\n",
    "  [3, 2, 3, 2]],\n",
    " [[3, 2, 3, 4],\n",
    "  [1, 1, 7, 6],\n",
    "  [6, 5, 4, 3],\n",
    "  [3, 2, 1, 7]],\n",
    " [[1, 2, 3, 1],\n",
    "  [3, 2, 1, 7],\n",
    "  [6, 7, 1, 2],\n",
    "  [3, 2, 1, 2]],\n",
    " [[1, 2, 3, 5],\n",
    "  [1, 2, 3, 1],\n",
    "  [4, 4, 3, 2],\n",
    "  [2, 3, 4, 5]],\n",
    " [[1, 7, 1, 2],\n",
    "  [3, 2, 1, 5],\n",
    "  [4, 3, 2, 3],\n",
    "  [2, 3, 2, 5]],\n",
    " [[6, 7, 1, 2],\n",
    "  [2, 1, 7, 5],\n",
    "  [4, 5, 6, 7],\n",
    "  [5, 6, 7, 1]],\n",
    " [[1, 2, 1, 3],\n",
    "  [2, 3, 2, 1],\n",
    "  [6, 7, 1, 2],\n",
    "  [3, 2, 3, 4]],\n",
    " [[3, 6, 6, 3],\n",
    "  [2, 5, 5, 2],\n",
    "  [1, 1, 2, 3],\n",
    "  [7, 7, 1, 1]],\n",
    " [[6, 3, 1, 6],\n",
    "  [5, 2, 7, 5],\n",
    "  [6, 4, 1, 6],\n",
    "  [7, 5, 2, 4]],\n",
    " [[6, 6, 7, 1],\n",
    "  [1, 7, 7, 5],\n",
    "  [6, 6, 7, 1],\n",
    "  [5, 5, 6, 6]],\n",
    " [[1, 2, 3, 6],\n",
    "  [7, 1, 2, 5],\n",
    "  [6, 7, 1, 4],\n",
    "  [5, 5, 6, 6]],\n",
    " [[3, 2, 1, 7],\n",
    "  [2, 1, 7, 6],\n",
    "  [6, 1, 6, 5],\n",
    "  [6, 7, 1, 1]],\n",
    " [[1, 7, 1, 2],\n",
    "  [3, 2, 1, 7],\n",
    "  [6, 7, 1, 2],\n",
    "  [3, 3, 2, 1]],\n",
    " [[6, 6, 6, 6],\n",
    "  [5, 5, 5, 5],\n",
    "  [4, 4, 4, 4],\n",
    "  [5, 5, 5, 4]],\n",
    " [[3, 4, 5, 1],\n",
    "  [5, 1, 3, 2],\n",
    "  [6, 7, 1, 2],\n",
    "  [3, 2, 3, 4]],\n",
    " [[1, 2, 3, 1],\n",
    "  [3, 4, 5, 3],\n",
    "  [4, 4, 3, 2],\n",
    "  [2, 3, 4, 5]],\n",
    " [[5, 3, 5, 3],\n",
    "  [5, 3, 1, 7],\n",
    "  [6, 1, 2, 3],\n",
    "  [3, 2, 3, 4]],\n",
    " [[5, 3, 5, 1],\n",
    "  [1, 7, 5, 3],\n",
    "  [6, 5, 5, 4],\n",
    "  [5, 5, 6, 6]],\n",
    " [[1, 1, 7, 1],\n",
    "  [5, 6, 7, 5],\n",
    "  [6, 5, 6, 7],\n",
    "  [1, 7, 1, 1]],\n",
    " [[3, 2, 3, 5],\n",
    "  [1, 7, 1, 5],\n",
    "  [6, 7, 1, 2],\n",
    "  [2, 3, 4, 4]],\n",
    " [[3, 3, 4, 3],\n",
    "  [5, 5, 6, 7],\n",
    "  [1, 1, 7, 1],\n",
    "  [5, 4, 3, 1]],\n",
    " [[5, 3, 1, 3],\n",
    "  [7, 5, 3, 5],\n",
    "  [6, 4, 1, 4],\n",
    "  [7, 5, 2, 4]],\n",
    " [[3, 4, 5, 5],\n",
    "  [5, 1, 7, 1],\n",
    "  [6, 5, 4, 3],\n",
    "  [3, 2, 1, 6]]])\n",
    "\n",
    "targets = np.array(\n",
    "[[1, 6, 2, 5],\n",
    " [1, 6, 2, 5],\n",
    " [1, 6, 2, 5],\n",
    " [1, 6, 2, 5],\n",
    " [1, 6, 2, 5],\n",
    " [1, 6, 2, 5],\n",
    " [1, 6, 2, 5],\n",
    " [1, 6, 2, 5],\n",
    " [1, 6, 2, 5],\n",
    " [6, 5, 4, 5],\n",
    " [6, 5, 4, 5],\n",
    " [6, 5, 4, 5],\n",
    " [6, 5, 4, 5],\n",
    " [6, 5, 4, 5],\n",
    " [6, 5, 4, 5],\n",
    " [6, 5, 4, 5],\n",
    " [6, 5, 4, 5],\n",
    " [6, 5, 4, 5],\n",
    " [1, 3, 4, 5],\n",
    " [1, 3, 4, 5],\n",
    " [1, 3, 4, 5],\n",
    " [1, 3, 4, 5],\n",
    " [1, 3, 4, 5],\n",
    " [1, 3, 4, 5],\n",
    " [1, 3, 4, 5],\n",
    " [1, 3, 4, 5],\n",
    " [1, 3, 4, 5]])\n",
    "\n",
    "print seq[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "Gated recurrent units (Long Short-Term Memory)\n",
    "Code provided by Mohammad Pezeshki - Nov. 2014 - Universite de Montreal\n",
    "This code is distributed without any warranty, express or implied.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "\n",
    "class LSTM(object):\n",
    "    def __init__(self, n_u, n_h):\n",
    "\n",
    "        self.n_u = int(n_u)\n",
    "        self.n_h = int(n_h)\n",
    "\n",
    "        # Here we define 8 weight matrices and 4 bias vector.\n",
    "        # To determine the dimension of weight matrices and bias vectors,\n",
    "        # we just need to have the following numbers: n_u, n_h\n",
    "        # (You can check it using the equations).\n",
    "        #\n",
    "        # Thus:\n",
    "        # <var>: <dimention>\n",
    "        #\n",
    "        # W_xi : n_h  x  n_u\n",
    "        # W_hi : n_h  x  n_h\n",
    "        # W_ci : n_h  x  n_h\n",
    "        # W_xf : n_h  x  n_u\n",
    "        # W_hf : n_h  x  n_h\n",
    "        # W_cf : n_h  x  n_h\n",
    "        # W_xc : n_h  x  n_u\n",
    "        # W_hc : n_h  x  n_h\n",
    "        #\n",
    "        # b_i : n_h  x  1 #Note: do not confuse b_i as the bias of input! It's the bias of input GATE\n",
    "        # b_f : n_h  x  1\n",
    "        # b_o : n_h  x  1\n",
    "        # b_c : n_h  x  1\n",
    "\n",
    "        # Input gate weights\n",
    "        self.W_xi = theano.shared(value = np.asarray(\n",
    "                                              np.random.uniform(\n",
    "                                                  size = (n_h, n_u),\n",
    "                                                  low = -.01, high = .01),\n",
    "                                              dtype = theano.config.floatX),\n",
    "                                  name = 'W_xi')\n",
    "        self.W_hi = theano.shared(value = np.asarray(\n",
    "                                              np.random.uniform(\n",
    "                                                  size = (n_h, n_h),\n",
    "                                                  low = -.01, high = .01),\n",
    "                                              dtype = theano.config.floatX),\n",
    "                                  name = 'W_hi')\n",
    "        self.W_ci = theano.shared(value = np.asarray(\n",
    "                                              np.random.uniform(\n",
    "                                                  size = (n_h, n_h),\n",
    "                                                  low = -.01, high = .01),\n",
    "                                              dtype = theano.config.floatX),\n",
    "                                  name = 'W_ci')\n",
    "\n",
    "        # Forget gate weights\n",
    "        self.W_xf = theano.shared(value = np.asarray(\n",
    "                                              np.random.uniform(\n",
    "                                                  size = (n_h, n_u),\n",
    "                                                  low = -.01, high = .01),\n",
    "                                              dtype = theano.config.floatX),\n",
    "                                  name = 'W_xf')\n",
    "        self.W_hf = theano.shared(value = np.asarray(\n",
    "                                              np.random.uniform(\n",
    "                                                  size = (n_h, n_h),\n",
    "                                                  low = -.01, high = .01),\n",
    "                                              dtype = theano.config.floatX),\n",
    "                                  name = 'W_hf')\n",
    "        self.W_cf = theano.shared(value = np.asarray(\n",
    "                                              np.random.uniform(\n",
    "                                                  size = (n_h, n_h),\n",
    "                                                  low = -.01, high = .01),\n",
    "                                              dtype = theano.config.floatX),\n",
    "                                  name = 'W_cf')\n",
    "\n",
    "        # Output gate weights\n",
    "        self.W_xo = theano.shared(value = np.asarray(\n",
    "                                              np.random.uniform(\n",
    "                                                  size = (n_h, n_u),\n",
    "                                                  low = -.01, high = .01),\n",
    "                                              dtype = theano.config.floatX),\n",
    "                                  name = 'W_xo')\n",
    "        self.W_ho = theano.shared(value = np.asarray(\n",
    "                                              np.random.uniform(\n",
    "                                                  size = (n_h, n_h),\n",
    "                                                  low = -.01, high = .01),\n",
    "                                              dtype = theano.config.floatX),\n",
    "                                  name = 'W_ho')\n",
    "        self.W_co = theano.shared(value = np.asarray(\n",
    "                                              np.random.uniform(\n",
    "                                                  size = (n_h, n_h),\n",
    "                                                  low = -.01, high = .01),\n",
    "                                              dtype = theano.config.floatX),\n",
    "                                  name = 'W_co')\n",
    "\n",
    "        # Cell weights\n",
    "        self.W_xc = theano.shared(value = np.asarray(\n",
    "                                              np.random.uniform(\n",
    "                                                  size = (n_h, n_u),\n",
    "                                                  low = -.01, high = .01),\n",
    "                                              dtype = theano.config.floatX),\n",
    "                                  name = 'W_xc')\n",
    "        self.W_hc = theano.shared(value = np.asarray(\n",
    "                                              np.random.uniform(\n",
    "                                                  size = (n_h, n_h),\n",
    "                                                  low = -.01, high = .01),\n",
    "                                              dtype = theano.config.floatX),\n",
    "                                  name = 'W_hc')\n",
    "\n",
    "        # Input gate bias\n",
    "        self.b_i = theano.shared(value = np.zeros(\n",
    "                                             (n_h, ),\n",
    "                                             dtype = theano.config.floatX),\n",
    "                                 name = 'b_i')\n",
    "\n",
    "        # Forget gate bias\n",
    "        self.b_f = theano.shared(value = np.zeros(\n",
    "                                             (n_h, ),\n",
    "                                             dtype = theano.config.floatX),\n",
    "                                 name = 'b_f')\n",
    "\n",
    "        # Output gate bias\n",
    "        self.b_o = theano.shared(value = np.zeros(\n",
    "                                             (n_h, ),\n",
    "                                             dtype = theano.config.floatX),\n",
    "                                 name = 'b_o')\n",
    "\n",
    "        # cell bias\n",
    "        self.b_c = theano.shared(value = np.zeros(\n",
    "                                             (n_h, ),\n",
    "                                             dtype = theano.config.floatX),\n",
    "                                 name = 'b_c')\n",
    "\n",
    "        self.params = [self.W_xi, self.W_hi, self.W_ci,\n",
    "                          self.W_xf, self.W_hf, self.W_cf,\n",
    "                          self.W_xo, self.W_ho, self.W_co,\n",
    "                          self.W_xc, self.W_hc,\n",
    "                          self.b_i, self.b_f, self.b_o,\n",
    "                          self.b_c]\n",
    "\n",
    "\n",
    "    def lstm_as_activation_function(self, x_t, h_tm1, c_tm1):\n",
    "        #print self.W_xi.get_value(borrow = True)\n",
    "        i_t = T.nnet.sigmoid(T.dot(self.W_xi, x_t) + \\\n",
    "                             T.dot(self.W_hi, h_tm1) + \\\n",
    "                             T.dot(self.W_ci, c_tm1) + \\\n",
    "                             self.b_i)\n",
    "        f_t = T.nnet.sigmoid(T.dot(self.W_xf, x_t) + \\\n",
    "                             T.dot(self.W_hf, h_tm1) + \\\n",
    "                             T.dot(self.W_cf, c_tm1) + \\\n",
    "                             self.b_f)\n",
    "        c_t = f_t * c_tm1 + i_t * \\\n",
    "                  T.tanh(T.dot(self.W_xc, x_t) + \\\n",
    "                         T.dot(self.W_hc, h_tm1) + \\\n",
    "                         self.b_c)\n",
    "        o_t = T.nnet.sigmoid(T.dot(self.W_xo, x_t) + \\\n",
    "                        T.dot(self.W_ho, h_tm1) + \\\n",
    "                        T.dot(self.W_co, c_t) + \\\n",
    "                        self.b_o)\n",
    "        h_t = o_t * T.tanh(c_t)\n",
    "\n",
    "        return h_t, c_t\n",
    "\n",
    "# Gated Recurrent Unit\n",
    "# Recently proposed: \n",
    "# K. Cho, B. van Merrienboer,\n",
    "# D. Bahdanau, and Y. Bengio.\n",
    "# On the properties of neural \n",
    "# machine translation: Encoder-decoder approaches. \n",
    "class GRU(object):\n",
    "    def __init__(self, n_u, n_h):\n",
    "\n",
    "        self.n_u = int(n_u)\n",
    "        self.n_h = int(n_h)\n",
    "\n",
    "        # Here we define 6 weight matrices and 3 bias vector.\n",
    "        # To determine the dimention of weight matrices and bias vectors,\n",
    "        # we just need to have the following numbers: n_u, n_h\n",
    "        # (You can check it using the equations).\n",
    "        #\n",
    "        # Thus:\n",
    "        # <var>: <dimention>\n",
    "        #\n",
    "        # W_xz : n_h  x  n_u\n",
    "        # W_hz : n_h  x  n_h\n",
    "        # W_xr : n_h  x  n_u\n",
    "        # W_hr : n_h  x  n_h\n",
    "        # W_xh : n_h  x  n_h\n",
    "        # W_hh : n_h  x  n_h\n",
    "        #\n",
    "        # b_z : n_h  x  1\n",
    "        # b_r : n_h  x  1\n",
    "        # b_h : n_h  x  1\n",
    "\n",
    "        # Update gate weights\n",
    "        self.W_xz = theano.shared(value = np.asarray(\n",
    "                                              np.random.uniform(\n",
    "                                                  size = (n_h, n_u),\n",
    "                                                  low = -.01, high = .01),\n",
    "                                              dtype = theano.config.floatX),\n",
    "                                  name = 'W_xz')\n",
    "        self.W_hz = theano.shared(value = np.asarray(\n",
    "                                              np.random.uniform(\n",
    "                                                  size = (n_h, n_h),\n",
    "                                                  low = -.01, high = .01),\n",
    "                                              dtype = theano.config.floatX),\n",
    "                                  name = 'W_hz')\n",
    "\n",
    "        # Reset gate weights\n",
    "        self.W_xr = theano.shared(value = np.asarray(\n",
    "                                              np.random.uniform(\n",
    "                                                  size = (n_h, n_u),\n",
    "                                                  low = -.01, high = .01),\n",
    "                                              dtype = theano.config.floatX),\n",
    "                                  name = 'W_xr')\n",
    "        self.W_hr = theano.shared(value = np.asarray(\n",
    "                                              np.random.uniform(\n",
    "                                                  size = (n_h, n_h),\n",
    "                                                  low = -.01, high = .01),\n",
    "                                              dtype = theano.config.floatX),\n",
    "                                  name = 'W_hr')\n",
    "\n",
    "        # Other weights :-)\n",
    "        self.W_xh = theano.shared(value = np.asarray(\n",
    "                                              np.random.uniform(\n",
    "                                                  size = (n_h, n_u),\n",
    "                                                  low = -.01, high = .01),\n",
    "                                              dtype = theano.config.floatX),\n",
    "                                  name = 'W_xh')\n",
    "        self.W_hh = theano.shared(value = np.asarray(\n",
    "                                              np.random.uniform(\n",
    "                                                  size = (n_h, n_h),\n",
    "                                                  low = -.01, high = .01),\n",
    "                                              dtype = theano.config.floatX),\n",
    "                                  name = 'W_hh')\n",
    "\n",
    "        # Update gate bias\n",
    "        self.b_z = theano.shared(value = np.zeros(\n",
    "                                             (n_h, ),\n",
    "                                             dtype = theano.config.floatX),\n",
    "                                 name = 'b_z')\n",
    "\n",
    "        # Reset gate bias\n",
    "        self.b_r = theano.shared(value = np.zeros(\n",
    "                                             (n_h, ),\n",
    "                                             dtype = theano.config.floatX),\n",
    "                                 name = 'b_r')\n",
    "\n",
    "        # Hidden layer bias\n",
    "        self.b_h = theano.shared(value = np.zeros(\n",
    "                                             (n_h, ),\n",
    "                                             dtype = theano.config.floatX),\n",
    "                                 name = 'b_h')\n",
    "\n",
    "        self.params = [self.W_xz, self.W_hz, self.W_xr, self.W_hr, \n",
    "                          self.W_xh, self.W_hh, self.b_z, self.b_r, \n",
    "                          self.b_h]\n",
    "\n",
    "\n",
    "    def gru_as_activation_function(self, x_t, h_tm1):\n",
    "        # update gate\n",
    "        z_t = T.nnet.sigmoid(T.dot(self.W_xz, x_t) + \\\n",
    "                             T.dot(self.W_hz, h_tm1) + \\\n",
    "                             self.b_z)\n",
    "        # reset gate\n",
    "        r_t = T.nnet.sigmoid(T.dot(self.W_xr, x_t) + \\\n",
    "                             T.dot(self.W_hr, h_tm1) + \\\n",
    "                             self.b_r)\n",
    "        # candidate h_t\n",
    "        can_h_t = T.tanh(T.dot(self.W_xh, x_t) + \\\n",
    "                         r_t * T.dot(self.W_hh, h_tm1) + \\\n",
    "                         self.b_h)\n",
    "        # h_t\n",
    "        h_t = (1 - z_t) * h_tm1 + z_t * can_h_t\n",
    "\n",
    "        return h_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing model with softmax outputs\n",
      "Buiding model ..."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import time\n",
    "import os\n",
    "import datetime\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "plt.ion()\n",
    "\n",
    "mode = theano.Mode(linker='cvm') #the runtime algo to execute the code is in c\n",
    "\n",
    "class RNN(object):\n",
    "    def __init__(self, n_u, n_h, n_y, learning_rate, learning_rate_decay, L1_reg, L2_reg,\n",
    "                 initial_momentum, final_momentum, momentum_switchover,\n",
    "                 n_epochs):\n",
    "\n",
    "        self.n_u = int(n_u)\n",
    "        self.n_h = int(n_h)\n",
    "        self.n_y = int(n_y)\n",
    "        self.gru = GRU(n_u, n_h)\n",
    "        self.learning_rate = float(learning_rate)\n",
    "        self.learning_rate_decay = float(learning_rate_decay)\n",
    "        self.L1_reg = float(L1_reg)\n",
    "        self.L2_reg = float(L2_reg)\n",
    "        self.initial_momentum = float(initial_momentum)\n",
    "        self.final_momentum = float(final_momentum)\n",
    "        self.momentum_switchover = int(momentum_switchover)\n",
    "        self.n_epochs = int(n_epochs)\n",
    "\n",
    "        # input which is `x`\n",
    "        self.x = T.matrix()\n",
    "\n",
    "        # Note that some the bellow variables are not used when\n",
    "        # the activation function is LSTM or GRU. But we simply\n",
    "        # don't care because theano optimize this for us.\n",
    "        #\n",
    "        # Weights are initialized from an uniform distribution\n",
    "        self.W_uh = theano.shared(value = np.asarray(\n",
    "                                              np.random.uniform(\n",
    "                                                  size = (n_u, n_h),\n",
    "                                                  low = -.01, high = .01),\n",
    "                                              dtype = theano.config.floatX),\n",
    "                                  name = 'W_uh')\n",
    "\n",
    "        self.W_hh = theano.shared(value = np.asarray(\n",
    "                                              np.random.uniform(\n",
    "                                                  size = (n_h, n_h),\n",
    "                                                  low = -.01, high = .01),\n",
    "                                              dtype = theano.config.floatX),\n",
    "                                  name = 'W_hh')\n",
    "\n",
    "        self.W_hy = theano.shared(value = np.asarray(\n",
    "                                              np.random.uniform(\n",
    "                                                  size = (n_h, n_y),\n",
    "                                                  low = -.01, high = .01),\n",
    "                                              dtype = theano.config.floatX),\n",
    "                                  name = 'W_hy')\n",
    "\n",
    "        # initial value of hidden layer units are set to zero\n",
    "        self.h0 = theano.shared(value = np.zeros(\n",
    "                                            (n_h, ),\n",
    "                                            dtype = theano.config.floatX),\n",
    "                                name = 'h0')\n",
    "\n",
    "        self.c0 = theano.shared(value = np.zeros(\n",
    "                                            (n_h, ),\n",
    "                                            dtype = theano.config.floatX),\n",
    "                                name = 'c0')\n",
    "\n",
    "        # biases are initialized to zeros\n",
    "        self.b_h = theano.shared(value = np.zeros(\n",
    "                                             (n_h, ),\n",
    "                                             dtype = theano.config.floatX),\n",
    "                                 name = 'b_h')\n",
    "\n",
    "        self.b_y = theano.shared(value = np.zeros(\n",
    "                                             (n_y, ),\n",
    "                                             dtype = theano.config.floatX),\n",
    "                                 name = 'b_y')\n",
    "        # That's because when it is lstm or gru, parameters are different\n",
    "        #if activation == 'lstm':\n",
    "            # Note that `+` here is just a concatenation operator\n",
    "        self.params = self.gru.params + [self.W_hy, self.h0, self.b_y]\n",
    "        # Initial value for updates is zero matrix.\n",
    "        self.updates = {}\n",
    "        for param in self.params:\n",
    "            self.updates[param] = theano.shared(\n",
    "                                      value = np.zeros(\n",
    "                                                  param.get_value(\n",
    "                                                      borrow = True).shape,\n",
    "                                                  dtype = theano.config.floatX),\n",
    "                                      name = 'updates')\n",
    "        \n",
    "        # Default value of c_tm1 is None since we use it just when we have LSTM units\n",
    "        def recurrent_fn(u_t, h_tm1, c_tm1 = None):\n",
    "            # that's because LSTM needs both u_t and h_tm1 to compute gates\n",
    "            #if activation == 'lstm':\n",
    "            h_t = self.gru.gru_as_activation_function(u_t, h_tm1)\n",
    "            y_t = T.dot(h_t, self.W_hy) + self.b_y\n",
    "            c_t = h_t\n",
    "            return h_t, c_t, y_t\n",
    "\n",
    "        # Iteration over the first dimension of a tensor which is TIME in our case.\n",
    "        # recurrent_fn doesn't use y in the computations, so we do not need y0 (None)\n",
    "        # scan returns updates too which we do not need. (_)\n",
    "        [self.h, self.c, self.y_pred], _ = theano.scan(recurrent_fn,\n",
    "                                               sequences = self.x,\n",
    "                                               outputs_info = [self.h0, self.c0, None])\n",
    "\n",
    "        # L1 norm\n",
    "        self.L1 = abs(self.W_uh.sum()) + \\\n",
    "                  abs(self.W_hh.sum()) + \\\n",
    "                  abs(self.W_hy.sum())\n",
    "\n",
    "        # square of L2 norm\n",
    "        self.L2_sqr = (self.W_uh ** 2).sum() + \\\n",
    "                      (self.W_hh ** 2).sum() + \\\n",
    "                      (self.W_hy ** 2).sum()\n",
    "\n",
    "        # Loss function is different for different output types\n",
    "        # defining function in place is so easy! : lambda input: expresion\n",
    "        self.y = T.vector(name = 'y', dtype = 'int32')\n",
    "        self.p_y_given_x = T.nnet.softmax(self.y_pred)\n",
    "        self.y_out = T.argmax(self.p_y_given_x, axis = -1)\n",
    "        self.loss = lambda y: self.nll_multiclass(y)\n",
    "        self.predict_proba = theano.function(inputs = [self.x, ],\n",
    "                                             outputs = self.p_y_given_x,\n",
    "                                             mode = mode, allow_input_downcast=True)\n",
    "        self.predict = theano.function(inputs = [self.x, ],\n",
    "                                       outputs = self.y_out, # y-out is calculated by applying argmax\n",
    "                                       mode = mode, allow_input_downcast=True)\n",
    "        # Just for tracking training error for Graph 3\n",
    "        self.errors = []\n",
    "\n",
    "    def mse(self, y):\n",
    "        # mean is because of minibatch\n",
    "        return T.mean((self.y_pred - y) ** 2)\n",
    "\n",
    "    def nll_binary(self, y):\n",
    "        # negative log likelihood here is cross entropy\n",
    "        return T.mean(T.nnet.binary_crossentropy(self.p_y_given_x, y))\n",
    "\n",
    "    def nll_multiclass(self, y):\n",
    "        # notice to [  T.arange(y.shape[0])  ,  y  ]\n",
    "        return -T.mean(T.log(self.p_y_given_x)[T.arange(y.shape[0]), y])\n",
    "\n",
    "    # X_train, Y_train, X_test, and Y_test are numpy arrays\n",
    "    def build_trian(self, X_train, Y_train, X_test = None, Y_test = None):\n",
    "        train_set_x = theano.shared(np.asarray(X_train, dtype=theano.config.floatX))\n",
    "        train_set_y = theano.shared(np.asarray(Y_train, dtype=theano.config.floatX))\n",
    "        #if self.output_type in ('binary', 'softmax'):\n",
    "        train_set_y = T.cast(train_set_y, 'int32')\n",
    "\n",
    "        ######################\n",
    "        # BUILD ACTUAL MODEL #\n",
    "        ######################\n",
    "        print 'Buiding model ...'\n",
    "\n",
    "        index = T.lscalar('index')    # index to a case\n",
    "        # learning rate (may change)\n",
    "        lr = T.scalar('lr', dtype = theano.config.floatX)\n",
    "        mom = T.scalar('mom', dtype = theano.config.floatX)  # momentum\n",
    "\n",
    "\n",
    "        # Note that we use cost for training\n",
    "        # But, compute_train_error for just watching and printing\n",
    "        cost = self.loss(self.y) \\\n",
    "            + self.L1_reg * self.L1 \\\n",
    "            + self.L2_reg * self.L2_sqr\n",
    "\n",
    "        # We don't want to pass whole dataset every time we use this function.\n",
    "        # So, the solution is to put the dataset in the GPU as `givens`.\n",
    "        # And just pass index to the function each time as input.\n",
    "        compute_train_error = theano.function(inputs = [index, ],\n",
    "                                              outputs = self.loss(self.y),\n",
    "                                              givens = {\n",
    "                                                  self.x: train_set_x[index],\n",
    "                                                  self.y: train_set_y[index]},\n",
    "                                              mode = mode, allow_input_downcast=True)\n",
    "\n",
    "        # Gradients of cost wrt. [self.W, self.W_in, self.W_out,\n",
    "        # self.h0, self.b_h, self.b_y] using BPTT.\n",
    "        gparams = []\n",
    "        for param in self.params:\n",
    "            gparams.append(T.grad(cost, param))\n",
    "\n",
    "        # zip just concatenate two lists\n",
    "        updates = {}\n",
    "        for param, gparam in zip(self.params, gparams):\n",
    "            weight_update = self.updates[param]\n",
    "            upd = mom * weight_update - lr * gparam\n",
    "            updates[weight_update] = upd\n",
    "            updates[param] = param + upd\n",
    "\n",
    "        # compiling a Theano function `train_model` that returns the\n",
    "        # cost, but in the same time updates the parameter of the\n",
    "        # model based on the rules defined in `updates`\n",
    "        train_model = theano.function(inputs = [index, lr, mom],\n",
    "                                      outputs = cost,\n",
    "                                      updates = updates,\n",
    "                                      givens = {\n",
    "                                          self.x: train_set_x[index], # [:, batch_start:batch_stop]\n",
    "                                          self.y: train_set_y[index]},\n",
    "                                      mode = mode, allow_input_downcast=True)\n",
    "\n",
    "        ###############\n",
    "        # TRAIN MODEL #\n",
    "        ###############\n",
    "        print 'Training model ...'\n",
    "        epoch = 0\n",
    "        n_train = train_set_x.get_value(borrow = True).shape[0]\n",
    "\n",
    "        while (epoch < self.n_epochs):\n",
    "            epoch = epoch + 1\n",
    "            for idx in xrange(n_train):\n",
    "                effective_momentum = self.final_momentum \\\n",
    "                                     if epoch > self.momentum_switchover \\\n",
    "                                     else self.initial_momentum\n",
    "                example_cost = train_model(idx,\n",
    "                                           self.learning_rate,\n",
    "                                           effective_momentum)\n",
    "\n",
    "\n",
    "            # compute loss on training set\n",
    "            train_losses = [compute_train_error(i)\n",
    "                            for i in xrange(n_train)]\n",
    "            this_train_loss = np.mean(train_losses)\n",
    "            self.errors.append(this_train_loss)\n",
    "\n",
    "            print('epoch %i, train loss %f ''lr: %f' % \\\n",
    "                  (epoch, this_train_loss, self.learning_rate))\n",
    "            \n",
    "\n",
    "            self.learning_rate *= self.learning_rate_decay\n",
    "\"\"\"\n",
    "Here we define some testing functions.\n",
    "For more details see Graham Taylor model:\n",
    "https://github.com/gwtaylor/theano-rnn\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "Here we test the RNN with real output.\n",
    "We randomly generate `n_seq` sequences of length `time_steps`.\n",
    "Then we make a delay to get the targets. (+ adding some noise)\n",
    "Resulting graphs are saved under the name of `real.png`.\n",
    "\"\"\"\n",
    "\n",
    "def test_softmax(n_u = 4, n_h = 6, n_y = 7, time_steps = 4, n_seq= 27, n_epochs = 3000):\n",
    "    # n_y is equal to the number of calsses\n",
    "    print 'Testing model with softmax outputs'\n",
    "\n",
    "    np.random.seed(0)\n",
    "\n",
    "    model = RNN(n_u = n_u, n_h = n_h, n_y = n_y, \n",
    "                learning_rate = 0.07, learning_rate_decay = 0.999,\n",
    "                L1_reg = 0, L2_reg = 0, \n",
    "                initial_momentum = 0.4, final_momentum = 0.8,\n",
    "                momentum_switchover = 100,\n",
    "                n_epochs = n_epochs)\n",
    "\n",
    "    model.build_trian(seq, targets)\n",
    "\n",
    "    plt.close('all')\n",
    "    fig = plt.figure()\n",
    "    ax1 = plt.subplot(311)\n",
    "    plt.plot(seq[1])\n",
    "    plt.grid()\n",
    "    ax1.set_title('input')\n",
    "    ax2 = plt.subplot(312)\n",
    "\n",
    "    plt.scatter(xrange(time_steps), targets[1], marker = 'o', c = 'b')\n",
    "    plt.grid()\n",
    "    \n",
    "    guess = model.predict_proba(seq[1])\n",
    "    guessed_probs = plt.imshow(guess.T, interpolation = 'nearest', cmap = 'gray')\n",
    "    ax2.set_title('blue points: true class, grayscale: model output (white mean class)')\n",
    "\n",
    "    ax3 = plt.subplot(313)\n",
    "    plt.plot(model.errors)\n",
    "    plt.grid()\n",
    "    ax3.set_title('Training error')\n",
    "    plt.savefig('softmax_Epoch: ' + str(n_epochs) + '.png')\n",
    "    \n",
    "    print \"Image saved.\"\n",
    "    print model.predict([[3, 2, 1, 6],\n",
    "  [1, 7, 1, 5],\n",
    "  [4, 1, 1, 2],\n",
    "  [3, 4, 3, 1]])\n",
    "    \n",
    "    print model.predict([[3, 4, 5, 5],\n",
    "  [5, 4, 3, 4],\n",
    "  [1, 2, 3, 4],\n",
    "  [3, 2, 1, 1]])\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    t0 = time.time()\n",
    "    test_softmax()\n",
    "    print \"Elapsed time: %f\" % (time.time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
