{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5 1 5 1]\n",
      " [6 1 6 1]\n",
      " [4 2 4 2]\n",
      " [5 2 5 2]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "seq = np.array(\n",
    "[[[5, 5, 5, 3],\n",
    "  [1, 2, 3, 1],\n",
    "  [4, 2, 3, 2],\n",
    "  [2, 3, 4, 5]],\n",
    " [[5, 1, 5, 1],\n",
    "  [6, 1, 6, 1],\n",
    "  [4, 2, 4, 2],\n",
    "  [5, 2, 5, 2]],\n",
    " [[1, 3, 5, 1],\n",
    "  [1, 3, 6, 3],\n",
    "  [2, 4, 6, 4],\n",
    "  [2, 5, 7, 5]],\n",
    " [[1, 7, 1, 5],\n",
    "  [6, 5, 3, 1],\n",
    "  [4, 3, 2, 1],\n",
    "  [1, 7, 1, 2]],\n",
    " [[5, 1, 2, 1],\n",
    "  [1, 7, 1, 3],\n",
    "  [6, 7, 1, 2],\n",
    "  [3, 2, 3, 2]],\n",
    " [[3, 2, 3, 4],\n",
    "  [1, 1, 7, 6],\n",
    "  [6, 5, 4, 3],\n",
    "  [3, 2, 1, 7]],\n",
    " [[1, 2, 3, 1],\n",
    "  [3, 2, 1, 7],\n",
    "  [6, 7, 1, 2],\n",
    "  [3, 2, 1, 2]],\n",
    " [[1, 2, 3, 5],\n",
    "  [1, 2, 3, 1],\n",
    "  [4, 4, 3, 2],\n",
    "  [2, 3, 4, 5]],\n",
    " [[1, 7, 1, 2],\n",
    "  [3, 2, 1, 5],\n",
    "  [4, 3, 2, 3],\n",
    "  [2, 3, 2, 5]],\n",
    " [[6, 7, 1, 2],\n",
    "  [2, 1, 7, 5],\n",
    "  [4, 5, 6, 7],\n",
    "  [5, 6, 7, 1]],\n",
    " [[1, 2, 1, 3],\n",
    "  [2, 3, 2, 1],\n",
    "  [6, 7, 1, 2],\n",
    "  [3, 2, 3, 4]],\n",
    " [[3, 6, 6, 3],\n",
    "  [2, 5, 5, 2],\n",
    "  [1, 1, 2, 3],\n",
    "  [7, 7, 1, 1]],\n",
    " [[6, 3, 1, 6],\n",
    "  [5, 2, 7, 5],\n",
    "  [6, 4, 1, 6],\n",
    "  [7, 5, 2, 4]],\n",
    " [[6, 6, 7, 1],\n",
    "  [1, 7, 7, 5],\n",
    "  [6, 6, 7, 1],\n",
    "  [5, 5, 6, 6]],\n",
    " [[1, 2, 3, 6],\n",
    "  [7, 1, 2, 5],\n",
    "  [6, 7, 1, 4],\n",
    "  [5, 5, 6, 6]],\n",
    " [[3, 2, 1, 7],\n",
    "  [2, 1, 7, 6],\n",
    "  [6, 1, 6, 5],\n",
    "  [6, 7, 1, 1]],\n",
    " [[1, 7, 1, 2],\n",
    "  [3, 2, 1, 7],\n",
    "  [6, 7, 1, 2],\n",
    "  [3, 3, 2, 1]],\n",
    " [[6, 6, 6, 6],\n",
    "  [5, 5, 5, 5],\n",
    "  [4, 4, 4, 4],\n",
    "  [5, 5, 5, 4]],\n",
    " [[3, 4, 5, 1],\n",
    "  [5, 1, 3, 2],\n",
    "  [6, 7, 1, 2],\n",
    "  [3, 2, 3, 4]],\n",
    " [[1, 2, 3, 1],\n",
    "  [3, 4, 5, 3],\n",
    "  [4, 4, 3, 2],\n",
    "  [2, 3, 4, 5]],\n",
    " [[5, 3, 5, 3],\n",
    "  [5, 3, 1, 7],\n",
    "  [6, 1, 2, 3],\n",
    "  [3, 2, 3, 4]],\n",
    " [[5, 3, 5, 1],\n",
    "  [1, 7, 5, 3],\n",
    "  [6, 5, 5, 4],\n",
    "  [5, 5, 6, 6]],\n",
    " [[1, 1, 7, 1],\n",
    "  [5, 6, 7, 5],\n",
    "  [6, 5, 6, 7],\n",
    "  [1, 7, 1, 1]],\n",
    " [[3, 2, 3, 5],\n",
    "  [1, 7, 1, 5],\n",
    "  [6, 7, 1, 2],\n",
    "  [2, 3, 4, 4]],\n",
    " [[3, 3, 4, 3],\n",
    "  [5, 5, 6, 7],\n",
    "  [1, 1, 7, 1],\n",
    "  [5, 4, 3, 1]],\n",
    " [[5, 3, 1, 3],\n",
    "  [7, 5, 3, 5],\n",
    "  [6, 4, 1, 4],\n",
    "  [7, 5, 2, 4]],\n",
    " [[3, 4, 5, 5],\n",
    "  [5, 1, 7, 1],\n",
    "  [6, 5, 4, 3],\n",
    "  [3, 2, 1, 6]]])\n",
    "\n",
    "targets = np.array(\n",
    "[[1, 6, 2, 5],\n",
    " [1, 6, 2, 5],\n",
    " [1, 6, 2, 5],\n",
    " [1, 6, 2, 5],\n",
    " [1, 6, 2, 5],\n",
    " [1, 6, 2, 5],\n",
    " [1, 6, 2, 5],\n",
    " [1, 6, 2, 5],\n",
    " [1, 6, 2, 5],\n",
    " [6, 5, 4, 5],\n",
    " [6, 5, 4, 5],\n",
    " [6, 5, 4, 5],\n",
    " [6, 5, 4, 5],\n",
    " [6, 5, 4, 5],\n",
    " [6, 5, 4, 5],\n",
    " [6, 5, 4, 5],\n",
    " [6, 5, 4, 5],\n",
    " [6, 5, 4, 5],\n",
    " [1, 3, 4, 5],\n",
    " [1, 3, 4, 5],\n",
    " [1, 3, 4, 5],\n",
    " [1, 3, 4, 5],\n",
    " [1, 3, 4, 5],\n",
    " [1, 3, 4, 5],\n",
    " [1, 3, 4, 5],\n",
    " [1, 3, 4, 5],\n",
    " [1, 3, 4, 5]])\n",
    "\n",
    "print seq[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "Gated recurrent units (Long Short-Term Memory)\n",
    "Code provided by Mohammad Pezeshki - Nov. 2014 - Universite de Montreal\n",
    "This code is distributed without any warranty, express or implied.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "\n",
    "class LSTM(object):\n",
    "    def __init__(self, n_u, n_h):\n",
    "\n",
    "        self.n_u = int(n_u)\n",
    "        self.n_h = int(n_h)\n",
    "\n",
    "        # Here we define 8 weight matrices and 4 bias vector.\n",
    "        # To determine the dimension of weight matrices and bias vectors,\n",
    "        # we just need to have the following numbers: n_u, n_h\n",
    "        # (You can check it using the equations).\n",
    "        #\n",
    "        # Thus:\n",
    "        # <var>: <dimention>\n",
    "        #\n",
    "        # W_xi : n_h  x  n_u\n",
    "        # W_hi : n_h  x  n_h\n",
    "        # W_ci : n_h  x  n_h\n",
    "        # W_xf : n_h  x  n_u\n",
    "        # W_hf : n_h  x  n_h\n",
    "        # W_cf : n_h  x  n_h\n",
    "        # W_xc : n_h  x  n_u\n",
    "        # W_hc : n_h  x  n_h\n",
    "        #\n",
    "        # b_i : n_h  x  1 #Note: do not confuse b_i as the bias of input! It's the bias of input GATE\n",
    "        # b_f : n_h  x  1\n",
    "        # b_o : n_h  x  1\n",
    "        # b_c : n_h  x  1\n",
    "\n",
    "        # Input gate weights\n",
    "        self.W_xi = theano.shared(value = np.asarray(\n",
    "                                              np.random.uniform(\n",
    "                                                  size = (n_h, n_u),\n",
    "                                                  low = -.01, high = .01),\n",
    "                                              dtype = theano.config.floatX),\n",
    "                                  name = 'W_xi')\n",
    "        self.W_hi = theano.shared(value = np.asarray(\n",
    "                                              np.random.uniform(\n",
    "                                                  size = (n_h, n_h),\n",
    "                                                  low = -.01, high = .01),\n",
    "                                              dtype = theano.config.floatX),\n",
    "                                  name = 'W_hi')\n",
    "        self.W_ci = theano.shared(value = np.asarray(\n",
    "                                              np.random.uniform(\n",
    "                                                  size = (n_h, n_h),\n",
    "                                                  low = -.01, high = .01),\n",
    "                                              dtype = theano.config.floatX),\n",
    "                                  name = 'W_ci')\n",
    "\n",
    "        # Forget gate weights\n",
    "        self.W_xf = theano.shared(value = np.asarray(\n",
    "                                              np.random.uniform(\n",
    "                                                  size = (n_h, n_u),\n",
    "                                                  low = -.01, high = .01),\n",
    "                                              dtype = theano.config.floatX),\n",
    "                                  name = 'W_xf')\n",
    "        self.W_hf = theano.shared(value = np.asarray(\n",
    "                                              np.random.uniform(\n",
    "                                                  size = (n_h, n_h),\n",
    "                                                  low = -.01, high = .01),\n",
    "                                              dtype = theano.config.floatX),\n",
    "                                  name = 'W_hf')\n",
    "        self.W_cf = theano.shared(value = np.asarray(\n",
    "                                              np.random.uniform(\n",
    "                                                  size = (n_h, n_h),\n",
    "                                                  low = -.01, high = .01),\n",
    "                                              dtype = theano.config.floatX),\n",
    "                                  name = 'W_cf')\n",
    "\n",
    "        # Output gate weights\n",
    "        self.W_xo = theano.shared(value = np.asarray(\n",
    "                                              np.random.uniform(\n",
    "                                                  size = (n_h, n_u),\n",
    "                                                  low = -.01, high = .01),\n",
    "                                              dtype = theano.config.floatX),\n",
    "                                  name = 'W_xo')\n",
    "        self.W_ho = theano.shared(value = np.asarray(\n",
    "                                              np.random.uniform(\n",
    "                                                  size = (n_h, n_h),\n",
    "                                                  low = -.01, high = .01),\n",
    "                                              dtype = theano.config.floatX),\n",
    "                                  name = 'W_ho')\n",
    "        self.W_co = theano.shared(value = np.asarray(\n",
    "                                              np.random.uniform(\n",
    "                                                  size = (n_h, n_h),\n",
    "                                                  low = -.01, high = .01),\n",
    "                                              dtype = theano.config.floatX),\n",
    "                                  name = 'W_co')\n",
    "\n",
    "        # Cell weights\n",
    "        self.W_xc = theano.shared(value = np.asarray(\n",
    "                                              np.random.uniform(\n",
    "                                                  size = (n_h, n_u),\n",
    "                                                  low = -.01, high = .01),\n",
    "                                              dtype = theano.config.floatX),\n",
    "                                  name = 'W_xc')\n",
    "        self.W_hc = theano.shared(value = np.asarray(\n",
    "                                              np.random.uniform(\n",
    "                                                  size = (n_h, n_h),\n",
    "                                                  low = -.01, high = .01),\n",
    "                                              dtype = theano.config.floatX),\n",
    "                                  name = 'W_hc')\n",
    "\n",
    "        # Input gate bias\n",
    "        self.b_i = theano.shared(value = np.zeros(\n",
    "                                             (n_h, ),\n",
    "                                             dtype = theano.config.floatX),\n",
    "                                 name = 'b_i')\n",
    "\n",
    "        # Forget gate bias\n",
    "        self.b_f = theano.shared(value = np.zeros(\n",
    "                                             (n_h, ),\n",
    "                                             dtype = theano.config.floatX),\n",
    "                                 name = 'b_f')\n",
    "\n",
    "        # Output gate bias\n",
    "        self.b_o = theano.shared(value = np.zeros(\n",
    "                                             (n_h, ),\n",
    "                                             dtype = theano.config.floatX),\n",
    "                                 name = 'b_o')\n",
    "\n",
    "        # cell bias\n",
    "        self.b_c = theano.shared(value = np.zeros(\n",
    "                                             (n_h, ),\n",
    "                                             dtype = theano.config.floatX),\n",
    "                                 name = 'b_c')\n",
    "\n",
    "        self.params = [self.W_xi, self.W_hi, self.W_ci,\n",
    "                          self.W_xf, self.W_hf, self.W_cf,\n",
    "                          self.W_xo, self.W_ho, self.W_co,\n",
    "                          self.W_xc, self.W_hc,\n",
    "                          self.b_i, self.b_f, self.b_o,\n",
    "                          self.b_c]\n",
    "\n",
    "\n",
    "    def lstm_as_activation_function(self, x_t, h_tm1, c_tm1):\n",
    "        #print self.W_xi.get_value(borrow = True)\n",
    "        i_t = T.nnet.sigmoid(T.dot(self.W_xi, x_t) + \\\n",
    "                             T.dot(self.W_hi, h_tm1) + \\\n",
    "                             T.dot(self.W_ci, c_tm1) + \\\n",
    "                             self.b_i)\n",
    "        f_t = T.nnet.sigmoid(T.dot(self.W_xf, x_t) + \\\n",
    "                             T.dot(self.W_hf, h_tm1) + \\\n",
    "                             T.dot(self.W_cf, c_tm1) + \\\n",
    "                             self.b_f)\n",
    "        c_t = f_t * c_tm1 + i_t * \\\n",
    "                  T.tanh(T.dot(self.W_xc, x_t) + \\\n",
    "                         T.dot(self.W_hc, h_tm1) + \\\n",
    "                         self.b_c)\n",
    "        o_t = T.nnet.sigmoid(T.dot(self.W_xo, x_t) + \\\n",
    "                        T.dot(self.W_ho, h_tm1) + \\\n",
    "                        T.dot(self.W_co, c_t) + \\\n",
    "                        self.b_o)\n",
    "        h_t = o_t * T.tanh(c_t)\n",
    "\n",
    "        return h_t, c_t\n",
    "\n",
    "# Gated Recurrent Unit\n",
    "# Recently proposed: \n",
    "# K. Cho, B. van Merrienboer,\n",
    "# D. Bahdanau, and Y. Bengio.\n",
    "# On the properties of neural \n",
    "# machine translation: Encoder-decoder approaches. \n",
    "class GRU(object):\n",
    "    def __init__(self, n_u, n_h):\n",
    "\n",
    "        self.n_u = int(n_u)\n",
    "        self.n_h = int(n_h)\n",
    "\n",
    "        # Here we define 6 weight matrices and 3 bias vector.\n",
    "        # To determine the dimention of weight matrices and bias vectors,\n",
    "        # we just need to have the following numbers: n_u, n_h\n",
    "        # (You can check it using the equations).\n",
    "        #\n",
    "        # Thus:\n",
    "        # <var>: <dimention>\n",
    "        #\n",
    "        # W_xz : n_h  x  n_u\n",
    "        # W_hz : n_h  x  n_h\n",
    "        # W_xr : n_h  x  n_u\n",
    "        # W_hr : n_h  x  n_h\n",
    "        # W_xh : n_h  x  n_h\n",
    "        # W_hh : n_h  x  n_h\n",
    "        #\n",
    "        # b_z : n_h  x  1\n",
    "        # b_r : n_h  x  1\n",
    "        # b_h : n_h  x  1\n",
    "\n",
    "        # Update gate weights\n",
    "        self.W_xz = theano.shared(value = np.asarray(\n",
    "                                              np.random.uniform(\n",
    "                                                  size = (n_h, n_u),\n",
    "                                                  low = -.01, high = .01),\n",
    "                                              dtype = theano.config.floatX),\n",
    "                                  name = 'W_xz')\n",
    "        self.W_hz = theano.shared(value = np.asarray(\n",
    "                                              np.random.uniform(\n",
    "                                                  size = (n_h, n_h),\n",
    "                                                  low = -.01, high = .01),\n",
    "                                              dtype = theano.config.floatX),\n",
    "                                  name = 'W_hz')\n",
    "\n",
    "        # Reset gate weights\n",
    "        self.W_xr = theano.shared(value = np.asarray(\n",
    "                                              np.random.uniform(\n",
    "                                                  size = (n_h, n_u),\n",
    "                                                  low = -.01, high = .01),\n",
    "                                              dtype = theano.config.floatX),\n",
    "                                  name = 'W_xr')\n",
    "        self.W_hr = theano.shared(value = np.asarray(\n",
    "                                              np.random.uniform(\n",
    "                                                  size = (n_h, n_h),\n",
    "                                                  low = -.01, high = .01),\n",
    "                                              dtype = theano.config.floatX),\n",
    "                                  name = 'W_hr')\n",
    "\n",
    "        # Other weights :-)\n",
    "        self.W_xh = theano.shared(value = np.asarray(\n",
    "                                              np.random.uniform(\n",
    "                                                  size = (n_h, n_u),\n",
    "                                                  low = -.01, high = .01),\n",
    "                                              dtype = theano.config.floatX),\n",
    "                                  name = 'W_xh')\n",
    "        self.W_hh = theano.shared(value = np.asarray(\n",
    "                                              np.random.uniform(\n",
    "                                                  size = (n_h, n_h),\n",
    "                                                  low = -.01, high = .01),\n",
    "                                              dtype = theano.config.floatX),\n",
    "                                  name = 'W_hh')\n",
    "\n",
    "        # Update gate bias\n",
    "        self.b_z = theano.shared(value = np.zeros(\n",
    "                                             (n_h, ),\n",
    "                                             dtype = theano.config.floatX),\n",
    "                                 name = 'b_z')\n",
    "\n",
    "        # Reset gate bias\n",
    "        self.b_r = theano.shared(value = np.zeros(\n",
    "                                             (n_h, ),\n",
    "                                             dtype = theano.config.floatX),\n",
    "                                 name = 'b_r')\n",
    "\n",
    "        # Hidden layer bias\n",
    "        self.b_h = theano.shared(value = np.zeros(\n",
    "                                             (n_h, ),\n",
    "                                             dtype = theano.config.floatX),\n",
    "                                 name = 'b_h')\n",
    "\n",
    "        self.params = [self.W_xz, self.W_hz, self.W_xr, self.W_hr, \n",
    "                          self.W_xh, self.W_hh, self.b_z, self.b_r, \n",
    "                          self.b_h]\n",
    "\n",
    "\n",
    "    def gru_as_activation_function(self, x_t, h_tm1):\n",
    "        # update gate\n",
    "        z_t = T.nnet.sigmoid(T.dot(self.W_xz, x_t) + \\\n",
    "                             T.dot(self.W_hz, h_tm1) + \\\n",
    "                             self.b_z)\n",
    "        # reset gate\n",
    "        r_t = T.nnet.sigmoid(T.dot(self.W_xr, x_t) + \\\n",
    "                             T.dot(self.W_hr, h_tm1) + \\\n",
    "                             self.b_r)\n",
    "        # candidate h_t\n",
    "        can_h_t = T.tanh(T.dot(self.W_xh, x_t) + \\\n",
    "                         r_t * T.dot(self.W_hh, h_tm1) + \\\n",
    "                         self.b_h)\n",
    "        # h_t\n",
    "        h_t = (1 - z_t) * h_tm1 + z_t * can_h_t\n",
    "\n",
    "        return h_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing model with softmax outputs\n",
      "Buiding model ...\n",
      "Training model ...\n",
      "epoch 1, train loss 1.752924 lr: 0.070000\n",
      "epoch 2, train loss 1.685306 lr: 0.069930\n",
      "epoch 3, train loss 1.643456 lr: 0.069860\n",
      "epoch 4, train loss 1.600268 lr: 0.069790\n",
      "epoch 5, train loss 1.552204 lr: 0.069720\n",
      "epoch 6, train loss 1.497314 lr: 0.069651\n",
      "epoch 7, train loss 1.433867 lr: 0.069581\n",
      "epoch 8, train loss 1.362498 lr: 0.069511\n",
      "epoch 9, train loss 1.289573 lr: 0.069442\n",
      "epoch 10, train loss 1.225774 lr: 0.069373\n",
      "epoch 11, train loss 1.172825 lr: 0.069303\n",
      "epoch 12, train loss 1.127611 lr: 0.069234\n",
      "epoch 13, train loss 1.096652 lr: 0.069165\n",
      "epoch 14, train loss 1.078709 lr: 0.069095\n",
      "epoch 15, train loss 1.066548 lr: 0.069026\n",
      "epoch 16, train loss 1.059514 lr: 0.068957\n",
      "epoch 17, train loss 1.055513 lr: 0.068888\n",
      "epoch 18, train loss 1.051525 lr: 0.068819\n",
      "epoch 19, train loss 1.048064 lr: 0.068751\n",
      "epoch 20, train loss 1.046438 lr: 0.068682\n",
      "epoch 21, train loss 1.043776 lr: 0.068613\n",
      "epoch 22, train loss 1.045340 lr: 0.068545\n",
      "epoch 23, train loss 1.051093 lr: 0.068476\n",
      "epoch 24, train loss 1.054690 lr: 0.068408\n",
      "epoch 25, train loss 1.051732 lr: 0.068339\n",
      "epoch 26, train loss 1.043931 lr: 0.068271\n",
      "epoch 27, train loss 1.030055 lr: 0.068203\n",
      "epoch 28, train loss 1.006261 lr: 0.068134\n",
      "epoch 29, train loss 0.982089 lr: 0.068066\n",
      "epoch 30, train loss 0.965447 lr: 0.067998\n",
      "epoch 31, train loss 0.955223 lr: 0.067930\n",
      "epoch 32, train loss 0.948361 lr: 0.067862\n",
      "epoch 33, train loss 0.938915 lr: 0.067794\n",
      "epoch 34, train loss 0.935249 lr: 0.067727\n",
      "epoch 35, train loss 0.938741 lr: 0.067659\n",
      "epoch 36, train loss 0.948530 lr: 0.067591\n",
      "epoch 37, train loss 0.964173 lr: 0.067524\n",
      "epoch 38, train loss 0.980990 lr: 0.067456\n",
      "epoch 39, train loss 0.987846 lr: 0.067389\n",
      "epoch 40, train loss 0.991510 lr: 0.067321\n",
      "epoch 41, train loss 0.973896 lr: 0.067254\n",
      "epoch 42, train loss 0.980816 lr: 0.067187\n",
      "epoch 43, train loss 0.953811 lr: 0.067119\n",
      "epoch 44, train loss 0.929739 lr: 0.067052\n",
      "epoch 45, train loss 0.898026 lr: 0.066985\n",
      "epoch 46, train loss 0.872987 lr: 0.066918\n",
      "epoch 47, train loss 0.802444 lr: 0.066851\n",
      "epoch 48, train loss 0.741934 lr: 0.066785\n",
      "epoch 49, train loss 0.710972 lr: 0.066718\n",
      "epoch 50, train loss 0.679656 lr: 0.066651\n",
      "epoch 51, train loss 0.675898 lr: 0.066584\n",
      "epoch 52, train loss 0.657923 lr: 0.066518\n",
      "epoch 53, train loss 0.668142 lr: 0.066451\n",
      "epoch 54, train loss 0.698638 lr: 0.066385\n",
      "epoch 55, train loss 0.647497 lr: 0.066318\n",
      "epoch 56, train loss 0.596531 lr: 0.066252\n",
      "epoch 57, train loss 0.590935 lr: 0.066186\n",
      "epoch 58, train loss 0.643032 lr: 0.066120\n",
      "epoch 59, train loss 0.631703 lr: 0.066054\n",
      "epoch 60, train loss 0.637168 lr: 0.065988\n",
      "epoch 61, train loss 0.618312 lr: 0.065922\n",
      "epoch 62, train loss 0.621037 lr: 0.065856\n",
      "epoch 63, train loss 0.609434 lr: 0.065790\n",
      "epoch 64, train loss 0.597998 lr: 0.065724\n",
      "epoch 65, train loss 0.608765 lr: 0.065658\n",
      "epoch 66, train loss 0.595533 lr: 0.065593\n",
      "epoch 67, train loss 0.582667 lr: 0.065527\n",
      "epoch 68, train loss 0.604782 lr: 0.065461\n",
      "epoch 69, train loss 0.676090 lr: 0.065396\n",
      "epoch 70, train loss 0.629064 lr: 0.065331\n",
      "epoch 71, train loss 0.638855 lr: 0.065265\n",
      "epoch 72, train loss 1.661337 lr: 0.065200\n",
      "epoch 73, train loss 1.297727 lr: 0.065135\n",
      "epoch 74, train loss 1.153888 lr: 0.065070\n",
      "epoch 75, train loss 1.144694 lr: 0.065005\n",
      "epoch 76, train loss 1.134861 lr: 0.064940\n",
      "epoch 77, train loss 1.129795 lr: 0.064875\n",
      "epoch 78, train loss 1.124761 lr: 0.064810\n",
      "epoch 79, train loss 1.154815 lr: 0.064745\n",
      "epoch 80, train loss 1.132947 lr: 0.064680\n",
      "epoch 81, train loss 1.157662 lr: 0.064616\n",
      "epoch 82, train loss 1.143769 lr: 0.064551\n",
      "epoch 83, train loss 1.152458 lr: 0.064486\n",
      "epoch 84, train loss 1.151039 lr: 0.064422\n",
      "epoch 85, train loss 1.078344 lr: 0.064357\n",
      "epoch 86, train loss 1.113627 lr: 0.064293\n",
      "epoch 87, train loss 1.076024 lr: 0.064229\n",
      "epoch 88, train loss 1.060277 lr: 0.064165\n",
      "epoch 89, train loss 1.057306 lr: 0.064100\n",
      "epoch 90, train loss 1.034302 lr: 0.064036\n",
      "epoch 91, train loss 1.029406 lr: 0.063972\n",
      "epoch 92, train loss 1.023841 lr: 0.063908\n",
      "epoch 93, train loss 1.014036 lr: 0.063844\n",
      "epoch 94, train loss 0.990804 lr: 0.063781\n",
      "epoch 95, train loss 0.966679 lr: 0.063717\n",
      "epoch 96, train loss 0.948156 lr: 0.063653\n",
      "epoch 97, train loss 0.936664 lr: 0.063589\n",
      "epoch 98, train loss 0.979070 lr: 0.063526\n",
      "epoch 99, train loss 0.914938 lr: 0.063462\n",
      "epoch 100, train loss 0.932987 lr: 0.063399\n",
      "epoch 101, train loss 1.363914 lr: 0.063335\n",
      "epoch 102, train loss 1.540585 lr: 0.063272\n",
      "epoch 103, train loss 1.551809 lr: 0.063209\n",
      "epoch 104, train loss 1.562225 lr: 0.063146\n",
      "epoch 105, train loss 1.444871 lr: 0.063082\n",
      "epoch 106, train loss 1.582665 lr: 0.063019\n",
      "epoch 107, train loss 1.468976 lr: 0.062956\n",
      "epoch 108, train loss 1.460679 lr: 0.062893\n",
      "epoch 109, train loss 1.359256 lr: 0.062831\n",
      "epoch 110, train loss 1.493179 lr: 0.062768\n",
      "epoch 111, train loss 1.325582 lr: 0.062705\n",
      "epoch 112, train loss 1.313668 lr: 0.062642\n",
      "epoch 113, train loss 1.231367 lr: 0.062580\n",
      "epoch 114, train loss 1.175584 lr: 0.062517\n",
      "epoch 115, train loss 1.108882 lr: 0.062454\n",
      "epoch 116, train loss 1.144922 lr: 0.062392\n",
      "epoch 117, train loss 1.141244 lr: 0.062330\n",
      "epoch 118, train loss 1.286037 lr: 0.062267\n",
      "epoch 119, train loss 1.182281 lr: 0.062205\n",
      "epoch 120, train loss 1.229391 lr: 0.062143\n",
      "epoch 121, train loss 1.206796 lr: 0.062081\n",
      "epoch 122, train loss 1.255231 lr: 0.062019\n",
      "epoch 123, train loss 1.177907 lr: 0.061957\n",
      "epoch 124, train loss 1.109708 lr: 0.061895\n",
      "epoch 125, train loss 1.155657 lr: 0.061833\n",
      "epoch 126, train loss 1.087426 lr: 0.061771\n",
      "epoch 127, train loss 1.102240 lr: 0.061709\n",
      "epoch 128, train loss 1.113313 lr: 0.061647\n",
      "epoch 129, train loss 1.089442 lr: 0.061586\n",
      "epoch 130, train loss 1.109216 lr: 0.061524\n",
      "epoch 131, train loss 1.065630 lr: 0.061463\n",
      "epoch 132, train loss 1.113091 lr: 0.061401\n",
      "epoch 133, train loss 1.085918 lr: 0.061340\n",
      "epoch 134, train loss 1.101253 lr: 0.061278\n",
      "epoch 135, train loss 1.098817 lr: 0.061217\n",
      "epoch 136, train loss 1.102170 lr: 0.061156\n",
      "epoch 137, train loss 1.096061 lr: 0.061095\n",
      "epoch 138, train loss 1.093032 lr: 0.061034\n",
      "epoch 139, train loss 1.096002 lr: 0.060973\n",
      "epoch 140, train loss 1.098841 lr: 0.060912\n",
      "epoch 141, train loss 1.099744 lr: 0.060851\n",
      "epoch 142, train loss 1.099947 lr: 0.060790\n",
      "epoch 143, train loss 1.092252 lr: 0.060729\n",
      "epoch 144, train loss 1.082837 lr: 0.060668\n",
      "epoch 145, train loss 1.076291 lr: 0.060608\n",
      "epoch 146, train loss 1.071921 lr: 0.060547\n",
      "epoch 147, train loss 1.068503 lr: 0.060487\n",
      "epoch 148, train loss 1.065529 lr: 0.060426\n",
      "epoch 149, train loss 1.062807 lr: 0.060366\n",
      "epoch 150, train loss 1.060285 lr: 0.060305\n",
      "epoch 151, train loss 1.057899 lr: 0.060245\n",
      "epoch 152, train loss 1.055356 lr: 0.060185\n",
      "epoch 153, train loss 1.049148 lr: 0.060125\n",
      "epoch 154, train loss 1.032899 lr: 0.060064\n",
      "epoch 155, train loss 1.056784 lr: 0.060004\n",
      "epoch 156, train loss 1.056966 lr: 0.059944\n",
      "epoch 157, train loss 1.054060 lr: 0.059884\n",
      "epoch 158, train loss 1.054186 lr: 0.059825\n",
      "epoch 159, train loss 1.054259 lr: 0.059765\n",
      "epoch 160, train loss 1.054539 lr: 0.059705\n",
      "epoch 161, train loss 1.054527 lr: 0.059645\n",
      "epoch 162, train loss 1.054167 lr: 0.059586\n",
      "epoch 163, train loss 1.053528 lr: 0.059526\n",
      "epoch 164, train loss 1.052760 lr: 0.059467\n",
      "epoch 165, train loss 1.051938 lr: 0.059407\n",
      "epoch 166, train loss 1.051063 lr: 0.059348\n",
      "epoch 167, train loss 1.050106 lr: 0.059288\n",
      "epoch 168, train loss 1.049046 lr: 0.059229\n",
      "epoch 169, train loss 1.047872 lr: 0.059170\n",
      "epoch 170, train loss 1.046582 lr: 0.059111\n",
      "epoch 171, train loss 1.045177 lr: 0.059052\n",
      "epoch 172, train loss 1.043660 lr: 0.058992\n",
      "epoch 173, train loss 1.042032 lr: 0.058933\n",
      "epoch 174, train loss 1.040300 lr: 0.058875\n",
      "epoch 175, train loss 1.038468 lr: 0.058816\n",
      "epoch 176, train loss 1.036545 lr: 0.058757\n",
      "epoch 177, train loss 1.034540 lr: 0.058698\n",
      "epoch 178, train loss 1.032462 lr: 0.058639\n",
      "epoch 179, train loss 1.030321 lr: 0.058581\n",
      "epoch 180, train loss 1.028126 lr: 0.058522\n",
      "epoch 181, train loss 1.025887 lr: 0.058464\n",
      "epoch 182, train loss 1.023614 lr: 0.058405\n",
      "epoch 183, train loss 1.021314 lr: 0.058347\n",
      "epoch 184, train loss 1.018996 lr: 0.058288\n",
      "epoch 185, train loss 1.016667 lr: 0.058230\n",
      "epoch 186, train loss 1.014333 lr: 0.058172\n",
      "epoch 187, train loss 1.012001 lr: 0.058114\n",
      "epoch 188, train loss 1.009675 lr: 0.058056\n",
      "epoch 189, train loss 1.007360 lr: 0.057998\n",
      "epoch 190, train loss 1.005061 lr: 0.057940\n",
      "epoch 191, train loss 1.002779 lr: 0.057882\n",
      "epoch 192, train loss 1.000519 lr: 0.057824\n",
      "epoch 193, train loss 0.998283 lr: 0.057766\n",
      "epoch 194, train loss 0.996073 lr: 0.057708\n",
      "epoch 195, train loss 0.993891 lr: 0.057650\n",
      "epoch 196, train loss 0.991739 lr: 0.057593\n",
      "epoch 197, train loss 0.989618 lr: 0.057535\n",
      "epoch 198, train loss 0.987530 lr: 0.057478\n",
      "epoch 199, train loss 0.985476 lr: 0.057420\n",
      "epoch 200, train loss 0.983456 lr: 0.057363\n",
      "epoch 201, train loss 0.981473 lr: 0.057305\n",
      "epoch 202, train loss 0.979527 lr: 0.057248\n",
      "epoch 203, train loss 0.977618 lr: 0.057191\n",
      "epoch 204, train loss 0.975748 lr: 0.057134\n",
      "epoch 205, train loss 0.973917 lr: 0.057077\n",
      "epoch 206, train loss 0.972126 lr: 0.057019\n",
      "epoch 207, train loss 0.970376 lr: 0.056962\n",
      "epoch 208, train loss 0.968666 lr: 0.056905\n",
      "epoch 209, train loss 0.966999 lr: 0.056849\n",
      "epoch 210, train loss 0.965372 lr: 0.056792\n",
      "epoch 211, train loss 0.963788 lr: 0.056735\n",
      "epoch 212, train loss 0.962245 lr: 0.056678\n",
      "epoch 213, train loss 0.960745 lr: 0.056622\n",
      "epoch 214, train loss 0.959287 lr: 0.056565\n",
      "epoch 215, train loss 0.957871 lr: 0.056508\n",
      "epoch 216, train loss 0.956497 lr: 0.056452\n",
      "epoch 217, train loss 0.955165 lr: 0.056395\n",
      "epoch 218, train loss 0.953874 lr: 0.056339\n",
      "epoch 219, train loss 0.952624 lr: 0.056283\n",
      "epoch 220, train loss 0.951415 lr: 0.056226\n",
      "epoch 221, train loss 0.950245 lr: 0.056170\n",
      "epoch 222, train loss 0.949115 lr: 0.056114\n",
      "epoch 223, train loss 0.948023 lr: 0.056058\n",
      "epoch 224, train loss 0.946969 lr: 0.056002\n",
      "epoch 225, train loss 0.945952 lr: 0.055946\n",
      "epoch 226, train loss 0.944971 lr: 0.055890\n",
      "epoch 227, train loss 0.944026 lr: 0.055834\n",
      "epoch 228, train loss 0.943114 lr: 0.055778\n",
      "epoch 229, train loss 0.942236 lr: 0.055722\n",
      "epoch 230, train loss 0.941389 lr: 0.055667\n",
      "epoch 231, train loss 0.940574 lr: 0.055611\n",
      "epoch 232, train loss 0.939788 lr: 0.055555\n",
      "epoch 233, train loss 0.939032 lr: 0.055500\n",
      "epoch 234, train loss 0.938303 lr: 0.055444\n",
      "epoch 235, train loss 0.937600 lr: 0.055389\n",
      "epoch 236, train loss 0.936922 lr: 0.055333\n",
      "epoch 237, train loss 0.936269 lr: 0.055278\n",
      "epoch 238, train loss 0.935637 lr: 0.055223\n",
      "epoch 239, train loss 0.935028 lr: 0.055168\n",
      "epoch 240, train loss 0.934437 lr: 0.055112\n",
      "epoch 241, train loss 0.933866 lr: 0.055057\n",
      "epoch 242, train loss 0.933312 lr: 0.055002\n",
      "epoch 243, train loss 0.932775 lr: 0.054947\n",
      "epoch 244, train loss 0.932252 lr: 0.054892\n",
      "epoch 245, train loss 0.931743 lr: 0.054837\n",
      "epoch 246, train loss 0.931247 lr: 0.054783\n",
      "epoch 247, train loss 0.930761 lr: 0.054728\n",
      "epoch 248, train loss 0.930285 lr: 0.054673\n",
      "epoch 249, train loss 0.929817 lr: 0.054618\n",
      "epoch 250, train loss 0.929358 lr: 0.054564\n",
      "epoch 251, train loss 0.928904 lr: 0.054509\n",
      "epoch 252, train loss 0.928455 lr: 0.054455\n",
      "epoch 253, train loss 0.928011 lr: 0.054400\n",
      "epoch 254, train loss 0.927570 lr: 0.054346\n",
      "epoch 255, train loss 0.927130 lr: 0.054292\n",
      "epoch 256, train loss 0.926692 lr: 0.054237\n",
      "epoch 257, train loss 0.926255 lr: 0.054183\n",
      "epoch 258, train loss 0.925816 lr: 0.054129\n",
      "epoch 259, train loss 0.925376 lr: 0.054075\n",
      "epoch 260, train loss 0.924934 lr: 0.054021\n",
      "epoch 261, train loss 0.924490 lr: 0.053967\n",
      "epoch 262, train loss 0.924042 lr: 0.053913\n",
      "epoch 263, train loss 0.923591 lr: 0.053859\n",
      "epoch 264, train loss 0.923135 lr: 0.053805\n",
      "epoch 265, train loss 0.922674 lr: 0.053751\n",
      "epoch 266, train loss 0.922209 lr: 0.053697\n",
      "epoch 267, train loss 0.921738 lr: 0.053644\n",
      "epoch 268, train loss 0.921262 lr: 0.053590\n",
      "epoch 269, train loss 0.920780 lr: 0.053536\n",
      "epoch 270, train loss 0.920292 lr: 0.053483\n",
      "epoch 271, train loss 0.919798 lr: 0.053429\n",
      "epoch 272, train loss 0.919298 lr: 0.053376\n",
      "epoch 273, train loss 0.918793 lr: 0.053323\n",
      "epoch 274, train loss 0.918281 lr: 0.053269\n",
      "epoch 275, train loss 0.917765 lr: 0.053216\n",
      "epoch 276, train loss 0.917242 lr: 0.053163\n",
      "epoch 277, train loss 0.916714 lr: 0.053110\n",
      "epoch 278, train loss 0.916182 lr: 0.053056\n",
      "epoch 279, train loss 0.915644 lr: 0.053003\n",
      "epoch 280, train loss 0.915102 lr: 0.052950\n",
      "epoch 281, train loss 0.914555 lr: 0.052897\n",
      "epoch 282, train loss 0.914005 lr: 0.052845\n",
      "epoch 283, train loss 0.913451 lr: 0.052792\n",
      "epoch 284, train loss 0.912893 lr: 0.052739\n",
      "epoch 285, train loss 0.912332 lr: 0.052686\n",
      "epoch 286, train loss 0.911769 lr: 0.052633\n",
      "epoch 287, train loss 0.911203 lr: 0.052581\n",
      "epoch 288, train loss 0.910636 lr: 0.052528\n",
      "epoch 289, train loss 0.910067 lr: 0.052476\n",
      "epoch 290, train loss 0.909496 lr: 0.052423\n",
      "epoch 291, train loss 0.908925 lr: 0.052371\n",
      "epoch 292, train loss 0.908352 lr: 0.052318\n",
      "epoch 293, train loss 0.907779 lr: 0.052266\n",
      "epoch 294, train loss 0.907207 lr: 0.052214\n",
      "epoch 295, train loss 0.906635 lr: 0.052162\n",
      "epoch 296, train loss 0.906063 lr: 0.052110\n",
      "epoch 297, train loss 0.905492 lr: 0.052057\n",
      "epoch 298, train loss 0.904922 lr: 0.052005\n",
      "epoch 299, train loss 0.904354 lr: 0.051953\n",
      "epoch 300, train loss 0.903787 lr: 0.051901\n",
      "epoch 301, train loss 0.903221 lr: 0.051849\n",
      "epoch 302, train loss 0.902658 lr: 0.051798\n",
      "epoch 303, train loss 0.902097 lr: 0.051746\n",
      "epoch 304, train loss 0.901538 lr: 0.051694\n",
      "epoch 305, train loss 0.900982 lr: 0.051642\n",
      "epoch 306, train loss 0.900428 lr: 0.051591\n",
      "epoch 307, train loss 0.899877 lr: 0.051539\n",
      "epoch 308, train loss 0.899329 lr: 0.051488\n",
      "epoch 309, train loss 0.898784 lr: 0.051436\n",
      "epoch 310, train loss 0.898242 lr: 0.051385\n",
      "epoch 311, train loss 0.897702 lr: 0.051333\n",
      "epoch 312, train loss 0.897167 lr: 0.051282\n",
      "epoch 313, train loss 0.896634 lr: 0.051231\n",
      "epoch 314, train loss 0.896105 lr: 0.051179\n",
      "epoch 315, train loss 0.895579 lr: 0.051128\n",
      "epoch 316, train loss 0.895056 lr: 0.051077\n",
      "epoch 317, train loss 0.894537 lr: 0.051026\n",
      "epoch 318, train loss 0.894021 lr: 0.050975\n",
      "epoch 319, train loss 0.893508 lr: 0.050924\n",
      "epoch 320, train loss 0.892999 lr: 0.050873\n",
      "epoch 321, train loss 0.892493 lr: 0.050822\n",
      "epoch 322, train loss 0.891990 lr: 0.050771\n",
      "epoch 323, train loss 0.891491 lr: 0.050721\n",
      "epoch 324, train loss 0.890995 lr: 0.050670\n",
      "epoch 325, train loss 0.890502 lr: 0.050619\n",
      "epoch 326, train loss 0.890012 lr: 0.050569\n",
      "epoch 327, train loss 0.889526 lr: 0.050518\n",
      "epoch 328, train loss 0.889042 lr: 0.050468\n",
      "epoch 329, train loss 0.888562 lr: 0.050417\n",
      "epoch 330, train loss 0.888085 lr: 0.050367\n",
      "epoch 331, train loss 0.887610 lr: 0.050316\n",
      "epoch 332, train loss 0.887139 lr: 0.050266\n",
      "epoch 333, train loss 0.886671 lr: 0.050216\n",
      "epoch 334, train loss 0.886205 lr: 0.050166\n",
      "epoch 335, train loss 0.885742 lr: 0.050115\n",
      "epoch 336, train loss 0.885281 lr: 0.050065\n",
      "epoch 337, train loss 0.884823 lr: 0.050015\n",
      "epoch 338, train loss 0.884368 lr: 0.049965\n",
      "epoch 339, train loss 0.883915 lr: 0.049915\n",
      "epoch 340, train loss 0.883465 lr: 0.049865\n",
      "epoch 341, train loss 0.883016 lr: 0.049815\n",
      "epoch 342, train loss 0.882571 lr: 0.049766\n",
      "epoch 343, train loss 0.882127 lr: 0.049716\n",
      "epoch 344, train loss 0.881686 lr: 0.049666\n",
      "epoch 345, train loss 0.881247 lr: 0.049616\n",
      "epoch 346, train loss 0.880810 lr: 0.049567\n",
      "epoch 347, train loss 0.880375 lr: 0.049517\n",
      "epoch 348, train loss 0.879942 lr: 0.049468\n",
      "epoch 349, train loss 0.879511 lr: 0.049418\n",
      "epoch 350, train loss 0.879082 lr: 0.049369\n",
      "epoch 351, train loss 0.878654 lr: 0.049320\n",
      "epoch 352, train loss 0.878229 lr: 0.049270\n",
      "epoch 353, train loss 0.877805 lr: 0.049221\n",
      "epoch 354, train loss 0.877383 lr: 0.049172\n",
      "epoch 355, train loss 0.876962 lr: 0.049123\n",
      "epoch 356, train loss 0.876543 lr: 0.049073\n",
      "epoch 357, train loss 0.876125 lr: 0.049024\n",
      "epoch 358, train loss 0.875709 lr: 0.048975\n",
      "epoch 359, train loss 0.875295 lr: 0.048926\n",
      "epoch 360, train loss 0.874882 lr: 0.048877\n",
      "epoch 361, train loss 0.874470 lr: 0.048829\n",
      "epoch 362, train loss 0.874060 lr: 0.048780\n",
      "epoch 363, train loss 0.873651 lr: 0.048731\n",
      "epoch 364, train loss 0.873244 lr: 0.048682\n",
      "epoch 365, train loss 0.872837 lr: 0.048634\n",
      "epoch 366, train loss 0.872432 lr: 0.048585\n",
      "epoch 367, train loss 0.872028 lr: 0.048536\n",
      "epoch 368, train loss 0.871625 lr: 0.048488\n",
      "epoch 369, train loss 0.871223 lr: 0.048439\n",
      "epoch 370, train loss 0.870823 lr: 0.048391\n",
      "epoch 371, train loss 0.870423 lr: 0.048342\n",
      "epoch 372, train loss 0.870025 lr: 0.048294\n",
      "epoch 373, train loss 0.869627 lr: 0.048246\n",
      "epoch 374, train loss 0.869231 lr: 0.048198\n",
      "epoch 375, train loss 0.868835 lr: 0.048149\n",
      "epoch 376, train loss 0.868440 lr: 0.048101\n",
      "epoch 377, train loss 0.868047 lr: 0.048053\n",
      "epoch 378, train loss 0.867654 lr: 0.048005\n",
      "epoch 379, train loss 0.867262 lr: 0.047957\n",
      "epoch 380, train loss 0.866871 lr: 0.047909\n",
      "epoch 381, train loss 0.866481 lr: 0.047861\n",
      "epoch 382, train loss 0.866092 lr: 0.047813\n",
      "epoch 383, train loss 0.865703 lr: 0.047766\n",
      "epoch 384, train loss 0.865315 lr: 0.047718\n",
      "epoch 385, train loss 0.864928 lr: 0.047670\n",
      "epoch 386, train loss 0.864542 lr: 0.047622\n",
      "epoch 387, train loss 0.864156 lr: 0.047575\n",
      "epoch 388, train loss 0.863771 lr: 0.047527\n",
      "epoch 389, train loss 0.863387 lr: 0.047480\n",
      "epoch 390, train loss 0.863003 lr: 0.047432\n",
      "epoch 391, train loss 0.862620 lr: 0.047385\n",
      "epoch 392, train loss 0.862238 lr: 0.047337\n",
      "epoch 393, train loss 0.861856 lr: 0.047290\n",
      "epoch 394, train loss 0.861474 lr: 0.047243\n",
      "epoch 395, train loss 0.861094 lr: 0.047195\n",
      "epoch 396, train loss 0.860714 lr: 0.047148\n",
      "epoch 397, train loss 0.860334 lr: 0.047101\n",
      "epoch 398, train loss 0.859955 lr: 0.047054\n",
      "epoch 399, train loss 0.859576 lr: 0.047007\n",
      "epoch 400, train loss 0.859198 lr: 0.046960\n",
      "epoch 401, train loss 0.858820 lr: 0.046913\n",
      "epoch 402, train loss 0.858443 lr: 0.046866\n",
      "epoch 403, train loss 0.858066 lr: 0.046819\n",
      "epoch 404, train loss 0.857690 lr: 0.046772\n",
      "epoch 405, train loss 0.857314 lr: 0.046726\n",
      "epoch 406, train loss 0.856939 lr: 0.046679\n",
      "epoch 407, train loss 0.856563 lr: 0.046632\n",
      "epoch 408, train loss 0.856189 lr: 0.046586\n",
      "epoch 409, train loss 0.855814 lr: 0.046539\n",
      "epoch 410, train loss 0.855440 lr: 0.046492\n",
      "epoch 411, train loss 0.855066 lr: 0.046446\n",
      "epoch 412, train loss 0.854693 lr: 0.046400\n",
      "epoch 413, train loss 0.854319 lr: 0.046353\n",
      "epoch 414, train loss 0.853947 lr: 0.046307\n",
      "epoch 415, train loss 0.853574 lr: 0.046260\n",
      "epoch 416, train loss 0.853201 lr: 0.046214\n",
      "epoch 417, train loss 0.852829 lr: 0.046168\n",
      "epoch 418, train loss 0.852456 lr: 0.046122\n",
      "epoch 419, train loss 0.852085 lr: 0.046076\n",
      "epoch 420, train loss 0.851713 lr: 0.046030\n",
      "epoch 421, train loss 0.851341 lr: 0.045984\n",
      "epoch 422, train loss 0.850970 lr: 0.045938\n",
      "epoch 423, train loss 0.850598 lr: 0.045892\n",
      "epoch 424, train loss 0.850227 lr: 0.045846\n",
      "epoch 425, train loss 0.849856 lr: 0.045800\n",
      "epoch 426, train loss 0.849484 lr: 0.045754\n",
      "epoch 427, train loss 0.849113 lr: 0.045708\n",
      "epoch 428, train loss 0.848742 lr: 0.045663\n",
      "epoch 429, train loss 0.848371 lr: 0.045617\n",
      "epoch 430, train loss 0.847999 lr: 0.045571\n",
      "epoch 431, train loss 0.847628 lr: 0.045526\n",
      "epoch 432, train loss 0.847257 lr: 0.045480\n",
      "epoch 433, train loss 0.846885 lr: 0.045435\n",
      "epoch 434, train loss 0.846513 lr: 0.045389\n",
      "epoch 435, train loss 0.846142 lr: 0.045344\n",
      "epoch 436, train loss 0.845770 lr: 0.045299\n",
      "epoch 437, train loss 0.845397 lr: 0.045253\n",
      "epoch 438, train loss 0.845024 lr: 0.045208\n",
      "epoch 439, train loss 0.844651 lr: 0.045163\n",
      "epoch 440, train loss 0.844277 lr: 0.045118\n",
      "epoch 441, train loss 0.843903 lr: 0.045073\n",
      "epoch 442, train loss 0.843528 lr: 0.045028\n",
      "epoch 443, train loss 0.843153 lr: 0.044983\n",
      "epoch 444, train loss 0.842776 lr: 0.044938\n",
      "epoch 445, train loss 0.842399 lr: 0.044893\n",
      "epoch 446, train loss 0.842020 lr: 0.044848\n",
      "epoch 447, train loss 0.841640 lr: 0.044803\n",
      "epoch 448, train loss 0.841257 lr: 0.044758\n",
      "epoch 449, train loss 0.840873 lr: 0.044713\n",
      "epoch 450, train loss 0.840485 lr: 0.044669\n",
      "epoch 451, train loss 0.840094 lr: 0.044624\n",
      "epoch 452, train loss 0.839698 lr: 0.044579\n",
      "epoch 453, train loss 0.839297 lr: 0.044535\n",
      "epoch 454, train loss 0.838888 lr: 0.044490\n",
      "epoch 455, train loss 0.838468 lr: 0.044446\n",
      "epoch 456, train loss 0.838034 lr: 0.044401\n",
      "epoch 457, train loss 0.837578 lr: 0.044357\n",
      "epoch 458, train loss 0.837094 lr: 0.044312\n",
      "epoch 459, train loss 0.836568 lr: 0.044268\n",
      "epoch 460, train loss 0.835979 lr: 0.044224\n",
      "epoch 461, train loss 0.835304 lr: 0.044180\n",
      "epoch 462, train loss 0.834517 lr: 0.044136\n",
      "epoch 463, train loss 0.833630 lr: 0.044091\n",
      "epoch 464, train loss 0.832760 lr: 0.044047\n",
      "epoch 465, train loss 0.832170 lr: 0.044003\n",
      "epoch 466, train loss 0.832068 lr: 0.043959\n",
      "epoch 467, train loss 0.832372 lr: 0.043915\n",
      "epoch 468, train loss 0.832754 lr: 0.043871\n",
      "epoch 469, train loss 0.832864 lr: 0.043827\n",
      "epoch 470, train loss 0.832699 lr: 0.043784\n",
      "epoch 471, train loss 0.832436 lr: 0.043740\n",
      "epoch 472, train loss 0.832173 lr: 0.043696\n",
      "epoch 473, train loss 0.831930 lr: 0.043652\n",
      "epoch 474, train loss 0.831701 lr: 0.043609\n",
      "epoch 475, train loss 0.831482 lr: 0.043565\n",
      "epoch 476, train loss 0.831271 lr: 0.043522\n",
      "epoch 477, train loss 0.831074 lr: 0.043478\n",
      "epoch 478, train loss 0.830891 lr: 0.043435\n",
      "epoch 479, train loss 0.830726 lr: 0.043391\n",
      "epoch 480, train loss 0.830581 lr: 0.043348\n",
      "epoch 481, train loss 0.830461 lr: 0.043304\n",
      "epoch 482, train loss 0.830368 lr: 0.043261\n",
      "epoch 483, train loss 0.830306 lr: 0.043218\n",
      "epoch 484, train loss 0.830281 lr: 0.043175\n",
      "epoch 485, train loss 0.830299 lr: 0.043131\n",
      "epoch 486, train loss 0.830369 lr: 0.043088\n",
      "epoch 487, train loss 0.830509 lr: 0.043045\n",
      "epoch 488, train loss 0.830747 lr: 0.043002\n",
      "epoch 489, train loss 0.831133 lr: 0.042959\n",
      "epoch 490, train loss 0.831756 lr: 0.042916\n",
      "epoch 491, train loss 0.832745 lr: 0.042873\n",
      "epoch 492, train loss 0.834078 lr: 0.042830\n",
      "epoch 493, train loss 0.834935 lr: 0.042788\n",
      "epoch 494, train loss 0.834887 lr: 0.042745\n",
      "epoch 495, train loss 0.835350 lr: 0.042702\n",
      "epoch 496, train loss 0.831410 lr: 0.042659\n",
      "epoch 497, train loss 0.846247 lr: 0.042617\n",
      "epoch 498, train loss 0.794822 lr: 0.042574\n",
      "epoch 499, train loss 0.855531 lr: 0.042532\n",
      "epoch 500, train loss 0.820403 lr: 0.042489\n",
      "epoch 501, train loss 0.795995 lr: 0.042447\n",
      "epoch 502, train loss 0.934738 lr: 0.042404\n",
      "epoch 503, train loss 0.820906 lr: 0.042362\n",
      "epoch 504, train loss 0.842217 lr: 0.042319\n",
      "epoch 505, train loss 0.804933 lr: 0.042277\n",
      "epoch 506, train loss 0.842912 lr: 0.042235\n",
      "epoch 507, train loss 0.802513 lr: 0.042192\n",
      "epoch 508, train loss 0.832651 lr: 0.042150\n",
      "epoch 509, train loss 0.803480 lr: 0.042108\n",
      "epoch 510, train loss 0.796433 lr: 0.042066\n",
      "epoch 511, train loss 0.825317 lr: 0.042024\n",
      "epoch 512, train loss 0.803188 lr: 0.041982\n",
      "epoch 513, train loss 0.812916 lr: 0.041940\n",
      "epoch 514, train loss 0.804496 lr: 0.041898\n",
      "epoch 515, train loss 0.800801 lr: 0.041856\n",
      "epoch 516, train loss 0.797754 lr: 0.041814\n",
      "epoch 517, train loss 0.814325 lr: 0.041772\n",
      "epoch 518, train loss 0.803386 lr: 0.041731\n",
      "epoch 519, train loss 0.780112 lr: 0.041689\n",
      "epoch 520, train loss 0.790621 lr: 0.041647\n",
      "epoch 521, train loss 0.799951 lr: 0.041606\n",
      "epoch 522, train loss 0.794116 lr: 0.041564\n",
      "epoch 523, train loss 0.790189 lr: 0.041522\n",
      "epoch 524, train loss 0.791471 lr: 0.041481\n",
      "epoch 525, train loss 0.793521 lr: 0.041439\n",
      "epoch 526, train loss 0.795875 lr: 0.041398\n",
      "epoch 527, train loss 0.772161 lr: 0.041357\n",
      "epoch 528, train loss 0.781830 lr: 0.041315\n",
      "epoch 529, train loss 0.778916 lr: 0.041274\n",
      "epoch 530, train loss 0.769900 lr: 0.041233\n",
      "epoch 531, train loss 0.766895 lr: 0.041191\n",
      "epoch 532, train loss 0.765220 lr: 0.041150\n",
      "epoch 533, train loss 0.764473 lr: 0.041109\n",
      "epoch 534, train loss 0.763078 lr: 0.041068\n",
      "epoch 535, train loss 0.761851 lr: 0.041027\n",
      "epoch 536, train loss 0.760633 lr: 0.040986\n",
      "epoch 537, train loss 0.759458 lr: 0.040945\n",
      "epoch 538, train loss 0.758311 lr: 0.040904\n",
      "epoch 539, train loss 0.757190 lr: 0.040863\n",
      "epoch 540, train loss 0.756087 lr: 0.040822\n",
      "epoch 541, train loss 0.754996 lr: 0.040781\n",
      "epoch 542, train loss 0.753910 lr: 0.040741\n",
      "epoch 543, train loss 0.752825 lr: 0.040700\n",
      "epoch 544, train loss 0.751734 lr: 0.040659\n",
      "epoch 545, train loss 0.750633 lr: 0.040618\n",
      "epoch 546, train loss 0.749517 lr: 0.040578\n",
      "epoch 547, train loss 0.748381 lr: 0.040537\n",
      "epoch 548, train loss 0.747220 lr: 0.040497\n",
      "epoch 549, train loss 0.746031 lr: 0.040456\n",
      "epoch 550, train loss 0.744810 lr: 0.040416\n",
      "epoch 551, train loss 0.743554 lr: 0.040375\n",
      "epoch 552, train loss 0.742258 lr: 0.040335\n",
      "epoch 553, train loss 0.740921 lr: 0.040295\n",
      "epoch 554, train loss 0.739539 lr: 0.040254\n",
      "epoch 555, train loss 0.738112 lr: 0.040214\n",
      "epoch 556, train loss 0.736637 lr: 0.040174\n",
      "epoch 557, train loss 0.735114 lr: 0.040134\n",
      "epoch 558, train loss 0.733542 lr: 0.040094\n",
      "epoch 559, train loss 0.731924 lr: 0.040053\n",
      "epoch 560, train loss 0.730259 lr: 0.040013\n",
      "epoch 561, train loss 0.728549 lr: 0.039973\n",
      "epoch 562, train loss 0.726787 lr: 0.039933\n",
      "epoch 563, train loss 0.724932 lr: 0.039894\n",
      "epoch 564, train loss 0.722724 lr: 0.039854\n",
      "epoch 565, train loss 0.718059 lr: 0.039814\n",
      "epoch 566, train loss 0.719026 lr: 0.039774\n",
      "epoch 567, train loss 0.708386 lr: 0.039734\n",
      "epoch 568, train loss 0.747989 lr: 0.039694\n",
      "epoch 569, train loss 0.715754 lr: 0.039655\n",
      "epoch 570, train loss 0.712279 lr: 0.039615\n",
      "epoch 571, train loss 0.742856 lr: 0.039575\n",
      "epoch 572, train loss 0.710891 lr: 0.039536\n",
      "epoch 573, train loss 0.702238 lr: 0.039496\n",
      "epoch 574, train loss 0.692434 lr: 0.039457\n",
      "epoch 575, train loss 0.688378 lr: 0.039417\n",
      "epoch 576, train loss 0.688536 lr: 0.039378\n",
      "epoch 577, train loss 0.686885 lr: 0.039339\n",
      "epoch 578, train loss 0.686084 lr: 0.039299\n",
      "epoch 579, train loss 0.686537 lr: 0.039260\n",
      "epoch 580, train loss 0.687612 lr: 0.039221\n",
      "epoch 581, train loss 0.689749 lr: 0.039182\n",
      "epoch 582, train loss 0.693249 lr: 0.039142\n",
      "epoch 583, train loss 0.697911 lr: 0.039103\n",
      "epoch 584, train loss 0.703707 lr: 0.039064\n",
      "epoch 585, train loss 0.710635 lr: 0.039025\n",
      "epoch 586, train loss 0.718638 lr: 0.038986\n",
      "epoch 587, train loss 0.727593 lr: 0.038947\n",
      "epoch 588, train loss 0.737283 lr: 0.038908\n",
      "epoch 589, train loss 0.747383 lr: 0.038869\n",
      "epoch 590, train loss 0.757441 lr: 0.038830\n",
      "epoch 591, train loss 0.766861 lr: 0.038791\n",
      "epoch 592, train loss 0.774887 lr: 0.038753\n",
      "epoch 593, train loss 0.780593 lr: 0.038714\n",
      "epoch 594, train loss 0.782907 lr: 0.038675\n",
      "epoch 595, train loss 0.780716 lr: 0.038637\n",
      "epoch 596, train loss 0.773087 lr: 0.038598\n",
      "epoch 597, train loss 0.759622 lr: 0.038559\n",
      "epoch 598, train loss 0.740863 lr: 0.038521\n",
      "epoch 599, train loss 0.718590 lr: 0.038482\n",
      "epoch 600, train loss 0.695695 lr: 0.038444\n",
      "epoch 601, train loss 0.675471 lr: 0.038405\n",
      "epoch 602, train loss 0.660615 lr: 0.038367\n",
      "epoch 603, train loss 0.652418 lr: 0.038329\n",
      "epoch 604, train loss 0.650532 lr: 0.038290\n",
      "epoch 605, train loss 0.653436 lr: 0.038252\n",
      "epoch 606, train loss 0.659268 lr: 0.038214\n",
      "epoch 607, train loss 0.666455 lr: 0.038175\n",
      "epoch 608, train loss 0.673904 lr: 0.038137\n",
      "epoch 609, train loss 0.680941 lr: 0.038099\n",
      "epoch 610, train loss 0.687181 lr: 0.038061\n",
      "epoch 611, train loss 0.692427 lr: 0.038023\n",
      "epoch 612, train loss 0.696610 lr: 0.037985\n",
      "epoch 613, train loss 0.699740 lr: 0.037947\n",
      "epoch 614, train loss 0.701876 lr: 0.037909\n",
      "epoch 615, train loss 0.703106 lr: 0.037871\n",
      "epoch 616, train loss 0.703522 lr: 0.037833\n",
      "epoch 617, train loss 0.703219 lr: 0.037795\n",
      "epoch 618, train loss 0.702285 lr: 0.037758\n",
      "epoch 619, train loss 0.700802 lr: 0.037720\n",
      "epoch 620, train loss 0.698851 lr: 0.037682\n",
      "epoch 621, train loss 0.696503 lr: 0.037644\n",
      "epoch 622, train loss 0.693830 lr: 0.037607\n",
      "epoch 623, train loss 0.690899 lr: 0.037569\n",
      "epoch 624, train loss 0.687774 lr: 0.037532\n",
      "epoch 625, train loss 0.684513 lr: 0.037494\n",
      "epoch 626, train loss 0.681172 lr: 0.037457\n",
      "epoch 627, train loss 0.677804 lr: 0.037419\n",
      "epoch 628, train loss 0.674452 lr: 0.037382\n",
      "epoch 629, train loss 0.671160 lr: 0.037344\n",
      "epoch 630, train loss 0.667961 lr: 0.037307\n",
      "epoch 631, train loss 0.664886 lr: 0.037270\n",
      "epoch 632, train loss 0.661958 lr: 0.037232\n",
      "epoch 633, train loss 0.659195 lr: 0.037195\n",
      "epoch 634, train loss 0.656611 lr: 0.037158\n",
      "epoch 635, train loss 0.654213 lr: 0.037121\n",
      "epoch 636, train loss 0.652005 lr: 0.037084\n",
      "epoch 637, train loss 0.649985 lr: 0.037047\n",
      "epoch 638, train loss 0.648149 lr: 0.037010\n",
      "epoch 639, train loss 0.646491 lr: 0.036973\n",
      "epoch 640, train loss 0.644999 lr: 0.036936\n",
      "epoch 641, train loss 0.643661 lr: 0.036899\n",
      "epoch 642, train loss 0.642463 lr: 0.036862\n",
      "epoch 643, train loss 0.641389 lr: 0.036825\n",
      "epoch 644, train loss 0.640424 lr: 0.036788\n",
      "epoch 645, train loss 0.639551 lr: 0.036751\n",
      "epoch 646, train loss 0.638753 lr: 0.036715\n",
      "epoch 647, train loss 0.638016 lr: 0.036678\n",
      "epoch 648, train loss 0.637323 lr: 0.036641\n",
      "epoch 649, train loss 0.636661 lr: 0.036604\n",
      "epoch 650, train loss 0.636016 lr: 0.036568\n",
      "epoch 651, train loss 0.635379 lr: 0.036531\n",
      "epoch 652, train loss 0.634739 lr: 0.036495\n",
      "epoch 653, train loss 0.634089 lr: 0.036458\n",
      "epoch 654, train loss 0.633423 lr: 0.036422\n",
      "epoch 655, train loss 0.632740 lr: 0.036385\n",
      "epoch 656, train loss 0.632037 lr: 0.036349\n",
      "epoch 657, train loss 0.631317 lr: 0.036313\n",
      "epoch 658, train loss 0.630581 lr: 0.036276\n",
      "epoch 659, train loss 0.629838 lr: 0.036240\n",
      "epoch 660, train loss 0.629091 lr: 0.036204\n",
      "epoch 661, train loss 0.628354 lr: 0.036168\n",
      "epoch 662, train loss 0.627631 lr: 0.036131\n",
      "epoch 663, train loss 0.626949 lr: 0.036095\n",
      "epoch 664, train loss 0.626301 lr: 0.036059\n",
      "epoch 665, train loss 0.625739 lr: 0.036023\n",
      "epoch 666, train loss 0.625227 lr: 0.035987\n",
      "epoch 667, train loss 0.624869 lr: 0.035951\n",
      "epoch 668, train loss 0.624546 lr: 0.035915\n",
      "epoch 669, train loss 0.624489 lr: 0.035879\n",
      "epoch 670, train loss 0.624339 lr: 0.035843\n",
      "epoch 671, train loss 0.624702 lr: 0.035808\n",
      "epoch 672, train loss 0.624525 lr: 0.035772\n",
      "epoch 673, train loss 0.625588 lr: 0.035736\n",
      "epoch 674, train loss 0.624717 lr: 0.035700\n",
      "epoch 675, train loss 0.627454 lr: 0.035665\n",
      "epoch 676, train loss 0.624315 lr: 0.035629\n",
      "epoch 677, train loss 0.630622 lr: 0.035593\n",
      "epoch 678, train loss 0.624219 lr: 0.035558\n",
      "epoch 679, train loss 0.632256 lr: 0.035522\n",
      "epoch 680, train loss 0.625246 lr: 0.035487\n",
      "epoch 681, train loss 0.634667 lr: 0.035451\n",
      "epoch 682, train loss 0.626621 lr: 0.035416\n",
      "epoch 683, train loss 0.636371 lr: 0.035380\n",
      "epoch 684, train loss 0.628414 lr: 0.035345\n",
      "epoch 685, train loss 0.639209 lr: 0.035310\n",
      "epoch 686, train loss 0.630047 lr: 0.035274\n",
      "epoch 687, train loss 0.640178 lr: 0.035239\n",
      "epoch 688, train loss 0.631418 lr: 0.035204\n",
      "epoch 689, train loss 0.641573 lr: 0.035169\n",
      "epoch 690, train loss 0.633228 lr: 0.035133\n",
      "epoch 691, train loss 0.642434 lr: 0.035098\n",
      "epoch 692, train loss 0.634325 lr: 0.035063\n",
      "epoch 693, train loss 0.645885 lr: 0.035028\n",
      "epoch 694, train loss 0.635004 lr: 0.034993\n",
      "epoch 695, train loss 0.646173 lr: 0.034958\n",
      "epoch 696, train loss 0.635407 lr: 0.034923\n",
      "epoch 697, train loss 0.647057 lr: 0.034888\n",
      "epoch 698, train loss 0.636073 lr: 0.034853\n",
      "epoch 699, train loss 0.645624 lr: 0.034818\n",
      "epoch 700, train loss 0.636334 lr: 0.034784\n",
      "epoch 701, train loss 0.648353 lr: 0.034749\n",
      "epoch 702, train loss 0.634985 lr: 0.034714\n",
      "epoch 703, train loss 0.647346 lr: 0.034679\n",
      "epoch 704, train loss 0.634184 lr: 0.034645\n",
      "epoch 705, train loss 0.647999 lr: 0.034610\n",
      "epoch 706, train loss 0.633389 lr: 0.034575\n",
      "epoch 707, train loss 0.645099 lr: 0.034541\n",
      "epoch 708, train loss 0.634047 lr: 0.034506\n",
      "epoch 709, train loss 0.644266 lr: 0.034472\n",
      "epoch 710, train loss 0.630168 lr: 0.034437\n",
      "epoch 711, train loss 0.644947 lr: 0.034403\n",
      "epoch 712, train loss 0.628760 lr: 0.034368\n",
      "epoch 713, train loss 0.643742 lr: 0.034334\n",
      "epoch 714, train loss 0.626395 lr: 0.034300\n",
      "epoch 715, train loss 0.637010 lr: 0.034265\n",
      "epoch 716, train loss 0.629698 lr: 0.034231\n",
      "epoch 717, train loss 0.667348 lr: 0.034197\n",
      "epoch 718, train loss 0.816741 lr: 0.034163\n",
      "epoch 719, train loss 0.966957 lr: 0.034129\n",
      "epoch 720, train loss 0.809177 lr: 0.034094\n",
      "epoch 721, train loss 0.776791 lr: 0.034060\n",
      "epoch 722, train loss 0.755998 lr: 0.034026\n",
      "epoch 723, train loss 0.734337 lr: 0.033992\n",
      "epoch 724, train loss 0.711348 lr: 0.033958\n",
      "epoch 725, train loss 0.714773 lr: 0.033924\n",
      "epoch 726, train loss 0.688442 lr: 0.033890\n",
      "epoch 727, train loss 0.691783 lr: 0.033857\n",
      "epoch 728, train loss 0.680709 lr: 0.033823\n",
      "epoch 729, train loss 0.665229 lr: 0.033789\n",
      "epoch 730, train loss 0.659453 lr: 0.033755\n",
      "epoch 731, train loss 0.655249 lr: 0.033721\n",
      "epoch 732, train loss 0.653660 lr: 0.033688\n",
      "epoch 733, train loss 0.655231 lr: 0.033654\n",
      "epoch 734, train loss 0.659593 lr: 0.033620\n",
      "epoch 735, train loss 0.666242 lr: 0.033587\n",
      "epoch 736, train loss 0.674573 lr: 0.033553\n",
      "epoch 737, train loss 0.683854 lr: 0.033519\n",
      "epoch 738, train loss 0.693253 lr: 0.033486\n",
      "epoch 739, train loss 0.701889 lr: 0.033452\n",
      "epoch 740, train loss 0.708911 lr: 0.033419\n",
      "epoch 741, train loss 0.713599 lr: 0.033386\n",
      "epoch 742, train loss 0.715449 lr: 0.033352\n",
      "epoch 743, train loss 0.714243 lr: 0.033319\n",
      "epoch 744, train loss 0.710065 lr: 0.033286\n",
      "epoch 745, train loss 0.703272 lr: 0.033252\n",
      "epoch 746, train loss 0.694439 lr: 0.033219\n",
      "epoch 747, train loss 0.684281 lr: 0.033186\n",
      "epoch 748, train loss 0.673560 lr: 0.033153\n",
      "epoch 749, train loss 0.662994 lr: 0.033119\n",
      "epoch 750, train loss 0.653170 lr: 0.033086\n",
      "epoch 751, train loss 0.644497 lr: 0.033053\n",
      "epoch 752, train loss 0.637201 lr: 0.033020\n",
      "epoch 753, train loss 0.631357 lr: 0.032987\n",
      "epoch 754, train loss 0.626928 lr: 0.032954\n",
      "epoch 755, train loss 0.623810 lr: 0.032921\n",
      "epoch 756, train loss 0.621862 lr: 0.032888\n",
      "epoch 757, train loss 0.620923 lr: 0.032855\n",
      "epoch 758, train loss 0.620834 lr: 0.032823\n",
      "epoch 759, train loss 0.621437 lr: 0.032790\n",
      "epoch 760, train loss 0.622585 lr: 0.032757\n",
      "epoch 761, train loss 0.624145 lr: 0.032724\n",
      "epoch 762, train loss 0.626016 lr: 0.032691\n",
      "epoch 763, train loss 0.628141 lr: 0.032659\n",
      "epoch 764, train loss 0.630495 lr: 0.032626\n",
      "epoch 765, train loss 0.632980 lr: 0.032594\n",
      "epoch 766, train loss 0.635382 lr: 0.032561\n",
      "epoch 767, train loss 0.637537 lr: 0.032528\n",
      "epoch 768, train loss 0.639370 lr: 0.032496\n",
      "epoch 769, train loss 0.640823 lr: 0.032463\n",
      "epoch 770, train loss 0.641831 lr: 0.032431\n",
      "epoch 771, train loss 0.642300 lr: 0.032398\n",
      "epoch 772, train loss 0.642058 lr: 0.032366\n",
      "epoch 773, train loss 0.640806 lr: 0.032334\n",
      "epoch 774, train loss 0.638330 lr: 0.032301\n",
      "epoch 775, train loss 0.635388 lr: 0.032269\n",
      "epoch 776, train loss 0.633707 lr: 0.032237\n",
      "epoch 777, train loss 0.634870 lr: 0.032205\n",
      "epoch 778, train loss 0.668567 lr: 0.032172\n",
      "epoch 779, train loss 0.659412 lr: 0.032140\n",
      "epoch 780, train loss 0.650917 lr: 0.032108\n",
      "epoch 781, train loss 0.644538 lr: 0.032076\n",
      "epoch 782, train loss 0.641751 lr: 0.032044\n",
      "epoch 783, train loss 0.634886 lr: 0.032012\n",
      "epoch 784, train loss 0.628116 lr: 0.031980\n",
      "epoch 785, train loss 0.623900 lr: 0.031948\n",
      "epoch 786, train loss 0.620793 lr: 0.031916\n",
      "epoch 787, train loss 0.619014 lr: 0.031884\n",
      "epoch 788, train loss 0.619228 lr: 0.031852\n",
      "epoch 789, train loss 0.621071 lr: 0.031820\n",
      "epoch 790, train loss 0.625450 lr: 0.031788\n",
      "epoch 791, train loss 0.634008 lr: 0.031757\n",
      "epoch 792, train loss 0.637001 lr: 0.031725\n",
      "epoch 793, train loss 0.630970 lr: 0.031693\n",
      "epoch 794, train loss 0.625561 lr: 0.031661\n",
      "epoch 795, train loss 0.623957 lr: 0.031630\n",
      "epoch 796, train loss 0.634198 lr: 0.031598\n",
      "epoch 797, train loss 0.637387 lr: 0.031567\n",
      "epoch 798, train loss 0.631156 lr: 0.031535\n",
      "epoch 799, train loss 0.636179 lr: 0.031503\n",
      "epoch 800, train loss 0.635699 lr: 0.031472\n",
      "epoch 801, train loss 0.634309 lr: 0.031440\n",
      "epoch 802, train loss 0.636060 lr: 0.031409\n",
      "epoch 803, train loss 0.637396 lr: 0.031378\n",
      "epoch 804, train loss 0.635977 lr: 0.031346\n",
      "epoch 805, train loss 0.636324 lr: 0.031315\n",
      "epoch 806, train loss 0.637927 lr: 0.031284\n",
      "epoch 807, train loss 0.636405 lr: 0.031252\n",
      "epoch 808, train loss 0.634647 lr: 0.031221\n",
      "epoch 809, train loss 0.633452 lr: 0.031190\n",
      "epoch 810, train loss 0.628324 lr: 0.031159\n",
      "epoch 811, train loss 0.616856 lr: 0.031127\n",
      "epoch 812, train loss 0.609178 lr: 0.031096\n",
      "epoch 813, train loss 0.616649 lr: 0.031065\n",
      "epoch 814, train loss 0.627754 lr: 0.031034\n",
      "epoch 815, train loss 0.618203 lr: 0.031003\n",
      "epoch 816, train loss 0.602666 lr: 0.030972\n",
      "epoch 817, train loss 0.603678 lr: 0.030941\n",
      "epoch 818, train loss 0.602151 lr: 0.030910\n",
      "epoch 819, train loss 0.601884 lr: 0.030879\n",
      "epoch 820, train loss 0.600879 lr: 0.030848\n",
      "epoch 821, train loss 0.610211 lr: 0.030818\n",
      "epoch 822, train loss 0.678484 lr: 0.030787\n",
      "epoch 823, train loss 0.672051 lr: 0.030756\n",
      "epoch 824, train loss 0.742172 lr: 0.030725\n",
      "epoch 825, train loss 0.645266 lr: 0.030694\n",
      "epoch 826, train loss 0.620940 lr: 0.030664\n",
      "epoch 827, train loss 0.611792 lr: 0.030633\n",
      "epoch 828, train loss 0.845235 lr: 0.030602\n",
      "epoch 829, train loss 0.733919 lr: 0.030572\n",
      "epoch 830, train loss 0.722778 lr: 0.030541\n",
      "epoch 831, train loss 0.715290 lr: 0.030511\n",
      "epoch 832, train loss 0.705789 lr: 0.030480\n",
      "epoch 833, train loss 0.699809 lr: 0.030450\n",
      "epoch 834, train loss 0.692062 lr: 0.030419\n",
      "epoch 835, train loss 0.686357 lr: 0.030389\n",
      "epoch 836, train loss 0.679526 lr: 0.030359\n",
      "epoch 837, train loss 0.672743 lr: 0.030328\n",
      "epoch 838, train loss 0.665666 lr: 0.030298\n",
      "epoch 839, train loss 0.658841 lr: 0.030268\n",
      "epoch 840, train loss 0.652457 lr: 0.030237\n",
      "epoch 841, train loss 0.646782 lr: 0.030207\n",
      "epoch 842, train loss 0.641993 lr: 0.030177\n",
      "epoch 843, train loss 0.638232 lr: 0.030147\n",
      "epoch 844, train loss 0.635572 lr: 0.030117\n",
      "epoch 845, train loss 0.634019 lr: 0.030086\n",
      "epoch 846, train loss 0.633516 lr: 0.030056\n",
      "epoch 847, train loss 0.633945 lr: 0.030026\n",
      "epoch 848, train loss 0.635128 lr: 0.029996\n",
      "epoch 849, train loss 0.636799 lr: 0.029966\n",
      "epoch 850, train loss 0.638767 lr: 0.029936\n",
      "epoch 851, train loss 0.640454 lr: 0.029906\n",
      "epoch 852, train loss 0.639873 lr: 0.029876\n",
      "epoch 853, train loss 0.640521 lr: 0.029847\n",
      "epoch 854, train loss 0.642754 lr: 0.029817\n",
      "epoch 855, train loss 0.644461 lr: 0.029787\n",
      "epoch 856, train loss 0.644689 lr: 0.029757\n",
      "epoch 857, train loss 0.643423 lr: 0.029727\n",
      "epoch 858, train loss 0.641469 lr: 0.029698\n",
      "epoch 859, train loss 0.639352 lr: 0.029668\n",
      "epoch 860, train loss 0.636939 lr: 0.029638\n",
      "epoch 861, train loss 0.634160 lr: 0.029609\n",
      "epoch 862, train loss 0.630997 lr: 0.029579\n",
      "epoch 863, train loss 0.627904 lr: 0.029549\n",
      "epoch 864, train loss 0.625086 lr: 0.029520\n",
      "epoch 865, train loss 0.621515 lr: 0.029490\n",
      "epoch 866, train loss 0.618527 lr: 0.029461\n",
      "epoch 867, train loss 0.628141 lr: 0.029431\n",
      "epoch 868, train loss 0.629040 lr: 0.029402\n",
      "epoch 869, train loss 0.616238 lr: 0.029373\n",
      "epoch 870, train loss 0.618831 lr: 0.029343\n",
      "epoch 871, train loss 0.772094 lr: 0.029314\n",
      "epoch 872, train loss 0.620209 lr: 0.029285\n",
      "epoch 873, train loss 0.636833 lr: 0.029255\n",
      "epoch 874, train loss 0.628041 lr: 0.029226\n",
      "epoch 875, train loss 0.746813 lr: 0.029197\n",
      "epoch 876, train loss 0.651645 lr: 0.029168\n",
      "epoch 877, train loss 0.653767 lr: 0.029138\n",
      "epoch 878, train loss 0.649269 lr: 0.029109\n",
      "epoch 879, train loss 0.646937 lr: 0.029080\n",
      "epoch 880, train loss 0.645326 lr: 0.029051\n",
      "epoch 881, train loss 0.644448 lr: 0.029022\n",
      "epoch 882, train loss 0.644030 lr: 0.028993\n",
      "epoch 883, train loss 0.643625 lr: 0.028964\n",
      "epoch 884, train loss 0.643106 lr: 0.028935\n",
      "epoch 885, train loss 0.642584 lr: 0.028906\n",
      "epoch 886, train loss 0.642117 lr: 0.028877\n",
      "epoch 887, train loss 0.641716 lr: 0.028848\n",
      "epoch 888, train loss 0.641376 lr: 0.028819\n",
      "epoch 889, train loss 0.641088 lr: 0.028791\n",
      "epoch 890, train loss 0.640839 lr: 0.028762\n",
      "epoch 891, train loss 0.640621 lr: 0.028733\n",
      "epoch 892, train loss 0.640427 lr: 0.028704\n",
      "epoch 893, train loss 0.640252 lr: 0.028676\n",
      "epoch 894, train loss 0.640093 lr: 0.028647\n",
      "epoch 895, train loss 0.639945 lr: 0.028618\n",
      "epoch 896, train loss 0.639801 lr: 0.028590\n",
      "epoch 897, train loss 0.639656 lr: 0.028561\n",
      "epoch 898, train loss 0.639507 lr: 0.028533\n",
      "epoch 899, train loss 0.639349 lr: 0.028504\n",
      "epoch 900, train loss 0.639182 lr: 0.028476\n",
      "epoch 901, train loss 0.639005 lr: 0.028447\n",
      "epoch 902, train loss 0.638821 lr: 0.028419\n",
      "epoch 903, train loss 0.638628 lr: 0.028390\n",
      "epoch 904, train loss 0.638428 lr: 0.028362\n",
      "epoch 905, train loss 0.638221 lr: 0.028333\n",
      "epoch 906, train loss 0.638008 lr: 0.028305\n",
      "epoch 907, train loss 0.637788 lr: 0.028277\n",
      "epoch 908, train loss 0.637563 lr: 0.028249\n",
      "epoch 909, train loss 0.637332 lr: 0.028220\n",
      "epoch 910, train loss 0.637095 lr: 0.028192\n",
      "epoch 911, train loss 0.636853 lr: 0.028164\n",
      "epoch 912, train loss 0.636605 lr: 0.028136\n",
      "epoch 913, train loss 0.636351 lr: 0.028108\n",
      "epoch 914, train loss 0.636092 lr: 0.028079\n",
      "epoch 915, train loss 0.635828 lr: 0.028051\n",
      "epoch 916, train loss 0.635559 lr: 0.028023\n",
      "epoch 917, train loss 0.635284 lr: 0.027995\n",
      "epoch 918, train loss 0.635006 lr: 0.027967\n",
      "epoch 919, train loss 0.634722 lr: 0.027939\n",
      "epoch 920, train loss 0.634434 lr: 0.027911\n",
      "epoch 921, train loss 0.634143 lr: 0.027883\n",
      "epoch 922, train loss 0.633848 lr: 0.027856\n",
      "epoch 923, train loss 0.633550 lr: 0.027828\n",
      "epoch 924, train loss 0.633249 lr: 0.027800\n",
      "epoch 925, train loss 0.632946 lr: 0.027772\n",
      "epoch 926, train loss 0.632640 lr: 0.027744\n",
      "epoch 927, train loss 0.632334 lr: 0.027717\n",
      "epoch 928, train loss 0.632026 lr: 0.027689\n",
      "epoch 929, train loss 0.631717 lr: 0.027661\n",
      "epoch 930, train loss 0.631407 lr: 0.027634\n",
      "epoch 931, train loss 0.631095 lr: 0.027606\n",
      "epoch 932, train loss 0.630783 lr: 0.027578\n",
      "epoch 933, train loss 0.630468 lr: 0.027551\n",
      "epoch 934, train loss 0.630148 lr: 0.027523\n",
      "epoch 935, train loss 0.629822 lr: 0.027496\n",
      "epoch 936, train loss 0.629483 lr: 0.027468\n",
      "epoch 937, train loss 0.629119 lr: 0.027441\n",
      "epoch 938, train loss 0.628706 lr: 0.027413\n",
      "epoch 939, train loss 0.628171 lr: 0.027386\n",
      "epoch 940, train loss 0.627225 lr: 0.027358\n",
      "epoch 941, train loss 0.625662 lr: 0.027331\n",
      "epoch 942, train loss 0.642642 lr: 0.027304\n",
      "epoch 943, train loss 0.632398 lr: 0.027276\n",
      "epoch 944, train loss 0.625444 lr: 0.027249\n",
      "epoch 945, train loss 0.633063 lr: 0.027222\n",
      "epoch 946, train loss 0.613119 lr: 0.027195\n",
      "epoch 947, train loss 0.649140 lr: 0.027168\n",
      "epoch 948, train loss 0.624790 lr: 0.027140\n",
      "epoch 949, train loss 0.621079 lr: 0.027113\n",
      "epoch 950, train loss 0.616693 lr: 0.027086\n",
      "epoch 951, train loss 0.619044 lr: 0.027059\n",
      "epoch 952, train loss 0.600946 lr: 0.027032\n",
      "epoch 953, train loss 0.602644 lr: 0.027005\n",
      "epoch 954, train loss 0.602740 lr: 0.026978\n",
      "epoch 955, train loss 0.594410 lr: 0.026951\n",
      "epoch 956, train loss 0.602710 lr: 0.026924\n",
      "epoch 957, train loss 0.594454 lr: 0.026897\n",
      "epoch 958, train loss 0.613818 lr: 0.026870\n",
      "epoch 959, train loss 0.606726 lr: 0.026843\n",
      "epoch 960, train loss 0.601327 lr: 0.026816\n",
      "epoch 961, train loss 0.596950 lr: 0.026790\n",
      "epoch 962, train loss 0.601661 lr: 0.026763\n",
      "epoch 963, train loss 0.594467 lr: 0.026736\n",
      "epoch 964, train loss 0.613931 lr: 0.026709\n",
      "epoch 965, train loss 0.609422 lr: 0.026683\n",
      "epoch 966, train loss 0.595429 lr: 0.026656\n",
      "epoch 967, train loss 0.596109 lr: 0.026629\n",
      "epoch 968, train loss 0.595905 lr: 0.026603\n",
      "epoch 969, train loss 0.596791 lr: 0.026576\n",
      "epoch 970, train loss 0.605489 lr: 0.026549\n",
      "epoch 971, train loss 0.602466 lr: 0.026523\n",
      "epoch 972, train loss 0.595085 lr: 0.026496\n",
      "epoch 973, train loss 0.614312 lr: 0.026470\n",
      "epoch 974, train loss 0.612067 lr: 0.026443\n",
      "epoch 975, train loss 0.597881 lr: 0.026417\n",
      "epoch 976, train loss 0.610916 lr: 0.026391\n",
      "epoch 977, train loss 0.605067 lr: 0.026364\n",
      "epoch 978, train loss 0.596347 lr: 0.026338\n",
      "epoch 979, train loss 0.612995 lr: 0.026311\n",
      "epoch 980, train loss 0.604304 lr: 0.026285\n",
      "epoch 981, train loss 0.611611 lr: 0.026259\n",
      "epoch 982, train loss 0.656524 lr: 0.026233\n",
      "epoch 983, train loss 0.638955 lr: 0.026206\n",
      "epoch 984, train loss 0.623932 lr: 0.026180\n",
      "epoch 985, train loss 0.624333 lr: 0.026154\n",
      "epoch 986, train loss 0.610178 lr: 0.026128\n",
      "epoch 987, train loss 0.603415 lr: 0.026102\n",
      "epoch 988, train loss 0.603239 lr: 0.026076\n",
      "epoch 989, train loss 0.599132 lr: 0.026050\n",
      "epoch 990, train loss 0.622785 lr: 0.026024\n",
      "epoch 991, train loss 0.620584 lr: 0.025997\n",
      "epoch 992, train loss 0.614854 lr: 0.025971\n",
      "epoch 993, train loss 0.611512 lr: 0.025946\n",
      "epoch 994, train loss 0.621259 lr: 0.025920\n",
      "epoch 995, train loss 0.612114 lr: 0.025894\n",
      "epoch 996, train loss 0.607711 lr: 0.025868\n",
      "epoch 997, train loss 0.621015 lr: 0.025842\n",
      "epoch 998, train loss 0.616312 lr: 0.025816\n",
      "epoch 999, train loss 0.620171 lr: 0.025790\n",
      "epoch 1000, train loss 0.603696 lr: 0.025764\n",
      "epoch 1001, train loss 0.615176 lr: 0.025739\n",
      "epoch 1002, train loss 0.602465 lr: 0.025713\n",
      "epoch 1003, train loss 0.622812 lr: 0.025687\n",
      "epoch 1004, train loss 0.619093 lr: 0.025662\n",
      "epoch 1005, train loss 0.605650 lr: 0.025636\n",
      "epoch 1006, train loss 0.594278 lr: 0.025610\n",
      "epoch 1007, train loss 0.603251 lr: 0.025585\n",
      "epoch 1008, train loss 0.593481 lr: 0.025559\n",
      "epoch 1009, train loss 0.609203 lr: 0.025533\n",
      "epoch 1010, train loss 0.594734 lr: 0.025508\n",
      "epoch 1011, train loss 0.606697 lr: 0.025482\n",
      "epoch 1012, train loss 0.592174 lr: 0.025457\n",
      "epoch 1013, train loss 0.604250 lr: 0.025432\n",
      "epoch 1014, train loss 0.591001 lr: 0.025406\n",
      "epoch 1015, train loss 0.600308 lr: 0.025381\n",
      "epoch 1016, train loss 0.589854 lr: 0.025355\n",
      "epoch 1017, train loss 0.598131 lr: 0.025330\n",
      "epoch 1018, train loss 0.649644 lr: 0.025305\n",
      "epoch 1019, train loss 0.621378 lr: 0.025279\n",
      "epoch 1020, train loss 0.623991 lr: 0.025254\n",
      "epoch 1021, train loss 0.636625 lr: 0.025229\n",
      "epoch 1022, train loss 0.602858 lr: 0.025204\n",
      "epoch 1023, train loss 0.617449 lr: 0.025178\n",
      "epoch 1024, train loss 0.601928 lr: 0.025153\n",
      "epoch 1025, train loss 0.617242 lr: 0.025128\n",
      "epoch 1026, train loss 0.601825 lr: 0.025103\n",
      "epoch 1027, train loss 0.618120 lr: 0.025078\n",
      "epoch 1028, train loss 0.601588 lr: 0.025053\n",
      "epoch 1029, train loss 0.614571 lr: 0.025028\n",
      "epoch 1030, train loss 0.625576 lr: 0.025003\n",
      "epoch 1031, train loss 0.615364 lr: 0.024978\n",
      "epoch 1032, train loss 0.617870 lr: 0.024953\n",
      "epoch 1033, train loss 0.648137 lr: 0.024928\n",
      "epoch 1034, train loss 0.638831 lr: 0.024903\n",
      "epoch 1035, train loss 0.640577 lr: 0.024878\n",
      "epoch 1036, train loss 0.639068 lr: 0.024853\n",
      "epoch 1037, train loss 0.636753 lr: 0.024828\n",
      "epoch 1038, train loss 0.634577 lr: 0.024803\n",
      "epoch 1039, train loss 0.632683 lr: 0.024778\n",
      "epoch 1040, train loss 0.631276 lr: 0.024754\n",
      "epoch 1041, train loss 0.630502 lr: 0.024729\n",
      "epoch 1042, train loss 0.629923 lr: 0.024704\n",
      "epoch 1043, train loss 0.629121 lr: 0.024680\n",
      "epoch 1044, train loss 0.628280 lr: 0.024655\n",
      "epoch 1045, train loss 0.627468 lr: 0.024630\n",
      "epoch 1046, train loss 0.626698 lr: 0.024606\n",
      "epoch 1047, train loss 0.625975 lr: 0.024581\n",
      "epoch 1048, train loss 0.625300 lr: 0.024556\n",
      "epoch 1049, train loss 0.624674 lr: 0.024532\n",
      "epoch 1050, train loss 0.624095 lr: 0.024507\n",
      "epoch 1051, train loss 0.623568 lr: 0.024483\n",
      "epoch 1052, train loss 0.623096 lr: 0.024458\n",
      "epoch 1053, train loss 0.622688 lr: 0.024434\n",
      "epoch 1054, train loss 0.622352 lr: 0.024409\n",
      "epoch 1055, train loss 0.622085 lr: 0.024385\n",
      "epoch 1056, train loss 0.621855 lr: 0.024361\n",
      "epoch 1057, train loss 0.621615 lr: 0.024336\n",
      "epoch 1058, train loss 0.621343 lr: 0.024312\n",
      "epoch 1059, train loss 0.621073 lr: 0.024288\n",
      "epoch 1060, train loss 0.620860 lr: 0.024263\n",
      "epoch 1061, train loss 0.620740 lr: 0.024239\n",
      "epoch 1062, train loss 0.620729 lr: 0.024215\n",
      "epoch 1063, train loss 0.620824 lr: 0.024191\n",
      "epoch 1064, train loss 0.621021 lr: 0.024166\n",
      "epoch 1065, train loss 0.621310 lr: 0.024142\n",
      "epoch 1066, train loss 0.621687 lr: 0.024118\n",
      "epoch 1067, train loss 0.622143 lr: 0.024094\n",
      "epoch 1068, train loss 0.622673 lr: 0.024070\n",
      "epoch 1069, train loss 0.623270 lr: 0.024046\n",
      "epoch 1070, train loss 0.623928 lr: 0.024022\n",
      "epoch 1071, train loss 0.624640 lr: 0.023998\n",
      "epoch 1072, train loss 0.625398 lr: 0.023974\n",
      "epoch 1073, train loss 0.626194 lr: 0.023950\n",
      "epoch 1074, train loss 0.627020 lr: 0.023926\n",
      "epoch 1075, train loss 0.627867 lr: 0.023902\n",
      "epoch 1076, train loss 0.628726 lr: 0.023878\n",
      "epoch 1077, train loss 0.629587 lr: 0.023854\n",
      "epoch 1078, train loss 0.630441 lr: 0.023830\n",
      "epoch 1079, train loss 0.631277 lr: 0.023806\n",
      "epoch 1080, train loss 0.632085 lr: 0.023783\n",
      "epoch 1081, train loss 0.632855 lr: 0.023759\n",
      "epoch 1082, train loss 0.633577 lr: 0.023735\n",
      "epoch 1083, train loss 0.634243 lr: 0.023711\n",
      "epoch 1084, train loss 0.634843 lr: 0.023688\n",
      "epoch 1085, train loss 0.635369 lr: 0.023664\n",
      "epoch 1086, train loss 0.635813 lr: 0.023640\n",
      "epoch 1087, train loss 0.636167 lr: 0.023617\n",
      "epoch 1088, train loss 0.636427 lr: 0.023593\n",
      "epoch 1089, train loss 0.636586 lr: 0.023569\n",
      "epoch 1090, train loss 0.636639 lr: 0.023546\n",
      "epoch 1091, train loss 0.636584 lr: 0.023522\n",
      "epoch 1092, train loss 0.636417 lr: 0.023499\n",
      "epoch 1093, train loss 0.636135 lr: 0.023475\n",
      "epoch 1094, train loss 0.635736 lr: 0.023452\n",
      "epoch 1095, train loss 0.635213 lr: 0.023428\n",
      "epoch 1096, train loss 0.634556 lr: 0.023405\n",
      "epoch 1097, train loss 0.633734 lr: 0.023382\n",
      "epoch 1098, train loss 0.632637 lr: 0.023358\n",
      "epoch 1099, train loss 0.630612 lr: 0.023335\n",
      "epoch 1100, train loss 0.621933 lr: 0.023311\n",
      "epoch 1101, train loss 0.644291 lr: 0.023288\n",
      "epoch 1102, train loss 0.624402 lr: 0.023265\n",
      "epoch 1103, train loss 0.618625 lr: 0.023242\n",
      "epoch 1104, train loss 0.613840 lr: 0.023218\n",
      "epoch 1105, train loss 0.607347 lr: 0.023195\n",
      "epoch 1106, train loss 0.601750 lr: 0.023172\n",
      "epoch 1107, train loss 0.597488 lr: 0.023149\n",
      "epoch 1108, train loss 0.594129 lr: 0.023126\n",
      "epoch 1109, train loss 0.591374 lr: 0.023103\n",
      "epoch 1110, train loss 0.589040 lr: 0.023079\n",
      "epoch 1111, train loss 0.587109 lr: 0.023056\n",
      "epoch 1112, train loss 0.585596 lr: 0.023033\n",
      "epoch 1113, train loss 0.585918 lr: 0.023010\n",
      "epoch 1114, train loss 0.585001 lr: 0.022987\n",
      "epoch 1115, train loss 0.582313 lr: 0.022964\n",
      "epoch 1116, train loss 0.581714 lr: 0.022941\n",
      "epoch 1117, train loss 0.581414 lr: 0.022918\n",
      "epoch 1118, train loss 0.580008 lr: 0.022895\n",
      "epoch 1119, train loss 0.580040 lr: 0.022873\n",
      "epoch 1120, train loss 0.578924 lr: 0.022850\n",
      "epoch 1121, train loss 0.578749 lr: 0.022827\n",
      "epoch 1122, train loss 0.578742 lr: 0.022804\n",
      "epoch 1123, train loss 0.578176 lr: 0.022781\n",
      "epoch 1124, train loss 0.578105 lr: 0.022758\n",
      "epoch 1125, train loss 0.578212 lr: 0.022736\n",
      "epoch 1126, train loss 0.577976 lr: 0.022713\n",
      "epoch 1127, train loss 0.578013 lr: 0.022690\n",
      "epoch 1128, train loss 0.578175 lr: 0.022667\n",
      "epoch 1129, train loss 0.578223 lr: 0.022645\n",
      "epoch 1130, train loss 0.578350 lr: 0.022622\n",
      "epoch 1131, train loss 0.578519 lr: 0.022600\n",
      "epoch 1132, train loss 0.578714 lr: 0.022577\n",
      "epoch 1133, train loss 0.578931 lr: 0.022554\n",
      "epoch 1134, train loss 0.579169 lr: 0.022532\n",
      "epoch 1135, train loss 0.579433 lr: 0.022509\n",
      "epoch 1136, train loss 0.579718 lr: 0.022487\n",
      "epoch 1137, train loss 0.580013 lr: 0.022464\n",
      "epoch 1138, train loss 0.580310 lr: 0.022442\n",
      "epoch 1139, train loss 0.580608 lr: 0.022419\n",
      "epoch 1140, train loss 0.580907 lr: 0.022397\n",
      "epoch 1141, train loss 0.581234 lr: 0.022375\n",
      "epoch 1142, train loss 0.581614 lr: 0.022352\n",
      "epoch 1143, train loss 0.581996 lr: 0.022330\n",
      "epoch 1144, train loss 0.582394 lr: 0.022308\n",
      "epoch 1145, train loss 0.583361 lr: 0.022285\n",
      "epoch 1146, train loss 0.585069 lr: 0.022263\n",
      "epoch 1147, train loss 0.583616 lr: 0.022241\n",
      "epoch 1148, train loss 0.584122 lr: 0.022218\n",
      "epoch 1149, train loss 0.584678 lr: 0.022196\n",
      "epoch 1150, train loss 0.585286 lr: 0.022174\n",
      "epoch 1151, train loss 0.585235 lr: 0.022152\n",
      "epoch 1152, train loss 0.585716 lr: 0.022130\n",
      "epoch 1153, train loss 0.585896 lr: 0.022108\n",
      "epoch 1154, train loss 0.586251 lr: 0.022085\n",
      "epoch 1155, train loss 0.586689 lr: 0.022063\n",
      "epoch 1156, train loss 0.587166 lr: 0.022041\n",
      "epoch 1157, train loss 0.588346 lr: 0.022019\n",
      "epoch 1158, train loss 0.589897 lr: 0.021997\n",
      "epoch 1159, train loss 0.588964 lr: 0.021975\n",
      "epoch 1160, train loss 0.588688 lr: 0.021953\n",
      "epoch 1161, train loss 0.588794 lr: 0.021931\n",
      "epoch 1162, train loss 0.589240 lr: 0.021909\n",
      "epoch 1163, train loss 0.589675 lr: 0.021887\n",
      "epoch 1164, train loss 0.590757 lr: 0.021866\n",
      "epoch 1165, train loss 0.591909 lr: 0.021844\n",
      "epoch 1166, train loss 0.590432 lr: 0.021822\n",
      "epoch 1167, train loss 0.590629 lr: 0.021800\n",
      "epoch 1168, train loss 0.590906 lr: 0.021778\n",
      "epoch 1169, train loss 0.591469 lr: 0.021756\n",
      "epoch 1170, train loss 0.592603 lr: 0.021735\n",
      "epoch 1171, train loss 0.592237 lr: 0.021713\n",
      "epoch 1172, train loss 0.592436 lr: 0.021691\n",
      "epoch 1173, train loss 0.592141 lr: 0.021670\n",
      "epoch 1174, train loss 0.591881 lr: 0.021648\n",
      "epoch 1175, train loss 0.592021 lr: 0.021626\n",
      "epoch 1176, train loss 0.592454 lr: 0.021605\n",
      "epoch 1177, train loss 0.594260 lr: 0.021583\n",
      "epoch 1178, train loss 0.593688 lr: 0.021561\n",
      "epoch 1179, train loss 0.594372 lr: 0.021540\n",
      "epoch 1180, train loss 0.594276 lr: 0.021518\n",
      "epoch 1181, train loss 0.595552 lr: 0.021497\n",
      "epoch 1182, train loss 0.599066 lr: 0.021475\n",
      "epoch 1183, train loss 0.600130 lr: 0.021454\n",
      "epoch 1184, train loss 0.595620 lr: 0.021432\n",
      "epoch 1185, train loss 0.603032 lr: 0.021411\n",
      "epoch 1186, train loss 0.618132 lr: 0.021390\n",
      "epoch 1187, train loss 0.624773 lr: 0.021368\n",
      "epoch 1188, train loss 0.632682 lr: 0.021347\n",
      "epoch 1189, train loss 0.638487 lr: 0.021325\n",
      "epoch 1190, train loss 0.656492 lr: 0.021304\n",
      "epoch 1191, train loss 0.670794 lr: 0.021283\n",
      "epoch 1192, train loss 0.666808 lr: 0.021262\n",
      "epoch 1193, train loss 0.621028 lr: 0.021240\n",
      "epoch 1194, train loss 0.608894 lr: 0.021219\n",
      "epoch 1195, train loss 0.615988 lr: 0.021198\n",
      "epoch 1196, train loss 0.607236 lr: 0.021177\n",
      "epoch 1197, train loss 0.637394 lr: 0.021155\n",
      "epoch 1198, train loss 0.639494 lr: 0.021134\n",
      "epoch 1199, train loss 0.668041 lr: 0.021113\n",
      "epoch 1200, train loss 0.607069 lr: 0.021092\n",
      "epoch 1201, train loss 0.607846 lr: 0.021071\n",
      "epoch 1202, train loss 0.607115 lr: 0.021050\n",
      "epoch 1203, train loss 0.598239 lr: 0.021029\n",
      "epoch 1204, train loss 0.600560 lr: 0.021008\n",
      "epoch 1205, train loss 0.595801 lr: 0.020987\n",
      "epoch 1206, train loss 0.598357 lr: 0.020966\n",
      "epoch 1207, train loss 0.615933 lr: 0.020945\n",
      "epoch 1208, train loss 0.624839 lr: 0.020924\n",
      "epoch 1209, train loss 0.626740 lr: 0.020903\n",
      "epoch 1210, train loss 0.630735 lr: 0.020882\n",
      "epoch 1211, train loss 0.627143 lr: 0.020861\n",
      "epoch 1212, train loss 0.635713 lr: 0.020840\n",
      "epoch 1213, train loss 0.640444 lr: 0.020819\n",
      "epoch 1214, train loss 0.632947 lr: 0.020799\n",
      "epoch 1215, train loss 0.645431 lr: 0.020778\n",
      "epoch 1216, train loss 0.675379 lr: 0.020757\n",
      "epoch 1217, train loss 0.628126 lr: 0.020736\n",
      "epoch 1218, train loss 0.591701 lr: 0.020716\n",
      "epoch 1219, train loss 0.604565 lr: 0.020695\n",
      "epoch 1220, train loss 0.606742 lr: 0.020674\n",
      "epoch 1221, train loss 0.599500 lr: 0.020654\n",
      "epoch 1222, train loss 0.598903 lr: 0.020633\n",
      "epoch 1223, train loss 0.596336 lr: 0.020612\n",
      "epoch 1224, train loss 0.589792 lr: 0.020592\n",
      "epoch 1225, train loss 0.584347 lr: 0.020571\n",
      "epoch 1226, train loss 0.577591 lr: 0.020550\n",
      "epoch 1227, train loss 0.581437 lr: 0.020530\n",
      "epoch 1228, train loss 0.572957 lr: 0.020509\n",
      "epoch 1229, train loss 0.576983 lr: 0.020489\n",
      "epoch 1230, train loss 0.569131 lr: 0.020468\n",
      "epoch 1231, train loss 0.574904 lr: 0.020448\n",
      "epoch 1232, train loss 0.565648 lr: 0.020427\n",
      "epoch 1233, train loss 0.567042 lr: 0.020407\n",
      "epoch 1234, train loss 0.562281 lr: 0.020387\n",
      "epoch 1235, train loss 0.561337 lr: 0.020366\n",
      "epoch 1236, train loss 0.559471 lr: 0.020346\n",
      "epoch 1237, train loss 0.557165 lr: 0.020326\n",
      "epoch 1238, train loss 0.556228 lr: 0.020305\n",
      "epoch 1239, train loss 0.556914 lr: 0.020285\n",
      "epoch 1240, train loss 0.558345 lr: 0.020265\n",
      "epoch 1241, train loss 0.556245 lr: 0.020244\n",
      "epoch 1242, train loss 0.561359 lr: 0.020224\n",
      "epoch 1243, train loss 0.555201 lr: 0.020204\n",
      "epoch 1244, train loss 0.566545 lr: 0.020184\n",
      "epoch 1245, train loss 0.563933 lr: 0.020163\n",
      "epoch 1246, train loss 0.568048 lr: 0.020143\n",
      "epoch 1247, train loss 0.573215 lr: 0.020123\n",
      "epoch 1248, train loss 0.573411 lr: 0.020103\n",
      "epoch 1249, train loss 0.570539 lr: 0.020083\n",
      "epoch 1250, train loss 0.571221 lr: 0.020063\n",
      "epoch 1251, train loss 0.569075 lr: 0.020043\n",
      "epoch 1252, train loss 0.569458 lr: 0.020023\n",
      "epoch 1253, train loss 0.572242 lr: 0.020003\n",
      "epoch 1254, train loss 0.576726 lr: 0.019983\n",
      "epoch 1255, train loss 0.609884 lr: 0.019963\n",
      "epoch 1256, train loss 0.610623 lr: 0.019943\n",
      "epoch 1257, train loss 0.594558 lr: 0.019923\n",
      "epoch 1258, train loss 0.592085 lr: 0.019903\n",
      "epoch 1259, train loss 0.598396 lr: 0.019883\n",
      "epoch 1260, train loss 0.618171 lr: 0.019863\n",
      "epoch 1261, train loss 0.605443 lr: 0.019843\n",
      "epoch 1262, train loss 0.716873 lr: 0.019823\n",
      "epoch 1263, train loss 0.634059 lr: 0.019804\n",
      "epoch 1264, train loss 0.705815 lr: 0.019784\n",
      "epoch 1265, train loss 0.577068 lr: 0.019764\n",
      "epoch 1266, train loss 0.571552 lr: 0.019744\n",
      "epoch 1267, train loss 0.568847 lr: 0.019725\n",
      "epoch 1268, train loss 0.572774 lr: 0.019705\n",
      "epoch 1269, train loss 0.590484 lr: 0.019685\n",
      "epoch 1270, train loss 0.611548 lr: 0.019665\n",
      "epoch 1271, train loss 0.567207 lr: 0.019646\n",
      "epoch 1272, train loss 0.593148 lr: 0.019626\n",
      "epoch 1273, train loss 0.594027 lr: 0.019606\n",
      "epoch 1274, train loss 0.566689 lr: 0.019587\n",
      "epoch 1275, train loss 0.556724 lr: 0.019567\n",
      "epoch 1276, train loss 0.552768 lr: 0.019548\n",
      "epoch 1277, train loss 0.550839 lr: 0.019528\n",
      "epoch 1278, train loss 0.547480 lr: 0.019509\n",
      "epoch 1279, train loss 0.544888 lr: 0.019489\n",
      "epoch 1280, train loss 0.543132 lr: 0.019470\n",
      "epoch 1281, train loss 0.541015 lr: 0.019450\n",
      "epoch 1282, train loss 0.545463 lr: 0.019431\n",
      "epoch 1283, train loss 0.539048 lr: 0.019411\n",
      "epoch 1284, train loss 0.539487 lr: 0.019392\n",
      "epoch 1285, train loss 0.541551 lr: 0.019372\n",
      "epoch 1286, train loss 0.543354 lr: 0.019353\n",
      "epoch 1287, train loss 0.546260 lr: 0.019334\n",
      "epoch 1288, train loss 0.546217 lr: 0.019314\n",
      "epoch 1289, train loss 0.555991 lr: 0.019295\n",
      "epoch 1290, train loss 0.554475 lr: 0.019276\n",
      "epoch 1291, train loss 0.547546 lr: 0.019257\n",
      "epoch 1292, train loss 0.558246 lr: 0.019237\n",
      "epoch 1293, train loss 0.553262 lr: 0.019218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python2.7/site-packages/IPython/kernel/__main__.py:200: UserWarning: The parameter 'updates' of theano.function() expects an OrderedDict, got <type 'dict'>. Using a standard dictionary here results in non-deterministic behavior. You should use an OrderedDict if you are using Python 2.7 (theano.compat.python2x.OrderedDict for older python), or use a list of (shared, update) pairs. Do not just convert your dictionary to this type before the call as the conversion will still be non-deterministic.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-92bea8277e5c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    287\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"__main__\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    288\u001b[0m     \u001b[0mt0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 289\u001b[1;33m     \u001b[0mtest_softmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    290\u001b[0m     \u001b[1;32mprint\u001b[0m \u001b[1;34m\"Elapsed time: %f\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mt0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-92bea8277e5c>\u001b[0m in \u001b[0;36mtest_softmax\u001b[1;34m(n_u, n_h, n_y, time_steps, n_seq, n_epochs)\u001b[0m\n\u001b[0;32m    251\u001b[0m                 n_epochs = n_epochs)\n\u001b[0;32m    252\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 253\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild_trian\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    254\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    255\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'all'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-92bea8277e5c>\u001b[0m in \u001b[0;36mbuild_trian\u001b[1;34m(self, X_train, Y_train, X_test, Y_test)\u001b[0m\n\u001b[0;32m    218\u001b[0m             \u001b[1;31m# compute loss on training set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    219\u001b[0m             train_losses = [compute_train_error(i)\n\u001b[1;32m--> 220\u001b[1;33m                             for i in xrange(n_train)]\n\u001b[0m\u001b[0;32m    221\u001b[0m             \u001b[0mthis_train_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_losses\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mthis_train_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    593\u001b[0m         \u001b[0mt0_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    594\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 595\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    596\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    597\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'position_of_error'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib/python2.7/site-packages/theano/scan_module/scan_op.pyc\u001b[0m in \u001b[0;36mrval\u001b[1;34m(p, i, o, n, allow_gc)\u001b[0m\n\u001b[0;32m    668\u001b[0m         \u001b[0mallow_gc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mallow_gc\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mallow_gc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    669\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 670\u001b[1;33m         def rval(p=p, i=node_input_storage, o=node_output_storage, n=node,\n\u001b[0m\u001b[0;32m    671\u001b[0m                  allow_gc=allow_gc):\n\u001b[0;32m    672\u001b[0m             \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import time\n",
    "import os\n",
    "import datetime\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "plt.ion()\n",
    "\n",
    "mode = theano.Mode(linker='cvm') #the runtime algo to execute the code is in c\n",
    "\n",
    "class RNN(object):\n",
    "    def __init__(self, n_u, n_h, n_y, learning_rate, learning_rate_decay, L1_reg, L2_reg,\n",
    "                 initial_momentum, final_momentum, momentum_switchover,\n",
    "                 n_epochs):\n",
    "\n",
    "        self.n_u = int(n_u)\n",
    "        self.n_h = int(n_h)\n",
    "        self.n_y = int(n_y)\n",
    "        self.gru = GRU(n_u, n_h)\n",
    "        self.learning_rate = float(learning_rate)\n",
    "        self.learning_rate_decay = float(learning_rate_decay)\n",
    "        self.L1_reg = float(L1_reg)\n",
    "        self.L2_reg = float(L2_reg)\n",
    "        self.initial_momentum = float(initial_momentum)\n",
    "        self.final_momentum = float(final_momentum)\n",
    "        self.momentum_switchover = int(momentum_switchover)\n",
    "        self.n_epochs = int(n_epochs)\n",
    "\n",
    "        # input which is `x`\n",
    "        self.x = T.matrix()\n",
    "\n",
    "        # Note that some the bellow variables are not used when\n",
    "        # the activation function is LSTM or GRU. But we simply\n",
    "        # don't care because theano optimize this for us.\n",
    "        #\n",
    "        # Weights are initialized from an uniform distribution\n",
    "        self.W_uh = theano.shared(value = np.asarray(\n",
    "                                              np.random.uniform(\n",
    "                                                  size = (n_u, n_h),\n",
    "                                                  low = -.01, high = .01),\n",
    "                                              dtype = theano.config.floatX),\n",
    "                                  name = 'W_uh')\n",
    "\n",
    "        self.W_hh = theano.shared(value = np.asarray(\n",
    "                                              np.random.uniform(\n",
    "                                                  size = (n_h, n_h),\n",
    "                                                  low = -.01, high = .01),\n",
    "                                              dtype = theano.config.floatX),\n",
    "                                  name = 'W_hh')\n",
    "\n",
    "        self.W_hy = theano.shared(value = np.asarray(\n",
    "                                              np.random.uniform(\n",
    "                                                  size = (n_h, n_y),\n",
    "                                                  low = -.01, high = .01),\n",
    "                                              dtype = theano.config.floatX),\n",
    "                                  name = 'W_hy')\n",
    "\n",
    "        # initial value of hidden layer units are set to zero\n",
    "        self.h0 = theano.shared(value = np.zeros(\n",
    "                                            (n_h, ),\n",
    "                                            dtype = theano.config.floatX),\n",
    "                                name = 'h0')\n",
    "\n",
    "        self.c0 = theano.shared(value = np.zeros(\n",
    "                                            (n_h, ),\n",
    "                                            dtype = theano.config.floatX),\n",
    "                                name = 'c0')\n",
    "\n",
    "        # biases are initialized to zeros\n",
    "        self.b_h = theano.shared(value = np.zeros(\n",
    "                                             (n_h, ),\n",
    "                                             dtype = theano.config.floatX),\n",
    "                                 name = 'b_h')\n",
    "\n",
    "        self.b_y = theano.shared(value = np.zeros(\n",
    "                                             (n_y, ),\n",
    "                                             dtype = theano.config.floatX),\n",
    "                                 name = 'b_y')\n",
    "        # That's because when it is lstm or gru, parameters are different\n",
    "        #if activation == 'lstm':\n",
    "            # Note that `+` here is just a concatenation operator\n",
    "        self.params = self.gru.params + [self.W_hy, self.h0, self.b_y]\n",
    "        # Initial value for updates is zero matrix.\n",
    "        self.updates = {}\n",
    "        for param in self.params:\n",
    "            self.updates[param] = theano.shared(\n",
    "                                      value = np.zeros(\n",
    "                                                  param.get_value(\n",
    "                                                      borrow = True).shape,\n",
    "                                                  dtype = theano.config.floatX),\n",
    "                                      name = 'updates')\n",
    "        \n",
    "        # Default value of c_tm1 is None since we use it just when we have LSTM units\n",
    "        def recurrent_fn(u_t, h_tm1, c_tm1 = None):\n",
    "            # that's because LSTM needs both u_t and h_tm1 to compute gates\n",
    "            #if activation == 'lstm':\n",
    "            h_t = self.gru.gru_as_activation_function(u_t, h_tm1)\n",
    "            y_t = T.dot(h_t, self.W_hy) + self.b_y\n",
    "            c_t = h_t\n",
    "            return h_t, c_t, y_t\n",
    "\n",
    "        # Iteration over the first dimension of a tensor which is TIME in our case.\n",
    "        # recurrent_fn doesn't use y in the computations, so we do not need y0 (None)\n",
    "        # scan returns updates too which we do not need. (_)\n",
    "        [self.h, self.c, self.y_pred], _ = theano.scan(recurrent_fn,\n",
    "                                               sequences = self.x,\n",
    "                                               outputs_info = [self.h0, self.c0, None])\n",
    "\n",
    "        # L1 norm\n",
    "        self.L1 = abs(self.W_uh.sum()) + \\\n",
    "                  abs(self.W_hh.sum()) + \\\n",
    "                  abs(self.W_hy.sum())\n",
    "\n",
    "        # square of L2 norm\n",
    "        self.L2_sqr = (self.W_uh ** 2).sum() + \\\n",
    "                      (self.W_hh ** 2).sum() + \\\n",
    "                      (self.W_hy ** 2).sum()\n",
    "\n",
    "        # Loss function is different for different output types\n",
    "        # defining function in place is so easy! : lambda input: expresion\n",
    "        self.y = T.vector(name = 'y', dtype = 'int32')\n",
    "        self.p_y_given_x = T.nnet.softmax(self.y_pred)\n",
    "        self.y_out = T.argmax(self.p_y_given_x, axis = -1)\n",
    "        self.loss = lambda y: self.nll_multiclass(y)\n",
    "        self.predict_proba = theano.function(inputs = [self.x, ],\n",
    "                                             outputs = self.p_y_given_x,\n",
    "                                             mode = mode, allow_input_downcast=True)\n",
    "        self.predict = theano.function(inputs = [self.x, ],\n",
    "                                       outputs = self.y_out, # y-out is calculated by applying argmax\n",
    "                                       mode = mode, allow_input_downcast=True)\n",
    "        # Just for tracking training error for Graph 3\n",
    "        self.errors = []\n",
    "\n",
    "    def mse(self, y):\n",
    "        # mean is because of minibatch\n",
    "        return T.mean((self.y_pred - y) ** 2)\n",
    "\n",
    "    def nll_binary(self, y):\n",
    "        # negative log likelihood here is cross entropy\n",
    "        return T.mean(T.nnet.binary_crossentropy(self.p_y_given_x, y))\n",
    "\n",
    "    def nll_multiclass(self, y):\n",
    "        # notice to [  T.arange(y.shape[0])  ,  y  ]\n",
    "        return -T.mean(T.log(self.p_y_given_x)[T.arange(y.shape[0]), y])\n",
    "\n",
    "    # X_train, Y_train, X_test, and Y_test are numpy arrays\n",
    "    def build_trian(self, X_train, Y_train, X_test = None, Y_test = None):\n",
    "        train_set_x = theano.shared(np.asarray(X_train, dtype=theano.config.floatX))\n",
    "        train_set_y = theano.shared(np.asarray(Y_train, dtype=theano.config.floatX))\n",
    "        #if self.output_type in ('binary', 'softmax'):\n",
    "        train_set_y = T.cast(train_set_y, 'int32')\n",
    "\n",
    "        ######################\n",
    "        # BUILD ACTUAL MODEL #\n",
    "        ######################\n",
    "        print 'Buiding model ...'\n",
    "\n",
    "        index = T.lscalar('index')    # index to a case\n",
    "        # learning rate (may change)\n",
    "        lr = T.scalar('lr', dtype = theano.config.floatX)\n",
    "        mom = T.scalar('mom', dtype = theano.config.floatX)  # momentum\n",
    "\n",
    "\n",
    "        # Note that we use cost for training\n",
    "        # But, compute_train_error for just watching and printing\n",
    "        cost = self.loss(self.y) \\\n",
    "            + self.L1_reg * self.L1 \\\n",
    "            + self.L2_reg * self.L2_sqr\n",
    "\n",
    "        # We don't want to pass whole dataset every time we use this function.\n",
    "        # So, the solution is to put the dataset in the GPU as `givens`.\n",
    "        # And just pass index to the function each time as input.\n",
    "        compute_train_error = theano.function(inputs = [index, ],\n",
    "                                              outputs = self.loss(self.y),\n",
    "                                              givens = {\n",
    "                                                  self.x: train_set_x[index],\n",
    "                                                  self.y: train_set_y[index]},\n",
    "                                              mode = mode, allow_input_downcast=True)\n",
    "\n",
    "        # Gradients of cost wrt. [self.W, self.W_in, self.W_out,\n",
    "        # self.h0, self.b_h, self.b_y] using BPTT.\n",
    "        gparams = []\n",
    "        for param in self.params:\n",
    "            gparams.append(T.grad(cost, param))\n",
    "\n",
    "        # zip just concatenate two lists\n",
    "        updates = {}\n",
    "        for param, gparam in zip(self.params, gparams):\n",
    "            weight_update = self.updates[param]\n",
    "            upd = mom * weight_update - lr * gparam\n",
    "            updates[weight_update] = upd\n",
    "            updates[param] = param + upd\n",
    "\n",
    "        # compiling a Theano function `train_model` that returns the\n",
    "        # cost, but in the same time updates the parameter of the\n",
    "        # model based on the rules defined in `updates`\n",
    "        train_model = theano.function(inputs = [index, lr, mom],\n",
    "                                      outputs = cost,\n",
    "                                      updates = updates,\n",
    "                                      givens = {\n",
    "                                          self.x: train_set_x[index], # [:, batch_start:batch_stop]\n",
    "                                          self.y: train_set_y[index]},\n",
    "                                      mode = mode, allow_input_downcast=True)\n",
    "\n",
    "        ###############\n",
    "        # TRAIN MODEL #\n",
    "        ###############\n",
    "        print 'Training model ...'\n",
    "        epoch = 0\n",
    "        n_train = train_set_x.get_value(borrow = True).shape[0]\n",
    "\n",
    "        while (epoch < self.n_epochs):\n",
    "            epoch = epoch + 1\n",
    "            for idx in xrange(n_train):\n",
    "                effective_momentum = self.final_momentum \\\n",
    "                                     if epoch > self.momentum_switchover \\\n",
    "                                     else self.initial_momentum\n",
    "                example_cost = train_model(idx,\n",
    "                                           self.learning_rate,\n",
    "                                           effective_momentum)\n",
    "\n",
    "\n",
    "            # compute loss on training set\n",
    "            train_losses = [compute_train_error(i)\n",
    "                            for i in xrange(n_train)]\n",
    "            this_train_loss = np.mean(train_losses)\n",
    "            self.errors.append(this_train_loss)\n",
    "\n",
    "            print('epoch %i, train loss %f ''lr: %f' % \\\n",
    "                  (epoch, this_train_loss, self.learning_rate))\n",
    "            \n",
    "\n",
    "            self.learning_rate *= self.learning_rate_decay\n",
    "\"\"\"\n",
    "Here we define some testing functions.\n",
    "For more details see Graham Taylor model:\n",
    "https://github.com/gwtaylor/theano-rnn\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "Here we test the RNN with real output.\n",
    "We randomly generate `n_seq` sequences of length `time_steps`.\n",
    "Then we make a delay to get the targets. (+ adding some noise)\n",
    "Resulting graphs are saved under the name of `real.png`.\n",
    "\"\"\"\n",
    "\n",
    "def test_softmax(n_u = 4, n_h = 6, n_y = 7, time_steps = 4, n_seq= 27, n_epochs = 3000):\n",
    "    # n_y is equal to the number of calsses\n",
    "    print 'Testing model with softmax outputs'\n",
    "\n",
    "    np.random.seed(0)\n",
    "\n",
    "    model = RNN(n_u = n_u, n_h = n_h, n_y = n_y, \n",
    "                learning_rate = 0.06, learning_rate_decay = 0.999,\n",
    "                L1_reg = 0, L2_reg = 0, \n",
    "                initial_momentum = 0.4, final_momentum = 0.8,\n",
    "                momentum_switchover = 5,\n",
    "                n_epochs = n_epochs)\n",
    "\n",
    "    model.build_trian(seq, targets)\n",
    "\n",
    "    plt.close('all')\n",
    "    fig = plt.figure()\n",
    "    ax1 = plt.subplot(311)\n",
    "    plt.plot(seq[1])\n",
    "    plt.grid()\n",
    "    ax1.set_title('input')\n",
    "    ax2 = plt.subplot(312)\n",
    "\n",
    "    plt.scatter(xrange(time_steps), targets[1], marker = 'o', c = 'b')\n",
    "    plt.grid()\n",
    "    \n",
    "    guess = model.predict_proba(seq[1])\n",
    "    guessed_probs = plt.imshow(guess.T, interpolation = 'nearest', cmap = 'gray')\n",
    "    ax2.set_title('blue points: true class, grayscale: model output (white mean class)')\n",
    "\n",
    "    ax3 = plt.subplot(313)\n",
    "    plt.plot(model.errors)\n",
    "    plt.grid()\n",
    "    ax3.set_title('Training error')\n",
    "    plt.savefig('softmax_Epoch: ' + str(n_epochs) + '.png')\n",
    "    \n",
    "    print \"Image saved.\"\n",
    "    print model.predict([[3, 2, 1, 6],\n",
    "  [1, 7, 1, 5],\n",
    "  [4, 1, 1, 2],\n",
    "  [3, 4, 3, 1]])\n",
    "    \n",
    "    print model.predict([[3, 4, 5, 5],\n",
    "  [5, 4, 3, 4],\n",
    "  [1, 2, 3, 4],\n",
    "  [3, 2, 1, 1]])\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    t0 = time.time()\n",
    "    test_softmax()\n",
    "    print \"Elapsed time: %f\" % (time.time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
