{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing model with softmax outputs\n",
      "[[1 6 2 5]\n",
      " [1 6 2 5]\n",
      " [1 6 2 5]\n",
      " [1 4 5 1]\n",
      " [1 4 5 1]\n",
      " [1 4 5 1]]\n",
      "Buiding model ...\n",
      "Training model ...\n",
      "epoch 1, train loss 1.944956 lr: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python2.7/site-packages/IPython/kernel/__main__.py:323: UserWarning: The parameter 'updates' of theano.function() expects an OrderedDict, got <type 'dict'>. Using a standard dictionary here results in non-deterministic behavior. You should use an OrderedDict if you are using Python 2.7 (theano.compat.python2x.OrderedDict for older python), or use a list of (shared, update) pairs. Do not just convert your dictionary to this type before the call as the conversion will still be non-deterministic.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'TensorVariable' object has no attribute 'get_value'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-5933e2812aa0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    572\u001b[0m     \u001b[1;31m#test_real()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    573\u001b[0m     \u001b[1;31m#test_binary()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 574\u001b[1;33m     \u001b[0mtest_softmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    575\u001b[0m     \u001b[1;32mprint\u001b[0m \u001b[1;34m\"Elapsed time: %f\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mt0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-5933e2812aa0>\u001b[0m in \u001b[0;36mtest_softmax\u001b[1;34m(n_u, n_h, n_y, time_steps, n_seq, n_epochs)\u001b[0m\n\u001b[0;32m    543\u001b[0m                 n_epochs = n_epochs)\n\u001b[0;32m    544\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild_trian\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    546\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'all'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-5933e2812aa0>\u001b[0m in \u001b[0;36mbuild_trian\u001b[1;34m(self, X_train, Y_train, X_test, Y_test)\u001b[0m\n\u001b[0;32m    346\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    347\u001b[0m             \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'epoch %i, train loss %f '\u001b[0m\u001b[1;34m'lr: %f'\u001b[0m \u001b[1;33m%\u001b[0m                   \u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthis_train_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 348\u001b[1;33m             \u001b[1;32mprint\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my_out\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    349\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    350\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearning_rate\u001b[0m \u001b[1;33m*=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearning_rate_decay\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'TensorVariable' object has no attribute 'get_value'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "Recurrent Neural Network containing LSTM and GRU hidden layer\n",
    "Code provided by Mohammad Pezeshki - Nov. 2014 - Universite de Montreal\n",
    "This code is distributed without any warranty, express or implied.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import time\n",
    "import os\n",
    "import datetime\n",
    "import matplotlib\n",
    "import gating\n",
    "# Force matplotlib to not use any Xwindows backend.\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "plt.ion()\n",
    "\n",
    "mode = theano.Mode(linker='cvm') #the runtime algo to execute the code is in c\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "What we have in this class:\n",
    "\n",
    "    Model structure parameters:\n",
    "        n_u : length of input layer vector in each time-step\n",
    "        n_h : length of hidden layer vector in each time-step\n",
    "        n_y : length of output layer vector in each time-step\n",
    "        activation : type of activation function used for hidden layer\n",
    "                     can be: sigmoid, tanh, relu, lstm, or gru\n",
    "        output_type : type of output which could be `real`, `binary`, or `softmax`\n",
    "\n",
    "    Parameters to be learned:\n",
    "        W_uh : weight matrix from input to hidden layer\n",
    "        W_hh : recurrent weight matrix from hidden to hidden layer\n",
    "        W_hy : weight matrix from hidden to output layer\n",
    "        b_h : biases vector of hidden layer\n",
    "        b_y : biases vector of output layer\n",
    "        h0 : initial values for the hidden layer\n",
    "\n",
    "    Learning hyper-parameters:\n",
    "        learning_rate : learning rate which is not constant\n",
    "        learning_rate_decay : learning rate decay :)\n",
    "        L1_reg : L1 regularization term coefficient\n",
    "        L2_reg : L2 regularization term coefficient\n",
    "        initial_momentum : momentum value which we start with\n",
    "        final_momentum : final value of momentum\n",
    "        momentum_switchover : on which `epoch` should we switch from\n",
    "                              initial value to final value of momentum\n",
    "        n_epochs : number of iterations\n",
    "\n",
    "    Inner class variables:\n",
    "        self.x : symbolic input vector\n",
    "        self.y : target output\n",
    "        self.y_pred : raw output of the model\n",
    "        self.p_y_given_x : output after applying sigmoid (binary output case)\n",
    "        self.y_out : round (0,1) for binary and argmax (0,1,...,k) for softmax\n",
    "        self.loss : loss function (MSE or CrossEntropy)\n",
    "        self.predict : a function returns predictions which is type is related to output type\n",
    "        self.predict_proba : a function returns predictions probabilities (binary and softmax)\n",
    "    \n",
    "    build_train function:\n",
    "        train_set_x : input of network\n",
    "        train_set_y : target of network\n",
    "        index : index over each of training sequences (NOT the number of time-steps)\n",
    "        lr : learning rate\n",
    "        mom : momentum\n",
    "        cost : cost function value\n",
    "        compute_train_error : a function compute error on training\n",
    "        gparams : Gradients of model parameters\n",
    "        updates : updates which should be applied to parameters\n",
    "        train_model : a function that returns the cost, but \n",
    "                      in the same time updates the parameter\n",
    "                      of the model based on the rules defined\n",
    "                      in `updates`.\n",
    "        \n",
    "\"\"\"\n",
    "class RNN(object):\n",
    "    def __init__(self, n_u, n_h, n_y, activation, output_type,\n",
    "                 learning_rate, learning_rate_decay, L1_reg, L2_reg,\n",
    "                 initial_momentum, final_momentum, momentum_switchover,\n",
    "                 n_epochs):\n",
    "\n",
    "        self.n_u = int(n_u)\n",
    "        self.n_h = int(n_h)\n",
    "        self.n_y = int(n_y)\n",
    "\n",
    "        if activation == 'tanh':\n",
    "            self.activation = T.tanh\n",
    "        elif activation == 'sigmoid':\n",
    "            self.activation = T.nnet.sigmoid\n",
    "        elif activation == 'relu':\n",
    "            self.activation = lambda x: x * (x > 0) # T.maximum(x, 0)\n",
    "        elif activation == 'lstm':\n",
    "            self.lstm = gating.LSTM(n_u, n_h)\n",
    "            self.activation = self.lstm.lstm_as_activation_function\n",
    "        elif activation == 'gru':\n",
    "            self.gru = gating.GRU(n_u, n_h)\n",
    "            self.activation = self.gru.gru_as_activation_function\n",
    "        else:            \n",
    "            raise NotImplementedError   \n",
    "\n",
    "        self.output_type = output_type\n",
    "        self.learning_rate = float(learning_rate)\n",
    "        self.learning_rate_decay = float(learning_rate_decay)\n",
    "        self.L1_reg = float(L1_reg)\n",
    "        self.L2_reg = float(L2_reg)\n",
    "        self.initial_momentum = float(initial_momentum)\n",
    "        self.final_momentum = float(final_momentum)\n",
    "        self.momentum_switchover = int(momentum_switchover)\n",
    "        self.n_epochs = int(n_epochs)\n",
    "\n",
    "        # input which is `x`\n",
    "        self.x = T.matrix()\n",
    "\n",
    "        # Note that some the bellow variables are not used when\n",
    "        # the activation function is LSTM or GRU. But we simply\n",
    "        # don't care because theano optimize this for us.\n",
    "        #\n",
    "        # Weights are initialized from an uniform distribution\n",
    "        self.W_uh = theano.shared(value = np.asarray(\n",
    "                                              np.random.uniform(\n",
    "                                                  size = (n_u, n_h),\n",
    "                                                  low = -.01, high = .01),\n",
    "                                              dtype = theano.config.floatX),\n",
    "                                  name = 'W_uh')\n",
    "\n",
    "        self.W_hh = theano.shared(value = np.asarray(\n",
    "                                              np.random.uniform(\n",
    "                                                  size = (n_h, n_h),\n",
    "                                                  low = -.01, high = .01),\n",
    "                                              dtype = theano.config.floatX),\n",
    "                                  name = 'W_hh')\n",
    "\n",
    "        self.W_hy = theano.shared(value = np.asarray(\n",
    "                                              np.random.uniform(\n",
    "                                                  size = (n_h, n_y),\n",
    "                                                  low = -.01, high = .01),\n",
    "                                              dtype = theano.config.floatX),\n",
    "                                  name = 'W_hy')\n",
    "\n",
    "        # initial value of hidden layer units are set to zero\n",
    "        self.h0 = theano.shared(value = np.zeros(\n",
    "                                            (n_h, ),\n",
    "                                            dtype = theano.config.floatX),\n",
    "                                name = 'h0')\n",
    "\n",
    "        self.c0 = theano.shared(value = np.zeros(\n",
    "                                            (n_h, ),\n",
    "                                            dtype = theano.config.floatX),\n",
    "                                name = 'c0')\n",
    "\n",
    "        # biases are initialized to zeros\n",
    "        self.b_h = theano.shared(value = np.zeros(\n",
    "                                             (n_h, ),\n",
    "                                             dtype = theano.config.floatX),\n",
    "                                 name = 'b_h')\n",
    "\n",
    "        self.b_y = theano.shared(value = np.zeros(\n",
    "                                             (n_y, ),\n",
    "                                             dtype = theano.config.floatX),\n",
    "                                 name = 'b_y')\n",
    "        # That's because when it is lstm or gru, parameters are different\n",
    "        if activation == 'lstm':\n",
    "            # Note that `+` here is just a concatenation operator\n",
    "            self.params = self.lstm.params + [self.W_hy, self.h0, self.b_y]\n",
    "        elif activation == 'gru':\n",
    "            self.params = self.gru.params + [self.W_hy, self.h0, self.b_y]\n",
    "        else:\n",
    "            self.params = [self.W_uh, self.W_hh, self.W_hy, self.h0,\n",
    "                self.b_h, self.b_y]\n",
    "\n",
    "        # Initial value for updates is zero matrix.\n",
    "        self.updates = {}\n",
    "        for param in self.params:\n",
    "            self.updates[param] = theano.shared(\n",
    "                                      value = np.zeros(\n",
    "                                                  param.get_value(\n",
    "                                                      borrow = True).shape,\n",
    "                                                  dtype = theano.config.floatX),\n",
    "                                      name = 'updates')\n",
    "        \n",
    "        # Default value of c_tm1 is None since we use it just when we have LSTM units\n",
    "        def recurrent_fn(u_t, h_tm1, c_tm1 = None):\n",
    "            # that's because LSTM needs both u_t and h_tm1 to compute gates\n",
    "            if activation == 'lstm':\n",
    "                h_t, c_t = self.activation(u_t, h_tm1, c_tm1)\n",
    "            elif activation == 'gru':\n",
    "                h_t = self.activation(u_t, h_tm1)\n",
    "                # In this case, we don't need c_t; but we need to return something.\n",
    "                # On the other hand, we cnannot return None. Thus,\n",
    "                # To use theano optimazation features, let's just return h_t.\n",
    "                c_t = h_t # Just to get rid of c_t\n",
    "            else:\n",
    "                h_t = self.activation(T.dot(u_t, self.W_uh) + \\\n",
    "                                      T.dot(h_tm1, self.W_hh) + \\\n",
    "                                      self.b_h)\n",
    "                # Read above comment\n",
    "                c_t = h_t # Just to get rid of c_t\n",
    "\n",
    "            y_t = T.dot(h_t, self.W_hy) + self.b_y\n",
    "            return h_t, c_t, y_t\n",
    "\n",
    "        # Iteration over the first dimension of a tensor which is TIME in our case.\n",
    "        # recurrent_fn doesn't use y in the computations, so we do not need y0 (None)\n",
    "        # scan returns updates too which we do not need. (_)\n",
    "        [self.h, self.c, self.y_pred], _ = theano.scan(recurrent_fn,\n",
    "                                               sequences = self.x,\n",
    "                                               outputs_info = [self.h0, self.c0, None])\n",
    "\n",
    "        # L1 norm\n",
    "        self.L1 = abs(self.W_uh.sum()) + \\\n",
    "                  abs(self.W_hh.sum()) + \\\n",
    "                  abs(self.W_hy.sum())\n",
    "\n",
    "        # square of L2 norm\n",
    "        self.L2_sqr = (self.W_uh ** 2).sum() + \\\n",
    "                      (self.W_hh ** 2).sum() + \\\n",
    "                      (self.W_hy ** 2).sum()\n",
    "\n",
    "        # Loss function is different for different output types\n",
    "        # defining function in place is so easy! : lambda input: expresion\n",
    "        if self.output_type == 'real':\n",
    "            self.y = T.matrix(name = 'y', dtype = theano.config.floatX)\n",
    "            self.loss = lambda y: self.mse(y) # y is input and self.mse(y) is output\n",
    "            self.predict = theano.function(inputs = [self.x, ],\n",
    "                                           outputs = self.y_pred,\n",
    "                                           mode = mode)\n",
    "\n",
    "        elif self.output_type == 'binary':\n",
    "            self.y = T.matrix(name = 'y', dtype = 'int32')\n",
    "            self.p_y_given_x = T.nnet.sigmoid(self.y_pred)\n",
    "            self.y_out = T.round(self.p_y_given_x)  # round to {0,1}\n",
    "            self.loss = lambda y: self.nll_binary(y)\n",
    "            self.predict_proba = theano.function(inputs = [self.x, ],\n",
    "                                                 outputs = self.p_y_given_x,\n",
    "                                                 mode = mode)\n",
    "            self.predict = theano.function(inputs = [self.x, ],\n",
    "                                           outputs = T.round(self.p_y_given_x),\n",
    "                                           mode = mode)\n",
    "        \n",
    "        elif self.output_type == 'softmax':\n",
    "            self.y = T.vector(name = 'y', dtype = 'int32')\n",
    "            self.p_y_given_x = T.nnet.softmax(self.y_pred)\n",
    "            self.y_out = T.argmax(self.p_y_given_x, axis = -1)\n",
    "            self.loss = lambda y: self.nll_multiclass(y)\n",
    "            self.predict_proba = theano.function(inputs = [self.x, ],\n",
    "                                                 outputs = self.p_y_given_x,\n",
    "                                                 mode = mode)\n",
    "            self.predict = theano.function(inputs = [self.x, ],\n",
    "                                           outputs = self.y_out, # y-out is calculated by applying argmax\n",
    "                                           mode = mode)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        # Just for tracking training error for Graph 3\n",
    "        self.errors = []\n",
    "\n",
    "    def mse(self, y):\n",
    "        # mean is because of minibatch\n",
    "        return T.mean((self.y_pred - y) ** 2)\n",
    "\n",
    "    def nll_binary(self, y):\n",
    "        # negative log likelihood here is cross entropy\n",
    "        return T.mean(T.nnet.binary_crossentropy(self.p_y_given_x, y))\n",
    "\n",
    "    def nll_multiclass(self, y):\n",
    "        # notice to [  T.arange(y.shape[0])  ,  y  ]\n",
    "        return -T.mean(T.log(self.p_y_given_x)[T.arange(y.shape[0]), y])\n",
    "\n",
    "    # X_train, Y_train, X_test, and Y_test are numpy arrays\n",
    "    def build_trian(self, X_train, Y_train, X_test = None, Y_test = None):\n",
    "        train_set_x = theano.shared(np.asarray(X_train, dtype=theano.config.floatX))\n",
    "        train_set_y = theano.shared(np.asarray(Y_train, dtype=theano.config.floatX))\n",
    "        if self.output_type in ('binary', 'softmax'):\n",
    "            train_set_y = T.cast(train_set_y, 'int32')\n",
    "\n",
    "        ######################\n",
    "        # BUILD ACTUAL MODEL #\n",
    "        ######################\n",
    "        print 'Buiding model ...'\n",
    "\n",
    "        index = T.lscalar('index')    # index to a case\n",
    "        # learning rate (may change)\n",
    "        lr = T.scalar('lr', dtype = theano.config.floatX)\n",
    "        mom = T.scalar('mom', dtype = theano.config.floatX)  # momentum\n",
    "\n",
    "\n",
    "        # Note that we use cost for training\n",
    "        # But, compute_train_error for just watching and printing\n",
    "        cost = self.loss(self.y) \\\n",
    "            + self.L1_reg * self.L1 \\\n",
    "            + self.L2_reg * self.L2_sqr\n",
    "\n",
    "        # We don't want to pass whole dataset every time we use this function.\n",
    "        # So, the solution is to put the dataset in the GPU as `givens`.\n",
    "        # And just pass index to the function each time as input.\n",
    "        compute_train_error = theano.function(inputs = [index, ],\n",
    "                                              outputs = self.loss(self.y),\n",
    "                                              givens = {\n",
    "                                                  self.x: train_set_x[index],\n",
    "                                                  self.y: train_set_y[index]},\n",
    "                                              mode = mode)\n",
    "\n",
    "        # Gradients of cost wrt. [self.W, self.W_in, self.W_out,\n",
    "        # self.h0, self.b_h, self.b_y] using BPTT.\n",
    "        gparams = []\n",
    "        for param in self.params:\n",
    "            gparams.append(T.grad(cost, param))\n",
    "\n",
    "        # zip just concatenate two lists\n",
    "        updates = {}\n",
    "        for param, gparam in zip(self.params, gparams):\n",
    "            weight_update = self.updates[param]\n",
    "            upd = mom * weight_update - lr * gparam\n",
    "            updates[weight_update] = upd\n",
    "            updates[param] = param + upd\n",
    "\n",
    "        # compiling a Theano function `train_model` that returns the\n",
    "        # cost, but in the same time updates the parameter of the\n",
    "        # model based on the rules defined in `updates`\n",
    "        train_model = theano.function(inputs = [index, lr, mom],\n",
    "                                      outputs = cost,\n",
    "                                      updates = updates,\n",
    "                                      givens = {\n",
    "                                          self.x: train_set_x[index], # [:, batch_start:batch_stop]\n",
    "                                          self.y: train_set_y[index]},\n",
    "                                      mode = mode)\n",
    "\n",
    "        ###############\n",
    "        # TRAIN MODEL #\n",
    "        ###############\n",
    "        print 'Training model ...'\n",
    "        epoch = 0\n",
    "        n_train = train_set_x.get_value(borrow = True).shape[0]\n",
    "\n",
    "        while (epoch < self.n_epochs):\n",
    "            epoch = epoch + 1\n",
    "            for idx in xrange(n_train):\n",
    "                effective_momentum = self.final_momentum \\\n",
    "                                     if epoch > self.momentum_switchover \\\n",
    "                                     else self.initial_momentum\n",
    "                example_cost = train_model(idx,\n",
    "                                           self.learning_rate,\n",
    "                                           effective_momentum)\n",
    "\n",
    "\n",
    "            # compute loss on training set\n",
    "            train_losses = [compute_train_error(i)\n",
    "                            for i in xrange(n_train)]\n",
    "            this_train_loss = np.mean(train_losses)\n",
    "            self.errors.append(this_train_loss)\n",
    "\n",
    "            print('epoch %i, train loss %f ''lr: %f' % \\\n",
    "                  (epoch, this_train_loss, self.learning_rate))\n",
    "            print self.y_out.get_value()\n",
    "\n",
    "            self.learning_rate *= self.learning_rate_decay\n",
    "\"\"\"\n",
    "Here we define some testing functions.\n",
    "For more details see Graham Taylor model:\n",
    "https://github.com/gwtaylor/theano-rnn\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "Here we test the RNN with real output.\n",
    "We randomly generate `n_seq` sequences of length `time_steps`.\n",
    "Then we make a delay to get the targets. (+ adding some noise)\n",
    "Resulting graphs are saved under the name of `real.png`.\n",
    "\"\"\"\n",
    "def test_real(n_u = 3, n_h = 10, n_y = 3, time_steps = 20, n_seq= 100, n_epochs = 1000):\n",
    "    #n_u : input vector size (not time at this point)\n",
    "    #n_h : hidden vector size\n",
    "    #n_y : output vector size\n",
    "    #time_steps : number of time-steps in time\n",
    "    #n_seq : number of sequences for training\n",
    "\n",
    "    print 'Testing model with real outputs'\n",
    "    np.random.seed(0)\n",
    "    \n",
    "    # generating random sequences\n",
    "    seq = np.random.randn(n_seq, time_steps, n_u)\n",
    "    targets = np.zeros((n_seq, time_steps, n_y))\n",
    "\n",
    "    targets[:, 1:, 0] = seq[:, :-1, 0] # 1 time-step delay between input and output\n",
    "    targets[:, 4:, 1] = seq[:, :-4, 1] # 2 time-step delay\n",
    "    targets[:, 8:, 2] = seq[:, :-8, 2] # 3 time-step delay\n",
    "\n",
    "    targets += 0.01 * np.random.standard_normal(targets.shape)\n",
    "\n",
    "    model = RNN(n_u = n_u, n_h = n_h, n_y = n_y,\n",
    "                activation = 'relu', output_type = 'real',\n",
    "                learning_rate = 0.0015, learning_rate_decay = 0.9999,\n",
    "                L1_reg = 0, L2_reg = 0, \n",
    "                initial_momentum = 0.5, final_momentum = 0.9,\n",
    "                momentum_switchover = 5,\n",
    "                n_epochs = n_epochs)\n",
    "\n",
    "    model.build_trian(seq, targets)\n",
    "\n",
    "\n",
    "    # We just plot one of the sequences\n",
    "    plt.close('all')\n",
    "    fig = plt.figure()\n",
    "\n",
    "    # Graph 1\n",
    "    ax1 = plt.subplot(311) # numrows, numcols, fignum\n",
    "    plt.plot(seq[0])\n",
    "    plt.grid()\n",
    "    ax1.set_title('Input sequence')\n",
    "\n",
    "    # Graph 2\n",
    "    ax2 = plt.subplot(312)\n",
    "    true_targets = plt.plot(targets[0])\n",
    "\n",
    "    guess = model.predict(seq[0])\n",
    "    guessed_targets = plt.plot(guess, linestyle='--')\n",
    "    plt.grid()\n",
    "    for i, x in enumerate(guessed_targets):\n",
    "        x.set_color(true_targets[i].get_color())\n",
    "    ax2.set_title('solid: true output, dashed: model output')\n",
    "\n",
    "    # Graph 3\n",
    "    ax3 = plt.subplot(313)\n",
    "    plt.plot(model.errors)\n",
    "    plt.grid()\n",
    "    ax1.set_title('Training error')\n",
    "\n",
    "    # Save as a file\n",
    "    plt.savefig('real_' + str(model.activation) + '_Epoch: ' + str(n_epochs) + '.png')\n",
    "\n",
    "\"\"\"\n",
    "Here we test the RNN with binary output.\n",
    "We randomly generate `n_seq` sequences of length `time_steps`.\n",
    "Then we make a delay and make binary number which are obtained \n",
    "using comparison to get the targets. (+ adding some noise)\n",
    "Resulting graphs are saved under the name of `binary.png`.\n",
    "\"\"\"\n",
    "def test_binary(n_u = 2, n_h = 5, n_y = 1, time_steps = 20, n_seq= 100, n_epochs = 700):\n",
    "    print 'Testing model with binary outputs'\n",
    "\n",
    "    np.random.seed(0)\n",
    "\n",
    "    seq = np.random.randn(n_seq, time_steps, n_u)\n",
    "    targets = np.zeros((n_seq, time_steps, n_y))\n",
    "\n",
    "    # whether `dim 3` is greater than `dim 0`\n",
    "    targets[:, 2:, 0] = np.cast[np.int](seq[:, 1:-1, 1] > seq[:, :-2, 0])\n",
    "\n",
    "    model = RNN(n_u = n_u, n_h = n_h, n_y = n_y,\n",
    "                activation = 'tanh', output_type = 'binary',\n",
    "                learning_rate = 0.001, learning_rate_decay = 0.999,\n",
    "                L1_reg = 0, L2_reg = 0, \n",
    "                initial_momentum = 0.5, final_momentum = 0.9,\n",
    "                momentum_switchover = 5,\n",
    "                n_epochs = n_epochs)\n",
    "\n",
    "    model.build_trian(seq, targets)\n",
    "\n",
    "    plt.close('all')\n",
    "    fig = plt.figure()\n",
    "    ax1 = plt.subplot(311)\n",
    "    plt.plot(seq[1])\n",
    "    plt.grid()\n",
    "    ax1.set_title('input')\n",
    "    ax2 = plt.subplot(312)\n",
    "    guess = model.predict_proba(seq[1])\n",
    "    # put target and model output beside each other\n",
    "    plt.imshow(np.hstack((targets[1], guess)).T, interpolation = 'nearest', cmap = 'gray')\n",
    "\n",
    "    plt.grid()\n",
    "    ax2.set_title('first row: true output, second row: model output')\n",
    "\n",
    "    ax3 = plt.subplot(313)\n",
    "    plt.plot(model.errors)\n",
    "    plt.grid()\n",
    "    ax3.set_title('Training error')\n",
    "\n",
    "    plt.savefig('binary_' + str(model.activation) + '_Epoch: ' + str(n_epochs) + '.png')\n",
    "\n",
    "\"\"\"\n",
    "Here we test the RNN with softmax output.\n",
    "We randomly generate `n_seq` sequences of length `time_steps`.\n",
    "Then we make a delay and make classed which are obtained \n",
    "using comparison to get the targets.\n",
    "Resulting graphs are saved under the name of `softmax.png`.\n",
    "\"\"\"\n",
    "def test_softmax(n_u = 4, n_h = 6, n_y = 7, time_steps = 4, n_seq= 6, n_epochs = 10):\n",
    "    # n_y is equal to the number of calsses\n",
    "    print 'Testing model with softmax outputs'\n",
    "\n",
    "    np.random.seed(0)\n",
    "\n",
    "    #seq = np.random.randn(n_seq, time_steps, n_u)\n",
    "    \n",
    "    a=[[[ 5, 3, 5, 3 ],\n",
    "      [ 1, 2, 3, 1 ],\n",
    "      [ 4, 2, 3, 2 ],\n",
    "      [ 2, 3, 4, 5 ]],\n",
    "\n",
    "     [[ 5, 1, 5, 1 ],\n",
    "      [ 6, 1, 6, 1 ],\n",
    "      [ 4, 2, 4, 2 ],\n",
    "      [ 5, 2, 5, 2 ]],\n",
    "\n",
    "     [[ 1, 3, 5, 1 ],\n",
    "      [ 1, 3, 6, 3 ],\n",
    "      [ 2, 4, 6, 4 ],\n",
    "      [ 2, 5, 7, 5 ]],\n",
    "\n",
    "     [[ 5, 5, 5, 3 ],\n",
    "      [ 4, 4, 4, 2 ],\n",
    "      [ 3, 3, 3, 2 ],\n",
    "      [ 1, 2, 3, 1 ]],\n",
    "\n",
    "     [[ 1, 3, 5, 1 ],\n",
    "      [ 1, 4, 6, 1 ],\n",
    "      [ 2, 5, 7, 2 ],\n",
    "      [ 1, 3, 5, 1 ]],\n",
    "\n",
    "     [[ 5, 1, 5, 1 ],\n",
    "      [ 6, 1, 6, 1 ],\n",
    "      [ 7, 2, 7, 2 ],\n",
    "      [ 5, 1, 5, 1 ]]]\n",
    "    \n",
    "    seq = np.array(a)\n",
    "    \n",
    "    #print seq\n",
    "    \n",
    "    # Note that is this case `targets` is a 2d array\n",
    "    targets = np.zeros((n_seq, time_steps), dtype=np.int)\n",
    "\n",
    "    thresh = 0.5\n",
    "    # Comparisons to assing a class label in output\n",
    "    #targets[:, 2:][seq[:, 1:-1, 1] > seq[:, :-2, 0] + thresh] = 1\n",
    "    #targets[:, 2:][seq[:, 1:-1, 1] < seq[:, :-2, 0] - thresh] = 2\n",
    "    \n",
    "    # otherwise class is 0\n",
    "    targets=np.array([[1, 6, 2, 5],\n",
    " [1, 6, 2, 5],\n",
    " [1, 6, 2, 5],\n",
    " [1, 4, 5, 1],\n",
    " [1, 4, 5, 1],\n",
    " [1, 4, 5, 1]])\n",
    "    print targets\n",
    "    model = RNN(n_u = n_u, n_h = n_h, n_y = n_y,\n",
    "                activation = 'lstm', output_type = 'softmax',\n",
    "                learning_rate = 0.001, learning_rate_decay = 0.999,\n",
    "                L1_reg = 0, L2_reg = 0, \n",
    "                initial_momentum = 0.5, final_momentum = 0.9,\n",
    "                momentum_switchover = 5,\n",
    "                n_epochs = n_epochs)\n",
    "\n",
    "    model.build_trian(seq, targets)\n",
    "\n",
    "    plt.close('all')\n",
    "    fig = plt.figure()\n",
    "    ax1 = plt.subplot(311)\n",
    "    plt.plot(seq[1])\n",
    "    plt.grid()\n",
    "    ax1.set_title('input')\n",
    "    ax2 = plt.subplot(312)\n",
    "\n",
    "    plt.scatter(xrange(time_steps), targets[1], marker = 'o', c = 'b')\n",
    "    plt.grid()\n",
    "\n",
    "    guess = model.predict_proba(seq[1])\n",
    "    guessed_probs = plt.imshow(guess.T, interpolation = 'nearest', cmap = 'gray')\n",
    "    ax2.set_title('blue points: true class, grayscale: model output (white mean class)')\n",
    "\n",
    "    ax3 = plt.subplot(313)\n",
    "    plt.plot(model.errors)\n",
    "    plt.grid()\n",
    "    ax3.set_title('Training error')\n",
    "    plt.savefig('softmax_' + str(model.activation) + '_Epoch: ' + str(n_epochs) + '.png')\n",
    "    \n",
    "    #print \"Image saved.\"\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    t0 = time.time()\n",
    "    #test_real()\n",
    "    #test_binary()\n",
    "    test_softmax()\n",
    "    print \"Elapsed time: %f\" % (time.time() - t0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing model with softmax outputs\n",
      "Buiding model ..."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import time\n",
    "import os\n",
    "import datetime\n",
    "import matplotlib\n",
    "import gating\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "plt.ion()\n",
    "\n",
    "mode = theano.Mode(linker='cvm') #the runtime algo to execute the code is in c\n",
    "\n",
    "class RNN(object):\n",
    "    def __init__(self, n_u, n_h, n_y, learning_rate, learning_rate_decay, L1_reg, L2_reg,\n",
    "                 initial_momentum, final_momentum, momentum_switchover,\n",
    "                 n_epochs):\n",
    "\n",
    "        self.n_u = int(n_u)\n",
    "        self.n_h = int(n_h)\n",
    "        self.n_y = int(n_y)\n",
    "        self.lstm = gating.LSTM(n_u, n_h)\n",
    "        self.learning_rate = float(learning_rate)\n",
    "        self.learning_rate_decay = float(learning_rate_decay)\n",
    "        self.L1_reg = float(L1_reg)\n",
    "        self.L2_reg = float(L2_reg)\n",
    "        self.initial_momentum = float(initial_momentum)\n",
    "        self.final_momentum = float(final_momentum)\n",
    "        self.momentum_switchover = int(momentum_switchover)\n",
    "        self.n_epochs = int(n_epochs)\n",
    "\n",
    "        # input which is `x`\n",
    "        self.x = T.matrix()\n",
    "\n",
    "        # Note that some the bellow variables are not used when\n",
    "        # the activation function is LSTM or GRU. But we simply\n",
    "        # don't care because theano optimize this for us.\n",
    "        #\n",
    "        # Weights are initialized from an uniform distribution\n",
    "        self.W_uh = theano.shared(value = np.asarray(\n",
    "                                              np.random.uniform(\n",
    "                                                  size = (n_u, n_h),\n",
    "                                                  low = -.01, high = .01),\n",
    "                                              dtype = theano.config.floatX),\n",
    "                                  name = 'W_uh')\n",
    "\n",
    "        self.W_hh = theano.shared(value = np.asarray(\n",
    "                                              np.random.uniform(\n",
    "                                                  size = (n_h, n_h),\n",
    "                                                  low = -.01, high = .01),\n",
    "                                              dtype = theano.config.floatX),\n",
    "                                  name = 'W_hh')\n",
    "\n",
    "        self.W_hy = theano.shared(value = np.asarray(\n",
    "                                              np.random.uniform(\n",
    "                                                  size = (n_h, n_y),\n",
    "                                                  low = -.01, high = .01),\n",
    "                                              dtype = theano.config.floatX),\n",
    "                                  name = 'W_hy')\n",
    "\n",
    "        # initial value of hidden layer units are set to zero\n",
    "        self.h0 = theano.shared(value = np.zeros(\n",
    "                                            (n_h, ),\n",
    "                                            dtype = theano.config.floatX),\n",
    "                                name = 'h0')\n",
    "\n",
    "        self.c0 = theano.shared(value = np.zeros(\n",
    "                                            (n_h, ),\n",
    "                                            dtype = theano.config.floatX),\n",
    "                                name = 'c0')\n",
    "\n",
    "        # biases are initialized to zeros\n",
    "        self.b_h = theano.shared(value = np.zeros(\n",
    "                                             (n_h, ),\n",
    "                                             dtype = theano.config.floatX),\n",
    "                                 name = 'b_h')\n",
    "\n",
    "        self.b_y = theano.shared(value = np.zeros(\n",
    "                                             (n_y, ),\n",
    "                                             dtype = theano.config.floatX),\n",
    "                                 name = 'b_y')\n",
    "        # That's because when it is lstm or gru, parameters are different\n",
    "        self.params = self.lstm.params + [self.W_hy, self.h0, self.b_y]\n",
    "        \n",
    "        # Initial value for updates is zero matrix.\n",
    "        self.updates = {}\n",
    "        for param in self.params:\n",
    "            self.updates[param] = theano.shared(\n",
    "                                      value = np.zeros(\n",
    "                                                  param.get_value(\n",
    "                                                      borrow = True).shape,\n",
    "                                                  dtype = theano.config.floatX),\n",
    "                                      name = 'updates')\n",
    "        \n",
    "        # Default value of c_tm1 is None since we use it just when we have LSTM units\n",
    "        def recurrent_fn(u_t, h_tm1, c_tm1 = None):\n",
    "            # that's because LSTM needs both u_t and h_tm1 to compute gates\n",
    "            h_t, c_t = self.lstm.lstm_as_activation_function(u_t, h_tm1, c_tm1)\n",
    "            y_t = T.dot(h_t, self.W_hy) + self.b_y\n",
    "            return h_t, c_t, y_t\n",
    "\n",
    "        # Iteration over the first dimension of a tensor which is TIME in our case.\n",
    "        # recurrent_fn doesn't use y in the computations, so we do not need y0 (None)\n",
    "        # scan returns updates too which we do not need. (_)\n",
    "        [self.h, self.c, self.y_pred], _ = theano.scan(recurrent_fn,\n",
    "                                               sequences = self.x,\n",
    "                                               outputs_info = [self.h0, self.c0, None])\n",
    "\n",
    "        # L1 norm\n",
    "        self.L1 = abs(self.W_uh.sum()) + \\\n",
    "                  abs(self.W_hh.sum()) + \\\n",
    "                  abs(self.W_hy.sum())\n",
    "\n",
    "        # square of L2 norm\n",
    "        self.L2_sqr = (self.W_uh ** 2).sum() + \\\n",
    "                      (self.W_hh ** 2).sum() + \\\n",
    "                      (self.W_hy ** 2).sum()\n",
    "\n",
    "        # Loss function is different for different output types\n",
    "        # defining function in place is so easy! : lambda input: expresion\n",
    "        self.y = T.vector(name = 'y', dtype = 'int32')\n",
    "        self.p_y_given_x = T.nnet.softmax(self.y_pred)\n",
    "        self.y_out = T.argmax(self.p_y_given_x, axis = -1)\n",
    "        self.loss = lambda y: self.nll_multiclass(y)\n",
    "        self.predict_proba = theano.function(inputs = [self.x, ],\n",
    "                                             outputs = self.p_y_given_x,\n",
    "                                             mode = mode, allow_input_downcast=True)\n",
    "        self.predict = theano.function(inputs = [self.x, ],\n",
    "                                       outputs = self.y_out, # y-out is calculated by applying argmax\n",
    "                                       mode = mode, allow_input_downcast=True)\n",
    "\n",
    "        # Just for tracking training error for Graph 3\n",
    "        self.errors = []\n",
    "\n",
    "    def mse(self, y):\n",
    "        # mean is because of minibatch\n",
    "        return T.mean((self.y_pred - y) ** 2)\n",
    "\n",
    "    def nll_binary(self, y):\n",
    "        # negative log likelihood here is cross entropy\n",
    "        return T.mean(T.nnet.binary_crossentropy(self.p_y_given_x, y))\n",
    "\n",
    "    def nll_multiclass(self, y):\n",
    "        # notice to [  T.arange(y.shape[0])  ,  y  ]\n",
    "        return -T.mean(T.log(self.p_y_given_x)[T.arange(y.shape[0]), y])\n",
    "\n",
    "    # X_train, Y_train, X_test, and Y_test are numpy arrays\n",
    "    def build_trian(self, X_train, Y_train, X_test = None, Y_test = None):\n",
    "        train_set_x = theano.shared(np.asarray(X_train, dtype=theano.config.floatX))\n",
    "        train_set_y = theano.shared(np.asarray(Y_train, dtype=theano.config.floatX))\n",
    "        #if self.output_type in ('binary', 'softmax'):\n",
    "        train_set_y = T.cast(train_set_y, 'int32')\n",
    "\n",
    "        ######################\n",
    "        # BUILD ACTUAL MODEL #\n",
    "        ######################\n",
    "        print 'Buiding model ...'\n",
    "\n",
    "        index = T.lscalar('index')    # index to a case\n",
    "        # learning rate (may change)\n",
    "        lr = T.scalar('lr', dtype = theano.config.floatX)\n",
    "        mom = T.scalar('mom', dtype = theano.config.floatX)  # momentum\n",
    "\n",
    "\n",
    "        # Note that we use cost for training\n",
    "        # But, compute_train_error for just watching and printing\n",
    "        cost = self.loss(self.y) \\\n",
    "            + self.L1_reg * self.L1 \\\n",
    "            + self.L2_reg * self.L2_sqr\n",
    "\n",
    "        # We don't want to pass whole dataset every time we use this function.\n",
    "        # So, the solution is to put the dataset in the GPU as `givens`.\n",
    "        # And just pass index to the function each time as input.\n",
    "        compute_train_error = theano.function(inputs = [index, ],\n",
    "                                              outputs = self.loss(self.y),\n",
    "                                              givens = {\n",
    "                                                  self.x: train_set_x[index],\n",
    "                                                  self.y: train_set_y[index]},\n",
    "                                              mode = mode, allow_input_downcast=True)\n",
    "\n",
    "        # Gradients of cost wrt. [self.W, self.W_in, self.W_out,\n",
    "        # self.h0, self.b_h, self.b_y] using BPTT.\n",
    "        gparams = []\n",
    "        for param in self.params:\n",
    "            gparams.append(T.grad(cost, param))\n",
    "\n",
    "        # zip just concatenate two lists\n",
    "        updates = {}\n",
    "        for param, gparam in zip(self.params, gparams):\n",
    "            weight_update = self.updates[param]\n",
    "            upd = mom * weight_update - lr * gparam\n",
    "            updates[weight_update] = upd\n",
    "            updates[param] = param + upd\n",
    "\n",
    "        # compiling a Theano function `train_model` that returns the\n",
    "        # cost, but in the same time updates the parameter of the\n",
    "        # model based on the rules defined in `updates`\n",
    "        train_model = theano.function(inputs = [index, lr, mom],\n",
    "                                      outputs = cost,\n",
    "                                      updates = updates,\n",
    "                                      givens = {\n",
    "                                          self.x: train_set_x[index], # [:, batch_start:batch_stop]\n",
    "                                          self.y: train_set_y[index]},\n",
    "                                      mode = mode, allow_input_downcast=True)\n",
    "\n",
    "        ###############\n",
    "        # TRAIN MODEL #\n",
    "        ###############\n",
    "        print 'Training model ...'\n",
    "        epoch = 0\n",
    "        n_train = train_set_x.get_value(borrow = True).shape[0]\n",
    "\n",
    "        while (epoch < self.n_epochs):\n",
    "            epoch = epoch + 1\n",
    "            for idx in xrange(n_train):\n",
    "                effective_momentum = self.final_momentum \\\n",
    "                                     if epoch > self.momentum_switchover \\\n",
    "                                     else self.initial_momentum\n",
    "                example_cost = train_model(idx,\n",
    "                                           self.learning_rate,\n",
    "                                           effective_momentum)\n",
    "\n",
    "\n",
    "            # compute loss on training set\n",
    "            train_losses = [compute_train_error(i)\n",
    "                            for i in xrange(n_train)]\n",
    "            this_train_loss = np.mean(train_losses)\n",
    "            self.errors.append(this_train_loss)\n",
    "\n",
    "            print('epoch %i, train loss %f ''lr: %f' % \\\n",
    "                  (epoch, this_train_loss, self.learning_rate))\n",
    "\n",
    "            self.learning_rate *= self.learning_rate_decay\n",
    "\n",
    "def test_softmax(n_u = 4, n_h = 6, n_y = 7, time_steps = 4, n_seq= 6, n_epochs = 10):\n",
    "    # n_y is equal to the number of calsses\n",
    "    print 'Testing model with softmax outputs'\n",
    "\n",
    "    np.random.seed(0)\n",
    "\n",
    "    a=[[[ 5, 3, 5, 3 ],\n",
    "      [ 1, 2, 3, 1 ],\n",
    "      [ 4, 2, 3, 2 ],\n",
    "      [ 2, 3, 4, 5 ]],\n",
    "\n",
    "     [[ 5, 1, 5, 1 ],\n",
    "      [ 6, 1, 6, 1 ],\n",
    "      [ 4, 2, 4, 2 ],\n",
    "      [ 5, 2, 5, 2 ]],\n",
    "\n",
    "     [[ 1, 3, 5, 1 ],\n",
    "      [ 1, 3, 6, 3 ],\n",
    "      [ 2, 4, 6, 4 ],\n",
    "      [ 2, 5, 7, 5 ]],\n",
    "\n",
    "     [[ 5, 5, 5, 3 ],\n",
    "      [ 4, 4, 4, 2 ],\n",
    "      [ 3, 3, 3, 2 ],\n",
    "      [ 1, 2, 3, 1 ]],\n",
    "\n",
    "     [[ 1, 3, 5, 1 ],\n",
    "      [ 1, 4, 6, 1 ],\n",
    "      [ 2, 5, 7, 2 ],\n",
    "      [ 1, 3, 5, 1 ]],\n",
    "\n",
    "     [[ 5, 1, 5, 1 ],\n",
    "      [ 6, 1, 6, 1 ],\n",
    "      [ 7, 2, 7, 2 ],\n",
    "      [ 5, 1, 5, 1 ]]]\n",
    "    \n",
    "    seq = np.array(a)\n",
    "    \n",
    "    #print seq\n",
    "    \n",
    "    # Note that is this case `targets` is a 2d array\n",
    "    targets = np.zeros((n_seq, time_steps), dtype=np.int)\n",
    "\n",
    "    thresh = 0.5\n",
    "    # Comparisons to assing a class label in output\n",
    "    targets=np.array([[1, 6, 2, 5],\n",
    " [1, 6, 2, 5],\n",
    " [1, 6, 2, 5],\n",
    " [1, 4, 5, 1],\n",
    " [1, 4, 5, 1],\n",
    " [1, 4, 5, 1]])\n",
    "\n",
    "    model = RNN(n_u = n_u, n_h = n_h, n_y = n_y, \n",
    "                learning_rate = 0.01, learning_rate_decay = 0.999,\n",
    "                L1_reg = 0, L2_reg = 0, \n",
    "                initial_momentum = 0.5, final_momentum = 0.9,\n",
    "                momentum_switchover = 5,\n",
    "                n_epochs = n_epochs)\n",
    "\n",
    "    model.build_trian(seq, targets)\n",
    "\n",
    "    plt.close('all')\n",
    "    fig = plt.figure()\n",
    "    ax1 = plt.subplot(311)\n",
    "    plt.plot(seq[1])\n",
    "    plt.grid()\n",
    "    ax1.set_title('input')\n",
    "    ax2 = plt.subplot(312)\n",
    "\n",
    "    plt.scatter(xrange(time_steps), targets[1], marker = 'o', c = 'b')\n",
    "    plt.grid()\n",
    "\n",
    "    guess = model.predict_proba(seq[1])\n",
    "    guessed_probs = plt.imshow(guess.T, interpolation = 'nearest', cmap = 'gray')\n",
    "    ax2.set_title('blue points: true class, grayscale: model output (white mean class)')\n",
    "\n",
    "    ax3 = plt.subplot(313)\n",
    "    plt.plot(model.errors)\n",
    "    plt.grid()\n",
    "    ax3.set_title('Training error')\n",
    "    plt.savefig('softmax_Epoch: ' + str(n_epochs) + '.png')\n",
    "    \n",
    "    print \"Image saved.\"\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    t0 = time.time()\n",
    "    test_softmax()\n",
    "    print \"Elapsed time: %f\" % (time.time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
